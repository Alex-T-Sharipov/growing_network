Training with a single process on 1 device (cuda:0).
WARNING: No pretrained configuration specified for MONet_T_dynamic model. Using a default. Please add a config to the model pretrained_cfg registry or pass explicitly.
Model MONet_T_dynamic created, param count:5990442
Data processing configuration for current model + dataset:
	input_size: (3, 32, 32)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.875
	crop_mode: center
AMP not enabled. Training in float32.
/home/sharipov/monet/venv/lib64/python3.9/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Scheduled epochs: 300. LR stepped per epoch.
Train: 0 [   0/781 (  0%)]  Loss: 2.30 (2.30)  Time: 7.462s,    8.58/s  (7.462s,    8.58/s)  LR: 1.000e-05  Data: 1.303 (1.303)
Train: 0 [  50/781 (  6%)]  Loss: 2.28 (2.28)  Time: 0.080s,  798.87/s  (0.225s,  284.49/s)  LR: 1.000e-05  Data: 0.010 (0.035)
Train: 0 [ 100/781 ( 13%)]  Loss: 2.14 (2.25)  Time: 0.080s,  803.89/s  (0.153s,  418.33/s)  LR: 1.000e-05  Data: 0.010 (0.023)
Train: 0 [ 150/781 ( 19%)]  Loss: 2.05 (2.22)  Time: 0.079s,  810.28/s  (0.129s,  497.50/s)  LR: 1.000e-05  Data: 0.010 (0.019)
Train: 0 [ 200/781 ( 26%)]  Loss: 2.15 (2.21)  Time: 0.080s,  804.06/s  (0.116s,  549.93/s)  LR: 1.000e-05  Data: 0.009 (0.016)
Train: 0 [ 250/781 ( 32%)]  Loss: 2.15 (2.19)  Time: 0.080s,  796.78/s  (0.109s,  586.86/s)  LR: 1.000e-05  Data: 0.010 (0.015)
Train: 0 [ 300/781 ( 38%)]  Loss: 2.13 (2.19)  Time: 0.080s,  798.92/s  (0.104s,  613.77/s)  LR: 1.000e-05  Data: 0.010 (0.014)
Train: 0 [ 350/781 ( 45%)]  Loss: 2.29 (2.18)  Time: 0.081s,  786.15/s  (0.101s,  635.05/s)  LR: 1.000e-05  Data: 0.010 (0.014)
Train: 0 [ 400/781 ( 51%)]  Loss: 2.16 (2.17)  Time: 0.080s,  798.30/s  (0.098s,  652.24/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 0 [ 450/781 ( 58%)]  Loss: 2.13 (2.17)  Time: 0.079s,  805.47/s  (0.096s,  666.32/s)  LR: 1.000e-05  Data: 0.009 (0.013)
Train: 0 [ 500/781 ( 64%)]  Loss: 2.18 (2.17)  Time: 0.080s,  804.25/s  (0.094s,  678.02/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 0 [ 550/781 ( 71%)]  Loss: 2.25 (2.16)  Time: 0.079s,  814.24/s  (0.093s,  687.85/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 0 [ 600/781 ( 77%)]  Loss: 2.12 (2.16)  Time: 0.080s,  804.63/s  (0.092s,  696.34/s)  LR: 1.000e-05  Data: 0.015 (0.012)
Train: 0 [ 650/781 ( 83%)]  Loss: 2.07 (2.16)  Time: 0.084s,  766.45/s  (0.091s,  703.32/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 0 [ 700/781 ( 90%)]  Loss: 2.15 (2.15)  Time: 0.080s,  804.04/s  (0.090s,  709.49/s)  LR: 1.000e-05  Data: 0.009 (0.012)
Train: 0 [ 750/781 ( 96%)]  Loss: 2.14 (2.15)  Time: 0.080s,  802.96/s  (0.090s,  714.94/s)  LR: 1.000e-05  Data: 0.009 (0.012)
Test: [   0/156]  Time: 1.852 (1.852)  Loss:   1.824 ( 1.824)  Acc@1:  37.500 ( 37.500)  Acc@5:  93.750 ( 93.750)
Test: [  50/156]  Time: 0.023 (0.059)  Loss:   1.912 ( 1.881)  Acc@1:  37.500 ( 33.762)  Acc@5:  85.938 ( 85.233)
Test: [ 100/156]  Time: 0.023 (0.041)  Loss:   1.915 ( 1.881)  Acc@1:  26.562 ( 33.462)  Acc@5:  82.812 ( 85.396)
Test: [ 150/156]  Time: 0.016 (0.035)  Loss:   1.871 ( 1.878)  Acc@1:  39.062 ( 33.485)  Acc@5:  85.938 ( 85.286)
Test: [ 156/156]  Time: 0.898 (0.040)  Loss:   1.727 ( 1.877)  Acc@1:  25.000 ( 33.370)  Acc@5:  87.500 ( 85.340)
Current checkpoints:
 ('./output/train/Upd_Exp1_CIFAR10_dynamic_16_layer/checkpoint-0.pth.tar', 33.37)

Train: 1 [   0/781 (  0%)]  Loss: 2.05 (2.05)  Time: 0.309s,  206.82/s  (0.309s,  206.82/s)  LR: 1.900e-05  Data: 0.104 (0.104)
Train: 1 [  50/781 (  6%)]  Loss: 2.13 (2.10)  Time: 0.079s,  810.38/s  (0.085s,  756.42/s)  LR: 1.900e-05  Data: 0.009 (0.012)
Train: 1 [ 100/781 ( 13%)]  Loss: 2.22 (2.10)  Time: 0.080s,  796.31/s  (0.082s,  777.77/s)  LR: 1.900e-05  Data: 0.010 (0.011)
Train: 1 [ 150/781 ( 19%)]  Loss: 2.11 (2.10)  Time: 0.079s,  810.61/s  (0.082s,  784.43/s)  LR: 1.900e-05  Data: 0.010 (0.011)
Train: 1 [ 200/781 ( 26%)]  Loss: 1.96 (2.10)  Time: 0.080s,  802.23/s  (0.081s,  788.24/s)  LR: 1.900e-05  Data: 0.009 (0.010)
Train: 1 [ 250/781 ( 32%)]  Loss: 1.98 (2.10)  Time: 0.079s,  808.56/s  (0.081s,  791.04/s)  LR: 1.900e-05  Data: 0.010 (0.010)
Train: 1 [ 300/781 ( 38%)]  Loss: 2.14 (2.10)  Time: 0.080s,  798.02/s  (0.081s,  792.84/s)  LR: 1.900e-05  Data: 0.009 (0.010)
Train: 1 [ 350/781 ( 45%)]  Loss: 2.21 (2.09)  Time: 0.079s,  806.26/s  (0.081s,  794.16/s)  LR: 1.900e-05  Data: 0.009 (0.010)
Train: 1 [ 400/781 ( 51%)]  Loss: 2.14 (2.09)  Time: 0.079s,  810.04/s  (0.080s,  795.17/s)  LR: 1.900e-05  Data: 0.009 (0.010)
Train: 1 [ 450/781 ( 58%)]  Loss: 2.07 (2.09)  Time: 0.080s,  800.82/s  (0.080s,  795.93/s)  LR: 1.900e-05  Data: 0.010 (0.010)
Train: 1 [ 500/781 ( 64%)]  Loss: 1.96 (2.09)  Time: 0.079s,  808.26/s  (0.080s,  796.55/s)  LR: 1.900e-05  Data: 0.009 (0.010)
Train: 1 [ 550/781 ( 71%)]  Loss: 2.04 (2.09)  Time: 0.079s,  808.18/s  (0.080s,  796.53/s)  LR: 1.900e-05  Data: 0.010 (0.010)
Train: 1 [ 600/781 ( 77%)]  Loss: 2.10 (2.09)  Time: 0.080s,  797.79/s  (0.080s,  797.10/s)  LR: 1.900e-05  Data: 0.015 (0.010)
Train: 1 [ 650/781 ( 83%)]  Loss: 2.26 (2.08)  Time: 0.079s,  811.31/s  (0.080s,  797.45/s)  LR: 1.900e-05  Data: 0.009 (0.010)
Train: 1 [ 700/781 ( 90%)]  Loss: 2.18 (2.08)  Time: 0.079s,  808.37/s  (0.080s,  797.78/s)  LR: 1.900e-05  Data: 0.010 (0.010)
Train: 1 [ 750/781 ( 96%)]  Loss: 2.19 (2.08)  Time: 0.080s,  800.52/s  (0.080s,  798.08/s)  LR: 1.900e-05  Data: 0.010 (0.010)
Test: [   0/156]  Time: 0.088 (0.088)  Loss:   1.716 ( 1.716)  Acc@1:  40.625 ( 40.625)  Acc@5:  85.938 ( 85.938)
Test: [  50/156]  Time: 0.023 (0.024)  Loss:   1.808 ( 1.762)  Acc@1:  34.375 ( 38.480)  Acc@5:  90.625 ( 86.979)
Test: [ 100/156]  Time: 0.023 (0.024)  Loss:   1.824 ( 1.763)  Acc@1:  28.125 ( 37.949)  Acc@5:  84.375 ( 87.175)
Test: [ 150/156]  Time: 0.016 (0.023)  Loss:   1.759 ( 1.760)  Acc@1:  39.062 ( 37.697)  Acc@5:  82.812 ( 87.355)
Test: [ 156/156]  Time: 0.011 (0.023)  Loss:   1.713 ( 1.759)  Acc@1:  31.250 ( 37.540)  Acc@5:  87.500 ( 87.430)
Current checkpoints:
 ('./output/train/Upd_Exp1_CIFAR10_dynamic_16_layer/checkpoint-1.pth.tar', 37.54)
 ('./output/train/Upd_Exp1_CIFAR10_dynamic_16_layer/checkpoint-0.pth.tar', 33.37)

Train: 2 [   0/781 (  0%)]  Loss: 1.96 (1.96)  Time: 0.269s,  237.50/s  (0.269s,  237.50/s)  LR: 2.800e-05  Data: 0.117 (0.117)
Train: 2 [  50/781 (  6%)]  Loss: 2.16 (2.07)  Time: 0.079s,  806.51/s  (0.084s,  760.30/s)  LR: 2.800e-05  Data: 0.010 (0.012)
Train: 2 [ 100/781 ( 13%)]  Loss: 2.18 (2.07)  Time: 0.079s,  807.31/s  (0.082s,  778.56/s)  LR: 2.800e-05  Data: 0.009 (0.011)
Train: 2 [ 150/781 ( 19%)]  Loss: 2.15 (2.07)  Time: 0.079s,  809.58/s  (0.081s,  786.59/s)  LR: 2.800e-05  Data: 0.010 (0.010)
Train: 2 [ 200/781 ( 26%)]  Loss: 2.11 (2.07)  Time: 0.080s,  796.48/s  (0.081s,  790.32/s)  LR: 2.800e-05  Data: 0.010 (0.010)
Train: 2 [ 250/781 ( 32%)]  Loss: 2.11 (2.07)  Time: 0.080s,  801.41/s  (0.081s,  792.47/s)  LR: 2.800e-05  Data: 0.010 (0.010)
Train: 2 [ 300/781 ( 38%)]  Loss: 2.00 (2.06)  Time: 0.080s,  798.51/s  (0.081s,  794.19/s)  LR: 2.800e-05  Data: 0.010 (0.010)
Train: 2 [ 350/781 ( 45%)]  Loss: 2.16 (2.06)  Time: 0.079s,  813.10/s  (0.080s,  795.52/s)  LR: 2.800e-05  Data: 0.009 (0.010)
Train: 2 [ 400/781 ( 51%)]  Loss: 2.10 (2.06)  Time: 0.080s,  804.00/s  (0.080s,  796.70/s)  LR: 2.800e-05  Data: 0.009 (0.010)
Train: 2 [ 450/781 ( 58%)]  Loss: 2.06 (2.06)  Time: 0.080s,  802.66/s  (0.080s,  797.31/s)  LR: 2.800e-05  Data: 0.010 (0.010)
Train: 2 [ 500/781 ( 64%)]  Loss: 2.14 (2.06)  Time: 0.080s,  800.79/s  (0.080s,  797.59/s)  LR: 2.800e-05  Data: 0.009 (0.010)
Train: 2 [ 550/781 ( 71%)]  Loss: 2.14 (2.06)  Time: 0.079s,  811.16/s  (0.080s,  798.01/s)  LR: 2.800e-05  Data: 0.009 (0.010)
Train: 2 [ 600/781 ( 77%)]  Loss: 2.09 (2.05)  Time: 0.079s,  811.22/s  (0.080s,  798.38/s)  LR: 2.800e-05  Data: 0.009 (0.010)
Train: 2 [ 650/781 ( 83%)]  Loss: 2.19 (2.05)  Time: 0.079s,  807.88/s  (0.080s,  798.81/s)  LR: 2.800e-05  Data: 0.010 (0.010)
Train: 2 [ 700/781 ( 90%)]  Loss: 2.16 (2.05)  Time: 0.079s,  810.57/s  (0.080s,  799.15/s)  LR: 2.800e-05  Data: 0.010 (0.010)
Train: 2 [ 750/781 ( 96%)]  Loss: 2.02 (2.05)  Time: 0.079s,  812.01/s  (0.080s,  799.57/s)  LR: 2.800e-05  Data: 0.014 (0.010)
Test: [   0/156]  Time: 0.088 (0.088)  Loss:   1.612 ( 1.612)  Acc@1:  45.312 ( 45.312)  Acc@5:  95.312 ( 95.312)
Test: [  50/156]  Time: 0.023 (0.024)  Loss:   1.702 ( 1.677)  Acc@1:  43.750 ( 42.892)  Acc@5:  89.062 ( 90.227)
Test: [ 100/156]  Time: 0.023 (0.024)  Loss:   1.768 ( 1.679)  Acc@1:  37.500 ( 42.837)  Acc@5:  84.375 ( 89.851)
Test: [ 150/156]  Time: 0.016 (0.023)  Loss:   1.729 ( 1.677)  Acc@1:  42.188 ( 42.229)  Acc@5:  90.625 ( 89.901)
Test: [ 156/156]  Time: 0.011 (0.023)  Loss:   1.787 ( 1.677)  Acc@1:  31.250 ( 42.220)  Acc@5:  93.750 ( 89.930)
Current checkpoints:
 ('./output/train/Upd_Exp1_CIFAR10_dynamic_16_layer/checkpoint-2.pth.tar', 42.22)
 ('./output/train/Upd_Exp1_CIFAR10_dynamic_16_layer/checkpoint-1.pth.tar', 37.54)
 ('./output/train/Upd_Exp1_CIFAR10_dynamic_16_layer/checkpoint-0.pth.tar', 33.37)

Train: 3 [   0/781 (  0%)]  Loss: 2.16 (2.16)  Time: 0.293s,  218.34/s  (0.293s,  218.34/s)  LR: 3.700e-05  Data: 0.113 (0.113)
Train: 3 [  50/781 (  6%)]  Loss: 2.19 (2.02)  Time: 0.080s,  799.87/s  (0.084s,  760.70/s)  LR: 3.700e-05  Data: 0.010 (0.012)
Train: 3 [ 100/781 ( 13%)]  Loss: 2.25 (2.04)  Time: 0.080s,  801.00/s  (0.082s,  780.59/s)  LR: 3.700e-05  Data: 0.009 (0.011)
Train: 3 [ 150/781 ( 19%)]  Loss: 2.09 (2.04)  Time: 0.080s,  800.32/s  (0.081s,  788.46/s)  LR: 3.700e-05  Data: 0.010 (0.011)
Train: 3 [ 200/781 ( 26%)]  Loss: 2.16 (2.03)  Time: 0.079s,  811.34/s  (0.081s,  792.52/s)  LR: 3.700e-05  Data: 0.010 (0.011)
