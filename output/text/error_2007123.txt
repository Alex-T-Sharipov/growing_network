[2024-03-05 15:33:30,185] torch.distributed.run: [WARNING] 
[2024-03-05 15:33:30,185] torch.distributed.run: [WARNING] *****************************************
[2024-03-05 15:33:30,185] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-03-05 15:33:30,185] torch.distributed.run: [WARNING] *****************************************
Training in distributed mode with multiple processes, 1 device per process.Process 0, total 2, device cuda:0.
WARNING: No pretrained configuration specified for MONet_T model. Using a default. Please add a config to the model pretrained_cfg registry or pass explicitly.
Training in distributed mode with multiple processes, 1 device per process.Process 1, total 2, device cuda:1.
WARNING: No pretrained configuration specified for MONet_T model. Using a default. Please add a config to the model pretrained_cfg registry or pass explicitly.
Model MONet_T created, param count:10165736
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.875
	crop_mode: center
Learning rate (0.0007071067811865476) calculated from base learning rate (0.001) and effective global batch size (128) with sqrt scaling.
AMP not enabled. Training in float32.
Restoring model state from checkpoint...
Restoring optimizer state from checkpoint...
Loaded checkpoint '/home/sharipov/monet/output/train/Upd_Exp2_Image100/model_best.pth.tar' (epoch 63)
Using native Torch DistributedDataParallel.
/home/sharipov/monet/venv/lib64/python3.9/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/sharipov/monet/venv/lib64/python3.9/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Scheduled epochs: 300. LR stepped per epoch.
Train: 64 [   0/989 (  0%)]  Loss: 3.08 (3.08)  Time: 13.567s,    9.43/s  (13.567s,    9.43/s)  LR: 6.317e-04  Data: 4.130 (4.130)
Train: 64 [  50/989 (  5%)]  Loss: 2.08 (2.58)  Time: 0.667s,  191.81/s  (0.982s,  130.29/s)  LR: 6.317e-04  Data: 0.033 (0.115)
Train: 64 [ 100/989 ( 10%)]  Loss: 2.73 (2.63)  Time: 0.694s,  184.56/s  (0.852s,  150.28/s)  LR: 6.317e-04  Data: 0.045 (0.075)
Train: 64 [ 150/989 ( 15%)]  Loss: 2.87 (2.69)  Time: 0.767s,  166.80/s  (0.808s,  158.41/s)  LR: 6.317e-04  Data: 0.032 (0.059)
Train: 64 [ 200/989 ( 20%)]  Loss: 3.22 (2.80)  Time: 0.737s,  173.69/s  (0.786s,  162.95/s)  LR: 6.317e-04  Data: 0.037 (0.050)
Train: 64 [ 250/989 ( 25%)]  Loss: 2.81 (2.80)  Time: 0.752s,  170.26/s  (0.773s,  165.62/s)  LR: 6.317e-04  Data: 0.025 (0.046)
Train: 64 [ 300/989 ( 30%)]  Loss: 2.95 (2.82)  Time: 0.675s,  189.58/s  (0.765s,  167.22/s)  LR: 6.317e-04  Data: 0.015 (0.043)
Train: 64 [ 350/989 ( 35%)]  Loss: 2.51 (2.78)  Time: 0.686s,  186.49/s  (0.759s,  168.59/s)  LR: 6.317e-04  Data: 0.036 (0.041)
Train: 64 [ 400/989 ( 40%)]  Loss: 3.16 (2.82)  Time: 0.686s,  186.59/s  (0.755s,  169.53/s)  LR: 6.317e-04  Data: 0.013 (0.040)
Train: 64 [ 450/989 ( 46%)]  Loss: 2.31 (2.77)  Time: 0.664s,  192.70/s  (0.751s,  170.46/s)  LR: 6.317e-04  Data: 0.036 (0.039)
Train: 64 [ 500/989 ( 51%)]  Loss: 3.27 (2.82)  Time: 0.679s,  188.56/s  (0.748s,  171.23/s)  LR: 6.317e-04  Data: 0.047 (0.039)
Train: 64 [ 550/989 ( 56%)]  Loss: 2.55 (2.79)  Time: 0.722s,  177.38/s  (0.746s,  171.69/s)  LR: 6.317e-04  Data: 0.052 (0.039)
Train: 64 [ 600/989 ( 61%)]  Loss: 3.19 (2.82)  Time: 0.761s,  168.12/s  (0.743s,  172.16/s)  LR: 6.317e-04  Data: 0.048 (0.038)
Train: 64 [ 650/989 ( 66%)]  Loss: 3.10 (2.84)  Time: 0.710s,  180.38/s  (0.741s,  172.63/s)  LR: 6.317e-04  Data: 0.009 (0.037)
Train: 64 [ 700/989 ( 71%)]  Loss: 3.18 (2.87)  Time: 0.772s,  165.80/s  (0.740s,  172.93/s)  LR: 6.317e-04  Data: 0.031 (0.037)
Train: 64 [ 750/989 ( 76%)]  Loss: 3.28 (2.89)  Time: 0.684s,  187.01/s  (0.739s,  173.23/s)  LR: 6.317e-04  Data: 0.036 (0.037)
Train: 64 [ 800/989 ( 81%)]  Loss: 3.22 (2.91)  Time: 0.719s,  177.96/s  (0.738s,  173.50/s)  LR: 6.317e-04  Data: 0.018 (0.036)
Train: 64 [ 850/989 ( 86%)]  Loss: 2.47 (2.89)  Time: 0.716s,  178.68/s  (0.737s,  173.75/s)  LR: 6.317e-04  Data: 0.006 (0.036)
Train: 64 [ 900/989 ( 91%)]  Loss: 2.20 (2.85)  Time: 0.768s,  166.62/s  (0.736s,  173.83/s)  LR: 6.317e-04  Data: 0.027 (0.035)
Train: 64 [ 950/989 ( 96%)]  Loss: 3.24 (2.87)  Time: 0.729s,  175.48/s  (0.736s,  173.82/s)  LR: 6.317e-04  Data: 0.022 (0.035)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 4.968 (4.968)  Loss:   1.136 ( 1.136)  Acc@1:  76.562 ( 76.562)  Acc@5:  91.406 ( 91.406)
Test: [  39/39]  Time: 0.674 (0.448)  Loss:   0.344 ( 1.021)  Acc@1: 100.000 ( 76.400)  Acc@5: 100.000 ( 93.380)
Current checkpoints:
 ('./output/train/Upd_Exp3_Image100/checkpoint-64.pth.tar', 76.4)

Train: 65 [   0/989 (  0%)]  Loss: 3.61 (3.61)  Time: 2.264s,   56.55/s  (2.264s,   56.55/s)  LR: 6.294e-04  Data: 1.170 (1.170)
Train: 65 [  50/989 (  5%)]  Loss: 3.45 (3.53)  Time: 0.649s,  197.26/s  (0.709s,  180.48/s)  LR: 6.294e-04  Data: 0.024 (0.059)
Train: 65 [ 100/989 ( 10%)]  Loss: 2.70 (3.26)  Time: 0.644s,  198.78/s  (0.692s,  185.02/s)  LR: 6.294e-04  Data: 0.032 (0.049)
Train: 65 [ 150/989 ( 15%)]  Loss: 3.03 (3.20)  Time: 0.713s,  179.47/s  (0.688s,  186.18/s)  LR: 6.294e-04  Data: 0.038 (0.045)
Train: 65 [ 200/989 ( 20%)]  Loss: 3.17 (3.19)  Time: 0.656s,  195.20/s  (0.684s,  187.11/s)  LR: 6.294e-04  Data: 0.034 (0.043)
Train: 65 [ 250/989 ( 25%)]  Loss: 2.66 (3.11)  Time: 0.702s,  182.31/s  (0.682s,  187.67/s)  LR: 6.294e-04  Data: 0.043 (0.041)
Train: 65 [ 300/989 ( 30%)]  Loss: 3.16 (3.11)  Time: 0.692s,  184.96/s  (0.680s,  188.13/s)  LR: 6.294e-04  Data: 0.055 (0.040)
Train: 65 [ 350/989 ( 35%)]  Loss: 2.69 (3.06)  Time: 0.515s,  248.45/s  (0.678s,  188.89/s)  LR: 6.294e-04  Data: 0.011 (0.040)
Train: 65 [ 400/989 ( 40%)]  Loss: 3.03 (3.06)  Time: 0.660s,  193.95/s  (0.686s,  186.62/s)  LR: 6.294e-04  Data: 0.028 (0.044)
Train: 65 [ 450/989 ( 46%)]  Loss: 3.01 (3.05)  Time: 0.668s,  191.73/s  (0.684s,  187.11/s)  LR: 6.294e-04  Data: 0.037 (0.043)
Train: 65 [ 500/989 ( 51%)]  Loss: 2.70 (3.02)  Time: 0.656s,  195.09/s  (0.683s,  187.29/s)  LR: 6.294e-04  Data: 0.032 (0.042)
Train: 65 [ 550/989 ( 56%)]  Loss: 3.53 (3.06)  Time: 0.650s,  196.82/s  (0.682s,  187.69/s)  LR: 6.294e-04  Data: 0.029 (0.041)
Train: 65 [ 600/989 ( 61%)]  Loss: 3.69 (3.11)  Time: 0.658s,  194.57/s  (0.682s,  187.73/s)  LR: 6.294e-04  Data: 0.030 (0.041)
Train: 65 [ 650/989 ( 66%)]  Loss: 3.15 (3.11)  Time: 0.662s,  193.36/s  (0.681s,  187.83/s)  LR: 6.294e-04  Data: 0.035 (0.041)
Train: 65 [ 700/989 ( 71%)]  Loss: 2.78 (3.09)  Time: 0.692s,  184.96/s  (0.681s,  187.91/s)  LR: 6.294e-04  Data: 0.047 (0.041)
Train: 65 [ 750/989 ( 76%)]  Loss: 1.92 (3.02)  Time: 0.665s,  192.52/s  (0.681s,  187.91/s)  LR: 6.294e-04  Data: 0.031 (0.040)
Train: 65 [ 800/989 ( 81%)]  Loss: 3.02 (3.02)  Time: 0.686s,  186.58/s  (0.681s,  187.88/s)  LR: 6.294e-04  Data: 0.033 (0.040)
Train: 65 [ 850/989 ( 86%)]  Loss: 3.07 (3.02)  Time: 0.658s,  194.56/s  (0.682s,  187.80/s)  LR: 6.294e-04  Data: 0.027 (0.040)
Train: 65 [ 900/989 ( 91%)]  Loss: 2.96 (3.02)  Time: 0.669s,  191.37/s  (0.682s,  187.71/s)  LR: 6.294e-04  Data: 0.036 (0.039)
Train: 65 [ 950/989 ( 96%)]  Loss: 2.82 (3.01)  Time: 0.698s,  183.45/s  (0.683s,  187.50/s)  LR: 6.294e-04  Data: 0.049 (0.039)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.277 (1.277)  Loss:   1.385 ( 1.385)  Acc@1:  67.969 ( 67.969)  Acc@5:  90.625 ( 90.625)
Test: [  39/39]  Time: 0.048 (0.328)  Loss:   0.402 ( 1.060)  Acc@1:  87.500 ( 76.660)  Acc@5: 100.000 ( 93.520)
Current checkpoints:
 ('./output/train/Upd_Exp3_Image100/checkpoint-65.pth.tar', 76.66)
 ('./output/train/Upd_Exp3_Image100/checkpoint-64.pth.tar', 76.4)

Train: 66 [   0/989 (  0%)]  Loss: 2.83 (2.83)  Time: 1.903s,   67.26/s  (1.903s,   67.26/s)  LR: 6.271e-04  Data: 1.146 (1.146)
Train: 66 [  50/989 (  5%)]  Loss: 2.47 (2.65)  Time: 0.656s,  195.16/s  (0.688s,  186.16/s)  LR: 6.271e-04  Data: 0.038 (0.054)
Train: 66 [ 100/989 ( 10%)]  Loss: 2.03 (2.44)  Time: 0.667s,  191.87/s  (0.675s,  189.71/s)  LR: 6.271e-04  Data: 0.043 (0.046)
Train: 66 [ 150/989 ( 15%)]  Loss: 2.44 (2.44)  Time: 0.638s,  200.56/s  (0.670s,  191.12/s)  LR: 6.271e-04  Data: 0.026 (0.041)
Train: 66 [ 200/989 ( 20%)]  Loss: 2.88 (2.53)  Time: 0.667s,  191.90/s  (0.668s,  191.75/s)  LR: 6.271e-04  Data: 0.054 (0.040)
Train: 66 [ 250/989 ( 25%)]  Loss: 3.51 (2.69)  Time: 0.701s,  182.51/s  (0.666s,  192.12/s)  LR: 6.271e-04  Data: 0.036 (0.039)
Train: 66 [ 300/989 ( 30%)]  Loss: 2.56 (2.68)  Time: 0.686s,  186.47/s  (0.667s,  192.03/s)  LR: 6.271e-04  Data: 0.054 (0.039)
Train: 66 [ 350/989 ( 35%)]  Loss: 2.66 (2.67)  Time: 0.679s,  188.49/s  (0.667s,  191.91/s)  LR: 6.271e-04  Data: 0.035 (0.039)
Train: 66 [ 400/989 ( 40%)]  Loss: 2.29 (2.63)  Time: 0.678s,  188.81/s  (0.667s,  191.84/s)  LR: 6.271e-04  Data: 0.030 (0.038)
Train: 66 [ 450/989 ( 46%)]  Loss: 3.22 (2.69)  Time: 0.683s,  187.31/s  (0.667s,  191.88/s)  LR: 6.271e-04  Data: 0.031 (0.038)
Train: 66 [ 500/989 ( 51%)]  Loss: 3.44 (2.76)  Time: 0.648s,  197.57/s  (0.667s,  191.81/s)  LR: 6.271e-04  Data: 0.037 (0.038)
Train: 66 [ 550/989 ( 56%)]  Loss: 3.12 (2.79)  Time: 0.664s,  192.91/s  (0.667s,  191.78/s)  LR: 6.271e-04  Data: 0.038 (0.038)
Train: 66 [ 600/989 ( 61%)]  Loss: 3.10 (2.81)  Time: 0.650s,  196.78/s  (0.667s,  191.77/s)  LR: 6.271e-04  Data: 0.027 (0.038)
Train: 66 [ 650/989 ( 66%)]  Loss: 2.60 (2.80)  Time: 0.866s,  147.87/s  (0.668s,  191.54/s)  LR: 6.271e-04  Data: 0.030 (0.037)
Train: 66 [ 700/989 ( 71%)]  Loss: 2.80 (2.80)  Time: 0.704s,  181.71/s  (0.669s,  191.35/s)  LR: 6.271e-04  Data: 0.050 (0.038)
Train: 66 [ 750/989 ( 76%)]  Loss: 3.14 (2.82)  Time: 0.621s,  205.99/s  (0.669s,  191.26/s)  LR: 6.271e-04  Data: 0.033 (0.038)
Train: 66 [ 800/989 ( 81%)]  Loss: 3.39 (2.85)  Time: 0.673s,  190.25/s  (0.670s,  191.05/s)  LR: 6.271e-04  Data: 0.028 (0.038)
Train: 66 [ 850/989 ( 86%)]  Loss: 2.82 (2.85)  Time: 0.666s,  192.28/s  (0.671s,  190.85/s)  LR: 6.271e-04  Data: 0.030 (0.037)
Train: 66 [ 900/989 ( 91%)]  Loss: 3.32 (2.87)  Time: 0.684s,  187.18/s  (0.672s,  190.50/s)  LR: 6.271e-04  Data: 0.037 (0.037)
Train: 66 [ 950/989 ( 96%)]  Loss: 3.23 (2.89)  Time: 0.611s,  209.51/s  (0.673s,  190.29/s)  LR: 6.271e-04  Data: 0.031 (0.037)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.299 (1.299)  Loss:   1.122 ( 1.122)  Acc@1:  78.906 ( 78.906)  Acc@5:  92.188 ( 92.188)
Test: [  39/39]  Time: 0.049 (0.324)  Loss:   0.350 ( 1.073)  Acc@1:  87.500 ( 76.660)  Acc@5: 100.000 ( 93.760)
Current checkpoints:
 ('./output/train/Upd_Exp3_Image100/checkpoint-65.pth.tar', 76.66)
 ('./output/train/Upd_Exp3_Image100/checkpoint-66.pth.tar', 76.66)
 ('./output/train/Upd_Exp3_Image100/checkpoint-64.pth.tar', 76.4)

Train: 67 [   0/989 (  0%)]  Loss: 3.67 (3.67)  Time: 1.710s,   74.83/s  (1.710s,   74.83/s)  LR: 6.248e-04  Data: 0.977 (0.977)
Train: 67 [  50/989 (  5%)]  Loss: 2.71 (3.19)  Time: 0.669s,  191.38/s  (0.693s,  184.73/s)  LR: 6.248e-04  Data: 0.030 (0.056)
Train: 67 [ 100/989 ( 10%)]  Loss: 2.14 (2.84)  Time: 0.680s,  188.35/s  (0.677s,  189.03/s)  LR: 6.248e-04  Data: 0.037 (0.048)
Train: 67 [ 150/989 ( 15%)]  Loss: 3.37 (2.97)  Time: 0.646s,  198.28/s  (0.672s,  190.45/s)  LR: 6.248e-04  Data: 0.044 (0.044)
Train: 67 [ 200/989 ( 20%)]  Loss: 2.67 (2.91)  Time: 0.647s,  197.80/s  (0.668s,  191.49/s)  LR: 6.248e-04  Data: 0.033 (0.042)
Train: 67 [ 250/989 ( 25%)]  Loss: 3.19 (2.96)  Time: 0.671s,  190.88/s  (0.667s,  191.92/s)  LR: 6.248e-04  Data: 0.045 (0.040)
Train: 67 [ 300/989 ( 30%)]  Loss: 3.22 (3.00)  Time: 0.660s,  193.99/s  (0.666s,  192.07/s)  LR: 6.248e-04  Data: 0.034 (0.039)
Train: 67 [ 350/989 ( 35%)]  Loss: 3.00 (3.00)  Time: 0.646s,  198.22/s  (0.666s,  192.15/s)  LR: 6.248e-04  Data: 0.022 (0.039)
Train: 67 [ 400/989 ( 40%)]  Loss: 2.74 (2.97)  Time: 0.639s,  200.32/s  (0.666s,  192.18/s)  LR: 6.248e-04  Data: 0.030 (0.038)
Train: 67 [ 450/989 ( 46%)]  Loss: 2.86 (2.96)  Time: 0.674s,  189.95/s  (0.666s,  192.25/s)  LR: 6.248e-04  Data: 0.035 (0.038)
Train: 67 [ 500/989 ( 51%)]  Loss: 2.71 (2.93)  Time: 0.633s,  202.29/s  (0.666s,  192.23/s)  LR: 6.248e-04  Data: 0.022 (0.038)
Train: 67 [ 550/989 ( 56%)]  Loss: 2.61 (2.91)  Time: 0.656s,  194.99/s  (0.666s,  192.14/s)  LR: 6.248e-04  Data: 0.036 (0.038)
Train: 67 [ 600/989 ( 61%)]  Loss: 3.11 (2.92)  Time: 0.690s,  185.38/s  (0.666s,  192.16/s)  LR: 6.248e-04  Data: 0.049 (0.038)
Train: 67 [ 650/989 ( 66%)]  Loss: 1.94 (2.85)  Time: 0.684s,  187.16/s  (0.666s,  192.09/s)  LR: 6.248e-04  Data: 0.037 (0.038)
Train: 67 [ 700/989 ( 71%)]  Loss: 2.78 (2.85)  Time: 0.677s,  189.16/s  (0.667s,  191.80/s)  LR: 6.248e-04  Data: 0.040 (0.038)
Train: 67 [ 750/989 ( 76%)]  Loss: 3.25 (2.87)  Time: 0.621s,  206.05/s  (0.668s,  191.63/s)  LR: 6.248e-04  Data: 0.036 (0.038)
Train: 67 [ 800/989 ( 81%)]  Loss: 2.70 (2.86)  Time: 0.729s,  175.52/s  (0.669s,  191.41/s)  LR: 6.248e-04  Data: 0.063 (0.038)
Train: 67 [ 850/989 ( 86%)]  Loss: 2.41 (2.84)  Time: 0.665s,  192.44/s  (0.670s,  191.16/s)  LR: 6.248e-04  Data: 0.026 (0.038)
Train: 67 [ 900/989 ( 91%)]  Loss: 2.73 (2.83)  Time: 0.698s,  183.45/s  (0.671s,  190.88/s)  LR: 6.248e-04  Data: 0.041 (0.038)
Train: 67 [ 950/989 ( 96%)]  Loss: 2.42 (2.81)  Time: 0.663s,  193.03/s  (0.672s,  190.57/s)  LR: 6.248e-04  Data: 0.031 (0.038)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.278 (1.278)  Loss:   1.089 ( 1.089)  Acc@1:  80.469 ( 80.469)  Acc@5:  91.406 ( 91.406)
Test: [  39/39]  Time: 0.050 (0.324)  Loss:   0.757 ( 1.014)  Acc@1:  75.000 ( 77.260)  Acc@5:  87.500 ( 93.220)
Current checkpoints:
 ('./output/train/Upd_Exp3_Image100/checkpoint-67.pth.tar', 77.26)
 ('./output/train/Upd_Exp3_Image100/checkpoint-65.pth.tar', 76.66)
 ('./output/train/Upd_Exp3_Image100/checkpoint-66.pth.tar', 76.66)
 ('./output/train/Upd_Exp3_Image100/checkpoint-64.pth.tar', 76.4)

Train: 68 [   0/989 (  0%)]  Loss: 3.31 (3.31)  Time: 1.834s,   69.78/s  (1.834s,   69.78/s)  LR: 6.224e-04  Data: 1.095 (1.095)
Train: 68 [  50/989 (  5%)]  Loss: 3.09 (3.20)  Time: 0.656s,  194.99/s  (0.687s,  186.45/s)  LR: 6.224e-04  Data: 0.035 (0.057)
Train: 68 [ 100/989 ( 10%)]  Loss: 2.69 (3.03)  Time: 0.644s,  198.70/s  (0.670s,  190.97/s)  LR: 6.224e-04  Data: 0.014 (0.046)
Train: 68 [ 150/989 ( 15%)]  Loss: 2.90 (3.00)  Time: 0.641s,  199.71/s  (0.666s,  192.15/s)  LR: 6.224e-04  Data: 0.041 (0.043)
Train: 68 [ 200/989 ( 20%)]  Loss: 2.12 (2.82)  Time: 0.638s,  200.53/s  (0.665s,  192.56/s)  LR: 6.224e-04  Data: 0.022 (0.041)
Train: 68 [ 250/989 ( 25%)]  Loss: 2.30 (2.74)  Time: 0.655s,  195.46/s  (0.663s,  192.97/s)  LR: 6.224e-04  Data: 0.021 (0.041)
Train: 68 [ 300/989 ( 30%)]  Loss: 3.31 (2.82)  Time: 0.711s,  180.12/s  (0.662s,  193.28/s)  LR: 6.224e-04  Data: 0.024 (0.040)
Train: 68 [ 350/989 ( 35%)]  Loss: 2.64 (2.80)  Time: 0.657s,  194.74/s  (0.663s,  193.17/s)  LR: 6.224e-04  Data: 0.038 (0.040)
Train: 68 [ 400/989 ( 40%)]  Loss: 2.48 (2.76)  Time: 0.655s,  195.55/s  (0.662s,  193.31/s)  LR: 6.224e-04  Data: 0.021 (0.039)
Train: 68 [ 450/989 ( 46%)]  Loss: 3.10 (2.80)  Time: 0.623s,  205.60/s  (0.663s,  192.93/s)  LR: 6.224e-04  Data: 0.013 (0.039)
Train: 68 [ 500/989 ( 51%)]  Loss: 2.91 (2.81)  Time: 0.656s,  195.09/s  (0.663s,  193.07/s)  LR: 6.224e-04  Data: 0.015 (0.038)
Train: 68 [ 550/989 ( 56%)]  Loss: 2.38 (2.77)  Time: 0.656s,  195.00/s  (0.662s,  193.31/s)  LR: 6.224e-04  Data: 0.046 (0.038)
Train: 68 [ 600/989 ( 61%)]  Loss: 3.01 (2.79)  Time: 0.678s,  188.66/s  (0.662s,  193.29/s)  LR: 6.224e-04  Data: 0.039 (0.038)
Train: 68 [ 650/989 ( 66%)]  Loss: 3.09 (2.81)  Time: 0.663s,  193.19/s  (0.662s,  193.29/s)  LR: 6.224e-04  Data: 0.048 (0.038)
Train: 68 [ 700/989 ( 71%)]  Loss: 2.60 (2.80)  Time: 0.631s,  202.91/s  (0.662s,  193.23/s)  LR: 6.224e-04  Data: 0.013 (0.037)
Train: 68 [ 750/989 ( 76%)]  Loss: 3.14 (2.82)  Time: 0.652s,  196.44/s  (0.663s,  193.20/s)  LR: 6.224e-04  Data: 0.046 (0.037)
Train: 68 [ 800/989 ( 81%)]  Loss: 1.84 (2.76)  Time: 0.722s,  177.18/s  (0.663s,  193.07/s)  LR: 6.224e-04  Data: 0.043 (0.037)
Train: 68 [ 850/989 ( 86%)]  Loss: 2.67 (2.76)  Time: 0.685s,  186.99/s  (0.664s,  192.88/s)  LR: 6.224e-04  Data: 0.038 (0.036)
Train: 68 [ 900/989 ( 91%)]  Loss: 2.24 (2.73)  Time: 0.699s,  183.12/s  (0.665s,  192.62/s)  LR: 6.224e-04  Data: 0.040 (0.036)
Train: 68 [ 950/989 ( 96%)]  Loss: 3.33 (2.76)  Time: 0.667s,  191.89/s  (0.666s,  192.30/s)  LR: 6.224e-04  Data: 0.036 (0.036)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.254 (1.254)  Loss:   1.240 ( 1.240)  Acc@1:  74.219 ( 74.219)  Acc@5:  91.406 ( 91.406)
Test: [  39/39]  Time: 0.048 (0.324)  Loss:   0.364 ( 1.048)  Acc@1: 100.000 ( 77.020)  Acc@5: 100.000 ( 93.280)
Current checkpoints:
 ('./output/train/Upd_Exp3_Image100/checkpoint-67.pth.tar', 77.26)
 ('./output/train/Upd_Exp3_Image100/checkpoint-68.pth.tar', 77.02)
 ('./output/train/Upd_Exp3_Image100/checkpoint-65.pth.tar', 76.66)
 ('./output/train/Upd_Exp3_Image100/checkpoint-66.pth.tar', 76.66)
 ('./output/train/Upd_Exp3_Image100/checkpoint-64.pth.tar', 76.4)

Train: 69 [   0/989 (  0%)]  Loss: 3.20 (3.20)  Time: 1.648s,   77.67/s  (1.648s,   77.67/s)  LR: 6.200e-04  Data: 0.930 (0.930)
Train: 69 [  50/989 (  5%)]  Loss: 2.90 (3.05)  Time: 0.623s,  205.32/s  (0.681s,  187.95/s)  LR: 6.200e-04  Data: 0.008 (0.056)
Train: 69 [ 100/989 ( 10%)]  Loss: 2.51 (2.87)  Time: 0.667s,  191.89/s  (0.666s,  192.10/s)  LR: 6.200e-04  Data: 0.049 (0.047)
Train: 69 [ 150/989 ( 15%)]  Loss: 2.44 (2.76)  Time: 0.637s,  200.81/s  (0.662s,  193.41/s)  LR: 6.200e-04  Data: 0.015 (0.042)
Train: 69 [ 200/989 ( 20%)]  Loss: 3.52 (2.91)  Time: 0.669s,  191.28/s  (0.661s,  193.70/s)  LR: 6.200e-04  Data: 0.045 (0.041)
Train: 69 [ 250/989 ( 25%)]  Loss: 3.11 (2.94)  Time: 0.692s,  184.88/s  (0.659s,  194.16/s)  LR: 6.200e-04  Data: 0.015 (0.040)
Train: 69 [ 300/989 ( 30%)]  Loss: 3.20 (2.98)  Time: 0.670s,  190.95/s  (0.659s,  194.37/s)  LR: 6.200e-04  Data: 0.041 (0.039)
Train: 69 [ 350/989 ( 35%)]  Loss: 2.44 (2.91)  Time: 0.711s,  180.04/s  (0.659s,  194.37/s)  LR: 6.200e-04  Data: 0.047 (0.038)
Train: 69 [ 400/989 ( 40%)]  Loss: 3.04 (2.93)  Time: 0.704s,  181.71/s  (0.658s,  194.59/s)  LR: 6.200e-04  Data: 0.043 (0.038)
Train: 69 [ 450/989 ( 46%)]  Loss: 2.73 (2.91)  Time: 0.676s,  189.44/s  (0.658s,  194.56/s)  LR: 6.200e-04  Data: 0.049 (0.037)
Train: 69 [ 500/989 ( 51%)]  Loss: 2.91 (2.91)  Time: 0.663s,  192.94/s  (0.658s,  194.50/s)  LR: 6.200e-04  Data: 0.051 (0.037)
Train: 69 [ 550/989 ( 56%)]  Loss: 2.86 (2.90)  Time: 0.653s,  195.91/s  (0.658s,  194.39/s)  LR: 6.200e-04  Data: 0.038 (0.037)
Train: 69 [ 600/989 ( 61%)]  Loss: 2.98 (2.91)  Time: 0.652s,  196.22/s  (0.658s,  194.43/s)  LR: 6.200e-04  Data: 0.046 (0.037)
Train: 69 [ 650/989 ( 66%)]  Loss: 3.31 (2.94)  Time: 0.659s,  194.13/s  (0.659s,  194.34/s)  LR: 6.200e-04  Data: 0.062 (0.037)
Train: 69 [ 700/989 ( 71%)]  Loss: 3.37 (2.97)  Time: 0.636s,  201.13/s  (0.659s,  194.15/s)  LR: 6.200e-04  Data: 0.046 (0.037)
Train: 69 [ 750/989 ( 76%)]  Loss: 2.88 (2.96)  Time: 0.690s,  185.38/s  (0.660s,  193.97/s)  LR: 6.200e-04  Data: 0.012 (0.037)
Train: 69 [ 800/989 ( 81%)]  Loss: 2.29 (2.92)  Time: 0.657s,  194.68/s  (0.661s,  193.79/s)  LR: 6.200e-04  Data: 0.043 (0.037)
Train: 69 [ 850/989 ( 86%)]  Loss: 2.70 (2.91)  Time: 0.831s,  154.11/s  (0.662s,  193.42/s)  LR: 6.200e-04  Data: 0.038 (0.037)
Train: 69 [ 900/989 ( 91%)]  Loss: 2.24 (2.87)  Time: 0.724s,  176.84/s  (0.663s,  193.08/s)  LR: 6.200e-04  Data: 0.039 (0.037)
Train: 69 [ 950/989 ( 96%)]  Loss: 2.49 (2.86)  Time: 0.594s,  215.35/s  (0.664s,  192.71/s)  LR: 6.200e-04  Data: 0.026 (0.036)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.288 (1.288)  Loss:   1.122 ( 1.122)  Acc@1:  74.219 ( 74.219)  Acc@5:  89.844 ( 89.844)
Test: [  39/39]  Time: 0.045 (0.322)  Loss:   0.369 ( 0.954)  Acc@1: 100.000 ( 78.040)  Acc@5: 100.000 ( 93.980)
Current checkpoints:
 ('./output/train/Upd_Exp3_Image100/checkpoint-69.pth.tar', 78.04)
 ('./output/train/Upd_Exp3_Image100/checkpoint-67.pth.tar', 77.26)
 ('./output/train/Upd_Exp3_Image100/checkpoint-68.pth.tar', 77.02)
 ('./output/train/Upd_Exp3_Image100/checkpoint-65.pth.tar', 76.66)
 ('./output/train/Upd_Exp3_Image100/checkpoint-66.pth.tar', 76.66)
 ('./output/train/Upd_Exp3_Image100/checkpoint-64.pth.tar', 76.4)

Train: 70 [   0/989 (  0%)]  Loss: 2.61 (2.61)  Time: 1.964s,   65.18/s  (1.964s,   65.18/s)  LR: 6.176e-04  Data: 1.126 (1.126)
Train: 70 [  50/989 (  5%)]  Loss: 2.79 (2.70)  Time: 0.721s,  177.49/s  (0.688s,  185.97/s)  LR: 6.176e-04  Data: 0.050 (0.057)
Train: 70 [ 100/989 ( 10%)]  Loss: 3.06 (2.82)  Time: 0.636s,  201.26/s  (0.670s,  191.08/s)  LR: 6.176e-04  Data: 0.019 (0.045)
Train: 70 [ 150/989 ( 15%)]  Loss: 2.60 (2.77)  Time: 0.639s,  200.33/s  (0.664s,  192.71/s)  LR: 6.176e-04  Data: 0.023 (0.042)
Train: 70 [ 200/989 ( 20%)]  Loss: 2.57 (2.73)  Time: 0.661s,  193.57/s  (0.663s,  193.14/s)  LR: 6.176e-04  Data: 0.037 (0.041)
Train: 70 [ 250/989 ( 25%)]  Loss: 2.14 (2.63)  Time: 0.655s,  195.42/s  (0.661s,  193.51/s)  LR: 6.176e-04  Data: 0.039 (0.040)
Train: 70 [ 300/989 ( 30%)]  Loss: 2.90 (2.67)  Time: 0.728s,  175.73/s  (0.660s,  193.93/s)  LR: 6.176e-04  Data: 0.040 (0.039)
Train: 70 [ 350/989 ( 35%)]  Loss: 2.08 (2.60)  Time: 0.660s,  193.86/s  (0.660s,  193.88/s)  LR: 6.176e-04  Data: 0.041 (0.039)
Train: 70 [ 400/989 ( 40%)]  Loss: 2.76 (2.61)  Time: 0.666s,  192.08/s  (0.660s,  193.81/s)  LR: 6.176e-04  Data: 0.030 (0.039)
Train: 70 [ 450/989 ( 46%)]  Loss: 3.58 (2.71)  Time: 0.653s,  196.11/s  (0.661s,  193.73/s)  LR: 6.176e-04  Data: 0.018 (0.038)
Train: 70 [ 500/989 ( 51%)]  Loss: 2.70 (2.71)  Time: 0.647s,  197.82/s  (0.661s,  193.61/s)  LR: 6.176e-04  Data: 0.034 (0.038)
Train: 70 [ 550/989 ( 56%)]  Loss: 2.61 (2.70)  Time: 0.679s,  188.59/s  (0.661s,  193.58/s)  LR: 6.176e-04  Data: 0.046 (0.038)
Train: 70 [ 600/989 ( 61%)]  Loss: 2.74 (2.70)  Time: 0.637s,  200.96/s  (0.661s,  193.59/s)  LR: 6.176e-04  Data: 0.040 (0.038)
Train: 70 [ 650/989 ( 66%)]  Loss: 3.25 (2.74)  Time: 0.667s,  191.99/s  (0.662s,  193.48/s)  LR: 6.176e-04  Data: 0.044 (0.038)
Train: 70 [ 700/989 ( 71%)]  Loss: 2.65 (2.74)  Time: 0.692s,  185.07/s  (0.662s,  193.37/s)  LR: 6.176e-04  Data: 0.038 (0.038)
Train: 70 [ 750/989 ( 76%)]  Loss: 3.34 (2.77)  Time: 0.719s,  178.13/s  (0.662s,  193.25/s)  LR: 6.176e-04  Data: 0.022 (0.038)
Train: 70 [ 800/989 ( 81%)]  Loss: 2.84 (2.78)  Time: 0.634s,  201.79/s  (0.663s,  193.13/s)  LR: 6.176e-04  Data: 0.011 (0.037)
Train: 70 [ 850/989 ( 86%)]  Loss: 2.83 (2.78)  Time: 0.708s,  180.86/s  (0.664s,  192.90/s)  LR: 6.176e-04  Data: 0.048 (0.037)
Train: 70 [ 900/989 ( 91%)]  Loss: 2.63 (2.77)  Time: 0.672s,  190.45/s  (0.665s,  192.53/s)  LR: 6.176e-04  Data: 0.044 (0.036)
Train: 70 [ 950/989 ( 96%)]  Loss: 2.87 (2.78)  Time: 0.717s,  178.52/s  (0.666s,  192.27/s)  LR: 6.176e-04  Data: 0.044 (0.036)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.251 (1.251)  Loss:   0.906 ( 0.906)  Acc@1:  82.031 ( 82.031)  Acc@5:  92.188 ( 92.188)
Test: [  39/39]  Time: 0.065 (0.322)  Loss:   0.377 ( 0.927)  Acc@1:  87.500 ( 78.100)  Acc@5: 100.000 ( 93.620)
Current checkpoints:
 ('./output/train/Upd_Exp3_Image100/checkpoint-70.pth.tar', 78.1)
 ('./output/train/Upd_Exp3_Image100/checkpoint-69.pth.tar', 78.04)
 ('./output/train/Upd_Exp3_Image100/checkpoint-67.pth.tar', 77.26)
 ('./output/train/Upd_Exp3_Image100/checkpoint-68.pth.tar', 77.02)
 ('./output/train/Upd_Exp3_Image100/checkpoint-65.pth.tar', 76.66)
 ('./output/train/Upd_Exp3_Image100/checkpoint-66.pth.tar', 76.66)
 ('./output/train/Upd_Exp3_Image100/checkpoint-64.pth.tar', 76.4)

Train: 71 [   0/989 (  0%)]  Loss: 2.98 (2.98)  Time: 1.867s,   68.56/s  (1.867s,   68.56/s)  LR: 6.151e-04  Data: 1.109 (1.109)
Train: 71 [  50/989 (  5%)]  Loss: 3.02 (3.00)  Time: 0.655s,  195.52/s  (0.685s,  186.84/s)  LR: 6.151e-04  Data: 0.045 (0.055)
Train: 71 [ 100/989 ( 10%)]  Loss: 2.70 (2.90)  Time: 0.665s,  192.63/s  (0.673s,  190.08/s)  LR: 6.151e-04  Data: 0.020 (0.045)
Train: 71 [ 150/989 ( 15%)]  Loss: 3.29 (3.00)  Time: 0.644s,  198.72/s  (0.668s,  191.72/s)  LR: 6.151e-04  Data: 0.039 (0.042)
Train: 71 [ 200/989 ( 20%)]  Loss: 3.20 (3.04)  Time: 0.645s,  198.34/s  (0.666s,  192.16/s)  LR: 6.151e-04  Data: 0.039 (0.040)
Train: 71 [ 250/989 ( 25%)]  Loss: 2.89 (3.01)  Time: 0.683s,  187.43/s  (0.664s,  192.66/s)  LR: 6.151e-04  Data: 0.051 (0.040)
Train: 71 [ 300/989 ( 30%)]  Loss: 3.55 (3.09)  Time: 0.646s,  198.29/s  (0.663s,  193.04/s)  LR: 6.151e-04  Data: 0.036 (0.039)
Train: 71 [ 350/989 ( 35%)]  Loss: 2.10 (2.97)  Time: 0.645s,  198.55/s  (0.662s,  193.40/s)  LR: 6.151e-04  Data: 0.037 (0.038)
Train: 71 [ 400/989 ( 40%)]  Loss: 2.88 (2.96)  Time: 0.630s,  203.19/s  (0.661s,  193.56/s)  LR: 6.151e-04  Data: 0.043 (0.038)
Train: 71 [ 450/989 ( 46%)]  Loss: 2.03 (2.86)  Time: 0.651s,  196.60/s  (0.660s,  193.82/s)  LR: 6.151e-04  Data: 0.042 (0.038)
Train: 71 [ 500/989 ( 51%)]  Loss: 2.77 (2.86)  Time: 0.644s,  198.87/s  (0.660s,  193.90/s)  LR: 6.151e-04  Data: 0.015 (0.037)
Train: 71 [ 550/989 ( 56%)]  Loss: 2.64 (2.84)  Time: 0.682s,  187.74/s  (0.660s,  193.86/s)  LR: 6.151e-04  Data: 0.054 (0.037)
Train: 71 [ 600/989 ( 61%)]  Loss: 3.24 (2.87)  Time: 0.650s,  196.88/s  (0.660s,  193.81/s)  LR: 6.151e-04  Data: 0.043 (0.037)
Train: 71 [ 650/989 ( 66%)]  Loss: 2.69 (2.86)  Time: 0.671s,  190.87/s  (0.660s,  193.89/s)  LR: 6.151e-04  Data: 0.039 (0.037)
Train: 71 [ 700/989 ( 71%)]  Loss: 3.00 (2.87)  Time: 0.661s,  193.73/s  (0.660s,  193.80/s)  LR: 6.151e-04  Data: 0.046 (0.037)
Train: 71 [ 750/989 ( 76%)]  Loss: 2.36 (2.83)  Time: 0.660s,  194.01/s  (0.661s,  193.66/s)  LR: 6.151e-04  Data: 0.014 (0.036)
Train: 71 [ 800/989 ( 81%)]  Loss: 2.33 (2.80)  Time: 0.694s,  184.56/s  (0.661s,  193.56/s)  LR: 6.151e-04  Data: 0.037 (0.036)
Train: 71 [ 850/989 ( 86%)]  Loss: 2.37 (2.78)  Time: 0.671s,  190.62/s  (0.662s,  193.25/s)  LR: 6.151e-04  Data: 0.018 (0.036)
Train: 71 [ 900/989 ( 91%)]  Loss: 2.52 (2.77)  Time: 0.673s,  190.12/s  (0.663s,  192.94/s)  LR: 6.151e-04  Data: 0.054 (0.035)
Train: 71 [ 950/989 ( 96%)]  Loss: 2.19 (2.74)  Time: 0.675s,  189.68/s  (0.665s,  192.53/s)  LR: 6.151e-04  Data: 0.021 (0.035)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.319 (1.319)  Loss:   1.182 ( 1.182)  Acc@1:  75.781 ( 75.781)  Acc@5:  91.406 ( 91.406)
Test: [  39/39]  Time: 0.049 (0.325)  Loss:   0.263 ( 1.010)  Acc@1: 100.000 ( 78.220)  Acc@5: 100.000 ( 93.680)
Current checkpoints:
 ('./output/train/Upd_Exp3_Image100/checkpoint-71.pth.tar', 78.22)
 ('./output/train/Upd_Exp3_Image100/checkpoint-70.pth.tar', 78.1)
 ('./output/train/Upd_Exp3_Image100/checkpoint-69.pth.tar', 78.04)
 ('./output/train/Upd_Exp3_Image100/checkpoint-67.pth.tar', 77.26)
 ('./output/train/Upd_Exp3_Image100/checkpoint-68.pth.tar', 77.02)
 ('./output/train/Upd_Exp3_Image100/checkpoint-65.pth.tar', 76.66)
 ('./output/train/Upd_Exp3_Image100/checkpoint-66.pth.tar', 76.66)
 ('./output/train/Upd_Exp3_Image100/checkpoint-64.pth.tar', 76.4)

Train: 72 [   0/989 (  0%)]  Loss: 2.87 (2.87)  Time: 1.685s,   75.97/s  (1.685s,   75.97/s)  LR: 6.126e-04  Data: 0.967 (0.967)
Train: 72 [  50/989 (  5%)]  Loss: 2.13 (2.50)  Time: 0.634s,  202.01/s  (0.681s,  187.99/s)  LR: 6.126e-04  Data: 0.018 (0.055)
Train: 72 [ 100/989 ( 10%)]  Loss: 3.37 (2.79)  Time: 0.666s,  192.29/s  (0.669s,  191.42/s)  LR: 6.126e-04  Data: 0.028 (0.046)
Train: 72 [ 150/989 ( 15%)]  Loss: 3.15 (2.88)  Time: 0.651s,  196.59/s  (0.664s,  192.65/s)  LR: 6.126e-04  Data: 0.039 (0.043)
Train: 72 [ 200/989 ( 20%)]  Loss: 2.77 (2.86)  Time: 0.641s,  199.73/s  (0.663s,  193.18/s)  LR: 6.126e-04  Data: 0.018 (0.041)
Train: 72 [ 250/989 ( 25%)]  Loss: 3.36 (2.94)  Time: 0.656s,  195.18/s  (0.662s,  193.38/s)  LR: 6.126e-04  Data: 0.039 (0.040)
Train: 72 [ 300/989 ( 30%)]  Loss: 3.10 (2.96)  Time: 0.647s,  197.88/s  (0.663s,  193.16/s)  LR: 6.126e-04  Data: 0.036 (0.040)
Train: 72 [ 350/989 ( 35%)]  Loss: 2.17 (2.86)  Time: 0.679s,  188.48/s  (0.663s,  193.12/s)  LR: 6.126e-04  Data: 0.053 (0.040)
Train: 72 [ 400/989 ( 40%)]  Loss: 3.29 (2.91)  Time: 0.702s,  182.22/s  (0.663s,  193.02/s)  LR: 6.126e-04  Data: 0.044 (0.040)
Train: 72 [ 450/989 ( 46%)]  Loss: 2.68 (2.89)  Time: 0.651s,  196.68/s  (0.664s,  192.83/s)  LR: 6.126e-04  Data: 0.048 (0.039)
Train: 72 [ 500/989 ( 51%)]  Loss: 3.36 (2.93)  Time: 0.644s,  198.89/s  (0.665s,  192.60/s)  LR: 6.126e-04  Data: 0.017 (0.039)
Train: 72 [ 550/989 ( 56%)]  Loss: 3.17 (2.95)  Time: 0.648s,  197.55/s  (0.665s,  192.48/s)  LR: 6.126e-04  Data: 0.016 (0.039)
Train: 72 [ 600/989 ( 61%)]  Loss: 2.02 (2.88)  Time: 0.673s,  190.06/s  (0.666s,  192.34/s)  LR: 6.126e-04  Data: 0.045 (0.039)
Train: 72 [ 650/989 ( 66%)]  Loss: 2.98 (2.89)  Time: 0.662s,  193.29/s  (0.666s,  192.21/s)  LR: 6.126e-04  Data: 0.052 (0.039)
Train: 72 [ 700/989 ( 71%)]  Loss: 2.58 (2.87)  Time: 0.716s,  178.70/s  (0.667s,  192.00/s)  LR: 6.126e-04  Data: 0.020 (0.039)
Train: 72 [ 750/989 ( 76%)]  Loss: 3.36 (2.90)  Time: 0.661s,  193.53/s  (0.667s,  191.82/s)  LR: 6.126e-04  Data: 0.015 (0.039)
Train: 72 [ 800/989 ( 81%)]  Loss: 2.86 (2.90)  Time: 0.662s,  193.35/s  (0.668s,  191.57/s)  LR: 6.126e-04  Data: 0.036 (0.038)
Train: 72 [ 850/989 ( 86%)]  Loss: 1.89 (2.84)  Time: 0.679s,  188.55/s  (0.670s,  191.16/s)  LR: 6.126e-04  Data: 0.022 (0.038)
Train: 72 [ 900/989 ( 91%)]  Loss: 2.45 (2.82)  Time: 0.666s,  192.10/s  (0.671s,  190.85/s)  LR: 6.126e-04  Data: 0.033 (0.037)
Train: 72 [ 950/989 ( 96%)]  Loss: 2.50 (2.80)  Time: 0.668s,  191.55/s  (0.672s,  190.62/s)  LR: 6.126e-04  Data: 0.041 (0.037)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.286 (1.286)  Loss:   1.164 ( 1.164)  Acc@1:  76.562 ( 76.562)  Acc@5:  90.625 ( 90.625)
Test: [  39/39]  Time: 0.052 (0.325)  Loss:   0.333 ( 0.993)  Acc@1:  87.500 ( 77.540)  Acc@5: 100.000 ( 93.760)
Current checkpoints:
 ('./output/train/Upd_Exp3_Image100/checkpoint-71.pth.tar', 78.22)
 ('./output/train/Upd_Exp3_Image100/checkpoint-70.pth.tar', 78.1)
 ('./output/train/Upd_Exp3_Image100/checkpoint-69.pth.tar', 78.04)
 ('./output/train/Upd_Exp3_Image100/checkpoint-72.pth.tar', 77.54)
 ('./output/train/Upd_Exp3_Image100/checkpoint-67.pth.tar', 77.26)
 ('./output/train/Upd_Exp3_Image100/checkpoint-68.pth.tar', 77.02)
 ('./output/train/Upd_Exp3_Image100/checkpoint-65.pth.tar', 76.66)
 ('./output/train/Upd_Exp3_Image100/checkpoint-66.pth.tar', 76.66)
 ('./output/train/Upd_Exp3_Image100/checkpoint-64.pth.tar', 76.4)

Train: 73 [   0/989 (  0%)]  Loss: 2.45 (2.45)  Time: 1.830s,   69.95/s  (1.830s,   69.95/s)  LR: 6.101e-04  Data: 1.045 (1.045)
Train: 73 [  50/989 (  5%)]  Loss: 1.83 (2.14)  Time: 0.641s,  199.63/s  (0.685s,  186.81/s)  LR: 6.101e-04  Data: 0.017 (0.052)
Train: 73 [ 100/989 ( 10%)]  Loss: 3.28 (2.52)  Time: 0.672s,  190.45/s  (0.670s,  191.02/s)  LR: 6.101e-04  Data: 0.017 (0.042)
Train: 73 [ 150/989 ( 15%)]  Loss: 3.55 (2.78)  Time: 0.669s,  191.33/s  (0.665s,  192.50/s)  LR: 6.101e-04  Data: 0.018 (0.039)
Train: 73 [ 200/989 ( 20%)]  Loss: 2.83 (2.79)  Time: 0.630s,  203.12/s  (0.661s,  193.62/s)  LR: 6.101e-04  Data: 0.036 (0.038)
Train: 73 [ 250/989 ( 25%)]  Loss: 2.59 (2.75)  Time: 0.640s,  200.03/s  (0.659s,  194.29/s)  LR: 6.101e-04  Data: 0.034 (0.037)
Train: 73 [ 300/989 ( 30%)]  Loss: 2.91 (2.78)  Time: 0.636s,  201.17/s  (0.657s,  194.82/s)  LR: 6.101e-04  Data: 0.016 (0.036)
Train: 73 [ 350/989 ( 35%)]  Loss: 3.07 (2.81)  Time: 0.603s,  212.38/s  (0.657s,  194.95/s)  LR: 6.101e-04  Data: 0.036 (0.036)
Train: 73 [ 400/989 ( 40%)]  Loss: 2.65 (2.79)  Time: 0.672s,  190.49/s  (0.656s,  195.07/s)  LR: 6.101e-04  Data: 0.044 (0.036)
Train: 73 [ 450/989 ( 46%)]  Loss: 3.20 (2.83)  Time: 0.669s,  191.29/s  (0.657s,  194.86/s)  LR: 6.101e-04  Data: 0.034 (0.035)
Train: 73 [ 500/989 ( 51%)]  Loss: 2.96 (2.85)  Time: 0.635s,  201.65/s  (0.657s,  194.93/s)  LR: 6.101e-04  Data: 0.035 (0.035)
Train: 73 [ 550/989 ( 56%)]  Loss: 3.26 (2.88)  Time: 0.707s,  181.04/s  (0.657s,  194.93/s)  LR: 6.101e-04  Data: 0.041 (0.035)
Train: 73 [ 600/989 ( 61%)]  Loss: 2.70 (2.87)  Time: 0.589s,  217.43/s  (0.657s,  194.92/s)  LR: 6.101e-04  Data: 0.016 (0.035)
Train: 73 [ 650/989 ( 66%)]  Loss: 2.86 (2.87)  Time: 0.686s,  186.56/s  (0.657s,  194.74/s)  LR: 6.101e-04  Data: 0.052 (0.036)
Train: 73 [ 700/989 ( 71%)]  Loss: 3.16 (2.88)  Time: 0.712s,  179.86/s  (0.657s,  194.70/s)  LR: 6.101e-04  Data: 0.039 (0.035)
Train: 73 [ 750/989 ( 76%)]  Loss: 2.67 (2.87)  Time: 0.636s,  201.34/s  (0.658s,  194.54/s)  LR: 6.101e-04  Data: 0.009 (0.035)
Train: 73 [ 800/989 ( 81%)]  Loss: 2.48 (2.85)  Time: 0.665s,  192.39/s  (0.659s,  194.29/s)  LR: 6.101e-04  Data: 0.041 (0.035)
Train: 73 [ 850/989 ( 86%)]  Loss: 2.57 (2.83)  Time: 0.703s,  182.16/s  (0.660s,  194.01/s)  LR: 6.101e-04  Data: 0.042 (0.035)
Train: 73 [ 900/989 ( 91%)]  Loss: 2.11 (2.79)  Time: 0.706s,  181.18/s  (0.661s,  193.67/s)  LR: 6.101e-04  Data: 0.044 (0.035)
Train: 73 [ 950/989 ( 96%)]  Loss: 2.34 (2.77)  Time: 0.726s,  176.25/s  (0.662s,  193.35/s)  LR: 6.101e-04  Data: 0.013 (0.035)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.300 (1.300)  Loss:   1.221 ( 1.221)  Acc@1:  75.000 ( 75.000)  Acc@5:  90.625 ( 90.625)
Test: [  39/39]  Time: 0.047 (0.324)  Loss:   0.289 ( 1.007)  Acc@1: 100.000 ( 77.840)  Acc@5: 100.000 ( 93.400)
Current checkpoints:
 ('./output/train/Upd_Exp3_Image100/checkpoint-71.pth.tar', 78.22)
 ('./output/train/Upd_Exp3_Image100/checkpoint-70.pth.tar', 78.1)
 ('./output/train/Upd_Exp3_Image100/checkpoint-69.pth.tar', 78.04)
 ('./output/train/Upd_Exp3_Image100/checkpoint-73.pth.tar', 77.84)
 ('./output/train/Upd_Exp3_Image100/checkpoint-72.pth.tar', 77.54)
 ('./output/train/Upd_Exp3_Image100/checkpoint-67.pth.tar', 77.26)
 ('./output/train/Upd_Exp3_Image100/checkpoint-68.pth.tar', 77.02)
 ('./output/train/Upd_Exp3_Image100/checkpoint-65.pth.tar', 76.66)
 ('./output/train/Upd_Exp3_Image100/checkpoint-66.pth.tar', 76.66)
 ('./output/train/Upd_Exp3_Image100/checkpoint-64.pth.tar', 76.4)

Train: 74 [   0/989 (  0%)]  Loss: 2.70 (2.70)  Time: 1.966s,   65.11/s  (1.966s,   65.11/s)  LR: 6.076e-04  Data: 1.203 (1.203)
Train: 74 [  50/989 (  5%)]  Loss: 2.79 (2.74)  Time: 0.679s,  188.51/s  (0.684s,  187.19/s)  LR: 6.076e-04  Data: 0.043 (0.054)
Train: 74 [ 100/989 ( 10%)]  Loss: 2.72 (2.74)  Time: 0.670s,  190.94/s  (0.667s,  192.02/s)  LR: 6.076e-04  Data: 0.047 (0.043)
Train: 74 [ 150/989 ( 15%)]  Loss: 2.40 (2.65)  Time: 0.673s,  190.16/s  (0.662s,  193.39/s)  LR: 6.076e-04  Data: 0.022 (0.039)
Train: 74 [ 200/989 ( 20%)]  Loss: 2.81 (2.68)  Time: 0.628s,  203.92/s  (0.659s,  194.36/s)  LR: 6.076e-04  Data: 0.015 (0.036)
Train: 74 [ 250/989 ( 25%)]  Loss: 2.87 (2.71)  Time: 0.672s,  190.42/s  (0.658s,  194.49/s)  LR: 6.076e-04  Data: 0.052 (0.036)
Train: 74 [ 300/989 ( 30%)]  Loss: 3.01 (2.76)  Time: 0.630s,  203.31/s  (0.658s,  194.53/s)  LR: 6.076e-04  Data: 0.014 (0.035)
Train: 74 [ 350/989 ( 35%)]  Loss: 3.07 (2.80)  Time: 0.650s,  196.91/s  (0.658s,  194.62/s)  LR: 6.076e-04  Data: 0.033 (0.034)
Train: 74 [ 400/989 ( 40%)]  Loss: 2.85 (2.80)  Time: 0.654s,  195.73/s  (0.658s,  194.61/s)  LR: 6.076e-04  Data: 0.035 (0.034)
Train: 74 [ 450/989 ( 46%)]  Loss: 2.29 (2.75)  Time: 0.649s,  197.24/s  (0.660s,  194.07/s)  LR: 6.076e-04  Data: 0.036 (0.034)
Train: 74 [ 500/989 ( 51%)]  Loss: 2.87 (2.76)  Time: 0.654s,  195.83/s  (0.660s,  194.02/s)  LR: 6.076e-04  Data: 0.019 (0.034)
Train: 74 [ 550/989 ( 56%)]  Loss: 2.98 (2.78)  Time: 0.655s,  195.32/s  (0.660s,  193.88/s)  LR: 6.076e-04  Data: 0.044 (0.034)
Train: 74 [ 600/989 ( 61%)]  Loss: 3.31 (2.82)  Time: 0.552s,  231.86/s  (0.660s,  193.93/s)  LR: 6.076e-04  Data: 0.034 (0.034)
Train: 74 [ 650/989 ( 66%)]  Loss: 3.29 (2.85)  Time: 0.661s,  193.51/s  (0.662s,  193.46/s)  LR: 6.076e-04  Data: 0.010 (0.034)
Train: 74 [ 700/989 ( 71%)]  Loss: 2.72 (2.85)  Time: 0.657s,  194.93/s  (0.662s,  193.31/s)  LR: 6.076e-04  Data: 0.021 (0.034)
Train: 74 [ 750/989 ( 76%)]  Loss: 2.99 (2.85)  Time: 0.723s,  176.95/s  (0.663s,  193.14/s)  LR: 6.076e-04  Data: 0.052 (0.034)
Train: 74 [ 800/989 ( 81%)]  Loss: 2.98 (2.86)  Time: 0.670s,  191.12/s  (0.664s,  192.81/s)  LR: 6.076e-04  Data: 0.044 (0.034)
Train: 74 [ 850/989 ( 86%)]  Loss: 2.64 (2.85)  Time: 0.716s,  178.88/s  (0.665s,  192.52/s)  LR: 6.076e-04  Data: 0.016 (0.034)
Train: 74 [ 900/989 ( 91%)]  Loss: 3.41 (2.88)  Time: 0.690s,  185.52/s  (0.666s,  192.09/s)  LR: 6.076e-04  Data: 0.032 (0.034)
Train: 74 [ 950/989 ( 96%)]  Loss: 2.72 (2.87)  Time: 0.685s,  186.88/s  (0.667s,  191.81/s)  LR: 6.076e-04  Data: 0.018 (0.034)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.274 (1.274)  Loss:   1.270 ( 1.270)  Acc@1:  76.562 ( 76.562)  Acc@5:  89.062 ( 89.062)
Test: [  39/39]  Time: 0.047 (0.325)  Loss:   0.312 ( 1.044)  Acc@1: 100.000 ( 78.200)  Acc@5: 100.000 ( 94.100)
Current checkpoints:
 ('./output/train/Upd_Exp3_Image100/checkpoint-71.pth.tar', 78.22)
 ('./output/train/Upd_Exp3_Image100/checkpoint-74.pth.tar', 78.2)
 ('./output/train/Upd_Exp3_Image100/checkpoint-70.pth.tar', 78.1)
 ('./output/train/Upd_Exp3_Image100/checkpoint-69.pth.tar', 78.04)
 ('./output/train/Upd_Exp3_Image100/checkpoint-73.pth.tar', 77.84)
 ('./output/train/Upd_Exp3_Image100/checkpoint-72.pth.tar', 77.54)
 ('./output/train/Upd_Exp3_Image100/checkpoint-67.pth.tar', 77.26)
 ('./output/train/Upd_Exp3_Image100/checkpoint-68.pth.tar', 77.02)
 ('./output/train/Upd_Exp3_Image100/checkpoint-65.pth.tar', 76.66)
 ('./output/train/Upd_Exp3_Image100/checkpoint-66.pth.tar', 76.66)

Train: 75 [   0/989 (  0%)]  Loss: 3.11 (3.11)  Time: 1.737s,   73.70/s  (1.737s,   73.70/s)  LR: 6.050e-04  Data: 1.007 (1.007)
Train: 75 [  50/989 (  5%)]  Loss: 2.87 (2.99)  Time: 0.631s,  202.84/s  (0.685s,  186.83/s)  LR: 6.050e-04  Data: 0.016 (0.054)
Train: 75 [ 100/989 ( 10%)]  Loss: 3.05 (3.01)  Time: 0.664s,  192.69/s  (0.669s,  191.43/s)  LR: 6.050e-04  Data: 0.050 (0.044)
Train: 75 [ 150/989 ( 15%)]  Loss: 3.20 (3.06)  Time: 0.662s,  193.47/s  (0.664s,  192.70/s)  LR: 6.050e-04  Data: 0.050 (0.041)
Train: 75 [ 200/989 ( 20%)]  Loss: 2.82 (3.01)  Time: 0.640s,  199.90/s  (0.661s,  193.59/s)  LR: 6.050e-04  Data: 0.015 (0.040)
Train: 75 [ 250/989 ( 25%)]  Loss: 2.66 (2.95)  Time: 0.668s,  191.51/s  (0.660s,  193.99/s)  LR: 6.050e-04  Data: 0.040 (0.040)
Train: 75 [ 300/989 ( 30%)]  Loss: 2.59 (2.90)  Time: 0.633s,  202.09/s  (0.659s,  194.33/s)  LR: 6.050e-04  Data: 0.010 (0.039)
Train: 75 [ 350/989 ( 35%)]  Loss: 3.23 (2.94)  Time: 0.699s,  183.18/s  (0.659s,  194.30/s)  LR: 6.050e-04  Data: 0.044 (0.038)
Train: 75 [ 400/989 ( 40%)]  Loss: 2.79 (2.92)  Time: 0.662s,  193.32/s  (0.659s,  194.18/s)  LR: 6.050e-04  Data: 0.045 (0.038)
Train: 75 [ 450/989 ( 46%)]  Loss: 2.83 (2.91)  Time: 0.677s,  188.98/s  (0.660s,  194.06/s)  LR: 6.050e-04  Data: 0.050 (0.038)
Train: 75 [ 500/989 ( 51%)]  Loss: 2.69 (2.89)  Time: 0.661s,  193.59/s  (0.660s,  193.86/s)  LR: 6.050e-04  Data: 0.046 (0.038)
Train: 75 [ 550/989 ( 56%)]  Loss: 2.35 (2.85)  Time: 0.660s,  193.81/s  (0.660s,  193.85/s)  LR: 6.050e-04  Data: 0.019 (0.038)
Train: 75 [ 600/989 ( 61%)]  Loss: 2.73 (2.84)  Time: 0.673s,  190.07/s  (0.660s,  193.87/s)  LR: 6.050e-04  Data: 0.043 (0.038)
Train: 75 [ 650/989 ( 66%)]  Loss: 2.88 (2.84)  Time: 0.669s,  191.34/s  (0.661s,  193.65/s)  LR: 6.050e-04  Data: 0.022 (0.037)
Train: 75 [ 700/989 ( 71%)]  Loss: 3.04 (2.86)  Time: 0.661s,  193.75/s  (0.662s,  193.46/s)  LR: 6.050e-04  Data: 0.034 (0.037)
Train: 75 [ 750/989 ( 76%)]  Loss: 3.05 (2.87)  Time: 0.681s,  187.94/s  (0.662s,  193.26/s)  LR: 6.050e-04  Data: 0.015 (0.037)
Train: 75 [ 800/989 ( 81%)]  Loss: 2.62 (2.85)  Time: 0.671s,  190.66/s  (0.665s,  192.62/s)  LR: 6.050e-04  Data: 0.034 (0.037)
Train: 75 [ 850/989 ( 86%)]  Loss: 2.84 (2.85)  Time: 0.691s,  185.31/s  (0.665s,  192.39/s)  LR: 6.050e-04  Data: 0.053 (0.036)
Train: 75 [ 900/989 ( 91%)]  Loss: 2.62 (2.84)  Time: 0.707s,  181.02/s  (0.666s,  192.12/s)  LR: 6.050e-04  Data: 0.010 (0.036)
Train: 75 [ 950/989 ( 96%)]  Loss: 3.62 (2.88)  Time: 0.715s,  179.13/s  (0.667s,  191.86/s)  LR: 6.050e-04  Data: 0.054 (0.036)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.274 (1.274)  Loss:   1.313 ( 1.313)  Acc@1:  70.312 ( 70.312)  Acc@5:  89.844 ( 89.844)
Test: [  39/39]  Time: 0.053 (0.325)  Loss:   0.576 ( 0.966)  Acc@1:  87.500 ( 77.580)  Acc@5: 100.000 ( 94.180)
Current checkpoints:
 ('./output/train/Upd_Exp3_Image100/checkpoint-71.pth.tar', 78.22)
 ('./output/train/Upd_Exp3_Image100/checkpoint-74.pth.tar', 78.2)
 ('./output/train/Upd_Exp3_Image100/checkpoint-70.pth.tar', 78.1)
 ('./output/train/Upd_Exp3_Image100/checkpoint-69.pth.tar', 78.04)
 ('./output/train/Upd_Exp3_Image100/checkpoint-73.pth.tar', 77.84)
 ('./output/train/Upd_Exp3_Image100/checkpoint-75.pth.tar', 77.58)
 ('./output/train/Upd_Exp3_Image100/checkpoint-72.pth.tar', 77.54)
 ('./output/train/Upd_Exp3_Image100/checkpoint-67.pth.tar', 77.26)
 ('./output/train/Upd_Exp3_Image100/checkpoint-68.pth.tar', 77.02)
 ('./output/train/Upd_Exp3_Image100/checkpoint-65.pth.tar', 76.66)

Train: 76 [   0/989 (  0%)]  Loss: 3.02 (3.02)  Time: 1.921s,   66.63/s  (1.921s,   66.63/s)  LR: 6.024e-04  Data: 1.187 (1.187)
Train: 76 [  50/989 (  5%)]  Loss: 2.84 (2.93)  Time: 0.648s,  197.53/s  (0.685s,  186.86/s)  LR: 6.024e-04  Data: 0.017 (0.058)
Train: 76 [ 100/989 ( 10%)]  Loss: 2.88 (2.91)  Time: 0.639s,  200.16/s  (0.668s,  191.60/s)  LR: 6.024e-04  Data: 0.038 (0.047)
Train: 76 [ 150/989 ( 15%)]  Loss: 3.44 (3.04)  Time: 0.651s,  196.69/s  (0.663s,  192.96/s)  LR: 6.024e-04  Data: 0.028 (0.043)
Train: 76 [ 200/989 ( 20%)]  Loss: 2.86 (3.01)  Time: 0.642s,  199.31/s  (0.661s,  193.59/s)  LR: 6.024e-04  Data: 0.045 (0.041)
Train: 76 [ 250/989 ( 25%)]  Loss: 3.14 (3.03)  Time: 0.606s,  211.11/s  (0.660s,  193.86/s)  LR: 6.024e-04  Data: 0.016 (0.040)
Train: 76 [ 300/989 ( 30%)]  Loss: 2.50 (2.95)  Time: 0.650s,  196.82/s  (0.659s,  194.17/s)  LR: 6.024e-04  Data: 0.044 (0.039)
Train: 76 [ 350/989 ( 35%)]  Loss: 2.46 (2.89)  Time: 0.679s,  188.38/s  (0.659s,  194.35/s)  LR: 6.024e-04  Data: 0.022 (0.039)
Train: 76 [ 400/989 ( 40%)]  Loss: 2.68 (2.87)  Time: 0.657s,  194.87/s  (0.658s,  194.44/s)  LR: 6.024e-04  Data: 0.021 (0.038)
Train: 76 [ 450/989 ( 46%)]  Loss: 2.46 (2.83)  Time: 0.641s,  199.74/s  (0.659s,  194.25/s)  LR: 6.024e-04  Data: 0.011 (0.038)
Train: 76 [ 500/989 ( 51%)]  Loss: 2.27 (2.78)  Time: 0.652s,  196.17/s  (0.659s,  194.16/s)  LR: 6.024e-04  Data: 0.036 (0.038)
Train: 76 [ 550/989 ( 56%)]  Loss: 2.52 (2.76)  Time: 0.644s,  198.89/s  (0.659s,  194.22/s)  LR: 6.024e-04  Data: 0.015 (0.038)
Train: 76 [ 600/989 ( 61%)]  Loss: 3.12 (2.78)  Time: 0.643s,  199.03/s  (0.660s,  194.05/s)  LR: 6.024e-04  Data: 0.033 (0.038)
Train: 76 [ 650/989 ( 66%)]  Loss: 3.01 (2.80)  Time: 0.638s,  200.69/s  (0.660s,  194.00/s)  LR: 6.024e-04  Data: 0.018 (0.038)
Train: 76 [ 700/989 ( 71%)]  Loss: 2.52 (2.78)  Time: 0.652s,  196.47/s  (0.660s,  193.91/s)  LR: 6.024e-04  Data: 0.037 (0.038)
Train: 76 [ 750/989 ( 76%)]  Loss: 3.22 (2.81)  Time: 0.676s,  189.43/s  (0.661s,  193.75/s)  LR: 6.024e-04  Data: 0.016 (0.038)
Train: 76 [ 800/989 ( 81%)]  Loss: 2.94 (2.82)  Time: 0.665s,  192.44/s  (0.661s,  193.58/s)  LR: 6.024e-04  Data: 0.042 (0.037)
Train: 76 [ 850/989 ( 86%)]  Loss: 2.28 (2.79)  Time: 0.706s,  181.27/s  (0.662s,  193.34/s)  LR: 6.024e-04  Data: 0.011 (0.037)
Train: 76 [ 900/989 ( 91%)]  Loss: 2.75 (2.78)  Time: 0.663s,  193.05/s  (0.663s,  192.98/s)  LR: 6.024e-04  Data: 0.013 (0.037)
Train: 76 [ 950/989 ( 96%)]  Loss: 2.79 (2.79)  Time: 0.684s,  187.17/s  (0.664s,  192.67/s)  LR: 6.024e-04  Data: 0.049 (0.037)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.287 (1.287)  Loss:   1.124 ( 1.124)  Acc@1:  78.125 ( 78.125)  Acc@5:  92.188 ( 92.188)
Test: [  39/39]  Time: 0.049 (0.321)  Loss:   0.355 ( 0.939)  Acc@1:  87.500 ( 78.180)  Acc@5: 100.000 ( 93.940)
Current checkpoints:
 ('./output/train/Upd_Exp3_Image100/checkpoint-71.pth.tar', 78.22)
 ('./output/train/Upd_Exp3_Image100/checkpoint-74.pth.tar', 78.2)
 ('./output/train/Upd_Exp3_Image100/checkpoint-76.pth.tar', 78.18)
 ('./output/train/Upd_Exp3_Image100/checkpoint-70.pth.tar', 78.1)
 ('./output/train/Upd_Exp3_Image100/checkpoint-69.pth.tar', 78.04)
 ('./output/train/Upd_Exp3_Image100/checkpoint-73.pth.tar', 77.84)
 ('./output/train/Upd_Exp3_Image100/checkpoint-75.pth.tar', 77.58)
 ('./output/train/Upd_Exp3_Image100/checkpoint-72.pth.tar', 77.54)
 ('./output/train/Upd_Exp3_Image100/checkpoint-67.pth.tar', 77.26)
 ('./output/train/Upd_Exp3_Image100/checkpoint-68.pth.tar', 77.02)

Train: 77 [   0/989 (  0%)]  Loss: 2.62 (2.62)  Time: 1.674s,   76.48/s  (1.674s,   76.48/s)  LR: 5.998e-04  Data: 0.962 (0.962)
Train: 77 [  50/989 (  5%)]  Loss: 2.37 (2.49)  Time: 0.640s,  199.94/s  (0.676s,  189.26/s)  LR: 5.998e-04  Data: 0.037 (0.054)
Train: 77 [ 100/989 ( 10%)]  Loss: 2.69 (2.56)  Time: 0.632s,  202.62/s  (0.665s,  192.52/s)  LR: 5.998e-04  Data: 0.040 (0.046)
Train: 77 [ 150/989 ( 15%)]  Loss: 2.09 (2.44)  Time: 0.643s,  199.02/s  (0.661s,  193.51/s)  LR: 5.998e-04  Data: 0.043 (0.043)
Train: 77 [ 200/989 ( 20%)]  Loss: 2.40 (2.43)  Time: 0.632s,  202.39/s  (0.659s,  194.24/s)  LR: 5.998e-04  Data: 0.036 (0.042)
Train: 77 [ 250/989 ( 25%)]  Loss: 1.90 (2.34)  Time: 0.637s,  201.08/s  (0.658s,  194.59/s)  LR: 5.998e-04  Data: 0.038 (0.041)
Train: 77 [ 300/989 ( 30%)]  Loss: 2.57 (2.38)  Time: 0.643s,  199.05/s  (0.657s,  194.79/s)  LR: 5.998e-04  Data: 0.038 (0.040)
Train: 77 [ 350/989 ( 35%)]  Loss: 2.77 (2.43)  Time: 0.632s,  202.55/s  (0.657s,  194.87/s)  LR: 5.998e-04  Data: 0.036 (0.039)
Train: 77 [ 400/989 ( 40%)]  Loss: 2.94 (2.48)  Time: 0.661s,  193.63/s  (0.657s,  194.92/s)  LR: 5.998e-04  Data: 0.046 (0.039)
Train: 77 [ 450/989 ( 46%)]  Loss: 2.49 (2.48)  Time: 0.653s,  195.91/s  (0.656s,  195.00/s)  LR: 5.998e-04  Data: 0.047 (0.039)
Train: 77 [ 500/989 ( 51%)]  Loss: 2.22 (2.46)  Time: 0.665s,  192.39/s  (0.656s,  194.98/s)  LR: 5.998e-04  Data: 0.045 (0.039)
Train: 77 [ 550/989 ( 56%)]  Loss: 2.79 (2.49)  Time: 0.645s,  198.49/s  (0.657s,  194.95/s)  LR: 5.998e-04  Data: 0.044 (0.039)
Train: 77 [ 600/989 ( 61%)]  Loss: 2.53 (2.49)  Time: 0.684s,  187.17/s  (0.657s,  194.88/s)  LR: 5.998e-04  Data: 0.044 (0.039)
Train: 77 [ 650/989 ( 66%)]  Loss: 3.08 (2.53)  Time: 0.645s,  198.47/s  (0.657s,  194.72/s)  LR: 5.998e-04  Data: 0.029 (0.039)
Train: 77 [ 700/989 ( 71%)]  Loss: 2.84 (2.55)  Time: 0.666s,  192.17/s  (0.658s,  194.53/s)  LR: 5.998e-04  Data: 0.046 (0.039)
Train: 77 [ 750/989 ( 76%)]  Loss: 2.57 (2.55)  Time: 0.679s,  188.59/s  (0.659s,  194.18/s)  LR: 5.998e-04  Data: 0.043 (0.039)
Train: 77 [ 800/989 ( 81%)]  Loss: 2.68 (2.56)  Time: 0.754s,  169.76/s  (0.660s,  193.87/s)  LR: 5.998e-04  Data: 0.027 (0.038)
Train: 77 [ 850/989 ( 86%)]  Loss: 2.74 (2.57)  Time: 0.693s,  184.65/s  (0.661s,  193.51/s)  LR: 5.998e-04  Data: 0.054 (0.038)
Train: 77 [ 900/989 ( 91%)]  Loss: 2.31 (2.56)  Time: 0.658s,  194.64/s  (0.663s,  193.19/s)  LR: 5.998e-04  Data: 0.014 (0.038)
Train: 77 [ 950/989 ( 96%)]  Loss: 3.18 (2.59)  Time: 0.718s,  178.15/s  (0.664s,  192.87/s)  LR: 5.998e-04  Data: 0.048 (0.037)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.240 (1.240)  Loss:   1.057 ( 1.057)  Acc@1:  80.469 ( 80.469)  Acc@5:  92.969 ( 92.969)
Test: [  39/39]  Time: 0.037 (0.322)  Loss:   0.386 ( 0.921)  Acc@1:  87.500 ( 78.480)  Acc@5: 100.000 ( 94.240)
Current checkpoints:
 ('./output/train/Upd_Exp3_Image100/checkpoint-77.pth.tar', 78.48)
 ('./output/train/Upd_Exp3_Image100/checkpoint-71.pth.tar', 78.22)
 ('./output/train/Upd_Exp3_Image100/checkpoint-74.pth.tar', 78.2)
 ('./output/train/Upd_Exp3_Image100/checkpoint-76.pth.tar', 78.18)
 ('./output/train/Upd_Exp3_Image100/checkpoint-70.pth.tar', 78.1)
 ('./output/train/Upd_Exp3_Image100/checkpoint-69.pth.tar', 78.04)
 ('./output/train/Upd_Exp3_Image100/checkpoint-73.pth.tar', 77.84)
 ('./output/train/Upd_Exp3_Image100/checkpoint-75.pth.tar', 77.58)
 ('./output/train/Upd_Exp3_Image100/checkpoint-72.pth.tar', 77.54)
 ('./output/train/Upd_Exp3_Image100/checkpoint-67.pth.tar', 77.26)

Train: 78 [   0/989 (  0%)]  Loss: 2.07 (2.07)  Time: 2.101s,   60.92/s  (2.101s,   60.92/s)  LR: 5.972e-04  Data: 1.380 (1.380)
Train: 78 [  50/989 (  5%)]  Loss: 2.76 (2.41)  Time: 0.655s,  195.47/s  (0.683s,  187.42/s)  LR: 5.972e-04  Data: 0.047 (0.064)
Train: 78 [ 100/989 ( 10%)]  Loss: 2.78 (2.53)  Time: 0.647s,  197.79/s  (0.666s,  192.12/s)  LR: 5.972e-04  Data: 0.045 (0.050)
Train: 78 [ 150/989 ( 15%)]  Loss: 3.15 (2.69)  Time: 0.722s,  177.41/s  (0.663s,  192.98/s)  LR: 5.972e-04  Data: 0.049 (0.045)
Train: 78 [ 200/989 ( 20%)]  Loss: 2.17 (2.58)  Time: 0.671s,  190.89/s  (0.661s,  193.61/s)  LR: 5.972e-04  Data: 0.028 (0.043)
Train: 78 [ 250/989 ( 25%)]  Loss: 2.79 (2.62)  Time: 0.634s,  202.04/s  (0.660s,  194.04/s)  LR: 5.972e-04  Data: 0.038 (0.042)
Train: 78 [ 300/989 ( 30%)]  Loss: 2.73 (2.64)  Time: 0.656s,  195.03/s  (0.658s,  194.43/s)  LR: 5.972e-04  Data: 0.054 (0.041)
Train: 78 [ 350/989 ( 35%)]  Loss: 2.50 (2.62)  Time: 0.656s,  195.22/s  (0.658s,  194.53/s)  LR: 5.972e-04  Data: 0.044 (0.041)
Train: 78 [ 400/989 ( 40%)]  Loss: 3.35 (2.70)  Time: 0.642s,  199.31/s  (0.658s,  194.46/s)  LR: 5.972e-04  Data: 0.038 (0.040)
Train: 78 [ 450/989 ( 46%)]  Loss: 2.06 (2.64)  Time: 0.654s,  195.71/s  (0.658s,  194.60/s)  LR: 5.972e-04  Data: 0.035 (0.040)
Train: 78 [ 500/989 ( 51%)]  Loss: 2.74 (2.65)  Time: 0.683s,  187.51/s  (0.658s,  194.45/s)  LR: 5.972e-04  Data: 0.023 (0.040)
Train: 78 [ 550/989 ( 56%)]  Loss: 2.94 (2.67)  Time: 0.659s,  194.30/s  (0.658s,  194.53/s)  LR: 5.972e-04  Data: 0.043 (0.040)
Train: 78 [ 600/989 ( 61%)]  Loss: 2.23 (2.64)  Time: 0.658s,  194.57/s  (0.658s,  194.44/s)  LR: 5.972e-04  Data: 0.052 (0.039)
Train: 78 [ 650/989 ( 66%)]  Loss: 2.32 (2.61)  Time: 0.650s,  196.91/s  (0.659s,  194.26/s)  LR: 5.972e-04  Data: 0.036 (0.039)
Train: 78 [ 700/989 ( 71%)]  Loss: 2.19 (2.59)  Time: 0.631s,  202.88/s  (0.660s,  194.06/s)  LR: 5.972e-04  Data: 0.043 (0.039)
Train: 78 [ 750/989 ( 76%)]  Loss: 2.13 (2.56)  Time: 0.679s,  188.60/s  (0.661s,  193.78/s)  LR: 5.972e-04  Data: 0.049 (0.039)
Train: 78 [ 800/989 ( 81%)]  Loss: 2.95 (2.58)  Time: 0.626s,  204.53/s  (0.661s,  193.55/s)  LR: 5.972e-04  Data: 0.019 (0.039)
Train: 78 [ 850/989 ( 86%)]  Loss: 2.77 (2.59)  Time: 0.666s,  192.33/s  (0.662s,  193.35/s)  LR: 5.972e-04  Data: 0.023 (0.038)
Train: 78 [ 900/989 ( 91%)]  Loss: 1.81 (2.55)  Time: 0.676s,  189.47/s  (0.663s,  193.09/s)  LR: 5.972e-04  Data: 0.041 (0.038)
Train: 78 [ 950/989 ( 96%)]  Loss: 3.52 (2.60)  Time: 0.601s,  213.03/s  (0.664s,  192.86/s)  LR: 5.972e-04  Data: 0.046 (0.037)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.293 (1.293)  Loss:   1.201 ( 1.201)  Acc@1:  74.219 ( 74.219)  Acc@5:  92.188 ( 92.188)
Test: [  39/39]  Time: 0.049 (0.324)  Loss:   0.436 ( 1.083)  Acc@1: 100.000 ( 77.920)  Acc@5: 100.000 ( 94.120)
Current checkpoints:
 ('./output/train/Upd_Exp3_Image100/checkpoint-77.pth.tar', 78.48)
 ('./output/train/Upd_Exp3_Image100/checkpoint-71.pth.tar', 78.22)
 ('./output/train/Upd_Exp3_Image100/checkpoint-74.pth.tar', 78.2)
 ('./output/train/Upd_Exp3_Image100/checkpoint-76.pth.tar', 78.18)
 ('./output/train/Upd_Exp3_Image100/checkpoint-70.pth.tar', 78.1)
 ('./output/train/Upd_Exp3_Image100/checkpoint-69.pth.tar', 78.04)
 ('./output/train/Upd_Exp3_Image100/checkpoint-78.pth.tar', 77.92)
 ('./output/train/Upd_Exp3_Image100/checkpoint-73.pth.tar', 77.84)
 ('./output/train/Upd_Exp3_Image100/checkpoint-75.pth.tar', 77.58)
 ('./output/train/Upd_Exp3_Image100/checkpoint-72.pth.tar', 77.54)

Train: 79 [   0/989 (  0%)]  Loss: 2.47 (2.47)  Time: 1.935s,   66.16/s  (1.935s,   66.16/s)  LR: 5.945e-04  Data: 1.213 (1.213)
Train: 79 [  50/989 (  5%)]  Loss: 2.67 (2.57)  Time: 0.651s,  196.50/s  (0.688s,  186.03/s)  LR: 5.945e-04  Data: 0.045 (0.060)
Train: 79 [ 100/989 ( 10%)]  Loss: 2.18 (2.44)  Time: 0.590s,  216.82/s  (0.681s,  187.88/s)  LR: 5.945e-04  Data: 0.021 (0.048)
Train: 79 [ 150/989 ( 15%)]  Loss: 3.29 (2.65)  Time: 0.617s,  207.34/s  (0.672s,  190.40/s)  LR: 5.945e-04  Data: 0.005 (0.044)
Train: 79 [ 200/989 ( 20%)]  Loss: 2.34 (2.59)  Time: 0.640s,  199.89/s  (0.668s,  191.73/s)  LR: 5.945e-04  Data: 0.021 (0.043)
Train: 79 [ 250/989 ( 25%)]  Loss: 2.78 (2.62)  Time: 0.652s,  196.27/s  (0.665s,  192.38/s)  LR: 5.945e-04  Data: 0.052 (0.042)
Train: 79 [ 300/989 ( 30%)]  Loss: 2.00 (2.53)  Time: 0.649s,  197.38/s  (0.664s,  192.77/s)  LR: 5.945e-04  Data: 0.016 (0.041)
Train: 79 [ 350/989 ( 35%)]  Loss: 2.33 (2.51)  Time: 0.635s,  201.52/s  (0.663s,  193.19/s)  LR: 5.945e-04  Data: 0.006 (0.040)
Train: 79 [ 400/989 ( 40%)]  Loss: 3.37 (2.60)  Time: 0.653s,  196.10/s  (0.662s,  193.42/s)  LR: 5.945e-04  Data: 0.049 (0.040)
Train: 79 [ 450/989 ( 46%)]  Loss: 2.45 (2.59)  Time: 0.645s,  198.42/s  (0.661s,  193.72/s)  LR: 5.945e-04  Data: 0.041 (0.040)
Train: 79 [ 500/989 ( 51%)]  Loss: 2.62 (2.59)  Time: 0.649s,  197.30/s  (0.660s,  193.82/s)  LR: 5.945e-04  Data: 0.048 (0.039)
Train: 79 [ 550/989 ( 56%)]  Loss: 2.81 (2.61)  Time: 0.638s,  200.48/s  (0.660s,  193.86/s)  LR: 5.945e-04  Data: 0.013 (0.039)
Train: 79 [ 600/989 ( 61%)]  Loss: 2.54 (2.61)  Time: 0.668s,  191.75/s  (0.660s,  193.83/s)  LR: 5.945e-04  Data: 0.040 (0.039)
Train: 79 [ 650/989 ( 66%)]  Loss: 2.49 (2.60)  Time: 0.665s,  192.37/s  (0.661s,  193.70/s)  LR: 5.945e-04  Data: 0.046 (0.039)
Train: 79 [ 700/989 ( 71%)]  Loss: 3.15 (2.63)  Time: 0.648s,  197.52/s  (0.661s,  193.55/s)  LR: 5.945e-04  Data: 0.034 (0.039)
Train: 79 [ 750/989 ( 76%)]  Loss: 3.19 (2.67)  Time: 0.657s,  194.92/s  (0.662s,  193.43/s)  LR: 5.945e-04  Data: 0.040 (0.039)
Train: 79 [ 800/989 ( 81%)]  Loss: 3.60 (2.72)  Time: 0.635s,  201.48/s  (0.662s,  193.31/s)  LR: 5.945e-04  Data: 0.018 (0.039)
Train: 79 [ 850/989 ( 86%)]  Loss: 2.05 (2.69)  Time: 0.571s,  224.00/s  (0.664s,  192.91/s)  LR: 5.945e-04  Data: 0.049 (0.038)
Train: 79 [ 900/989 ( 91%)]  Loss: 3.28 (2.72)  Time: 0.644s,  198.74/s  (0.665s,  192.60/s)  LR: 5.945e-04  Data: 0.015 (0.038)
Train: 79 [ 950/989 ( 96%)]  Loss: 3.31 (2.75)  Time: 0.707s,  180.96/s  (0.666s,  192.16/s)  LR: 5.945e-04  Data: 0.051 (0.038)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.237 (1.237)  Loss:   1.090 ( 1.090)  Acc@1:  77.344 ( 77.344)  Acc@5:  91.406 ( 91.406)
Test: [  39/39]  Time: 0.049 (0.322)  Loss:   0.453 ( 0.953)  Acc@1:  87.500 ( 78.320)  Acc@5: 100.000 ( 94.180)
Current checkpoints:
 ('./output/train/Upd_Exp3_Image100/checkpoint-77.pth.tar', 78.48)
 ('./output/train/Upd_Exp3_Image100/checkpoint-79.pth.tar', 78.32)
 ('./output/train/Upd_Exp3_Image100/checkpoint-71.pth.tar', 78.22)
 ('./output/train/Upd_Exp3_Image100/checkpoint-74.pth.tar', 78.2)
 ('./output/train/Upd_Exp3_Image100/checkpoint-76.pth.tar', 78.18)
 ('./output/train/Upd_Exp3_Image100/checkpoint-70.pth.tar', 78.1)
 ('./output/train/Upd_Exp3_Image100/checkpoint-69.pth.tar', 78.04)
 ('./output/train/Upd_Exp3_Image100/checkpoint-78.pth.tar', 77.92)
 ('./output/train/Upd_Exp3_Image100/checkpoint-73.pth.tar', 77.84)
 ('./output/train/Upd_Exp3_Image100/checkpoint-75.pth.tar', 77.58)

Train: 80 [   0/989 (  0%)]  Loss: 2.62 (2.62)  Time: 2.172s,   58.94/s  (2.172s,   58.94/s)  LR: 5.918e-04  Data: 1.357 (1.357)
Train: 80 [  50/989 (  5%)]  Loss: 2.93 (2.78)  Time: 0.652s,  196.35/s  (0.693s,  184.74/s)  LR: 5.918e-04  Data: 0.035 (0.062)
Train: 80 [ 100/989 ( 10%)]  Loss: 3.11 (2.89)  Time: 0.667s,  191.76/s  (0.676s,  189.42/s)  LR: 5.918e-04  Data: 0.047 (0.049)
Train: 80 [ 150/989 ( 15%)]  Loss: 3.03 (2.92)  Time: 0.667s,  191.90/s  (0.669s,  191.43/s)  LR: 5.918e-04  Data: 0.027 (0.045)
Train: 80 [ 200/989 ( 20%)]  Loss: 3.43 (3.02)  Time: 0.653s,  195.93/s  (0.665s,  192.36/s)  LR: 5.918e-04  Data: 0.047 (0.043)
Train: 80 [ 250/989 ( 25%)]  Loss: 2.15 (2.88)  Time: 0.640s,  200.03/s  (0.663s,  193.10/s)  LR: 5.918e-04  Data: 0.018 (0.042)
Train: 80 [ 300/989 ( 30%)]  Loss: 3.17 (2.92)  Time: 0.662s,  193.42/s  (0.662s,  193.41/s)  LR: 5.918e-04  Data: 0.040 (0.041)
Train: 80 [ 350/989 ( 35%)]  Loss: 3.50 (2.99)  Time: 0.663s,  193.01/s  (0.661s,  193.68/s)  LR: 5.918e-04  Data: 0.026 (0.040)
Train: 80 [ 400/989 ( 40%)]  Loss: 2.62 (2.95)  Time: 0.659s,  194.23/s  (0.660s,  193.89/s)  LR: 5.918e-04  Data: 0.034 (0.040)
Train: 80 [ 450/989 ( 46%)]  Loss: 2.99 (2.96)  Time: 0.646s,  198.27/s  (0.660s,  194.08/s)  LR: 5.918e-04  Data: 0.035 (0.040)
Train: 80 [ 500/989 ( 51%)]  Loss: 3.27 (2.98)  Time: 0.707s,  181.07/s  (0.660s,  194.08/s)  LR: 5.918e-04  Data: 0.046 (0.040)
Train: 80 [ 550/989 ( 56%)]  Loss: 2.48 (2.94)  Time: 0.700s,  182.82/s  (0.659s,  194.11/s)  LR: 5.918e-04  Data: 0.047 (0.039)
Train: 80 [ 600/989 ( 61%)]  Loss: 2.16 (2.88)  Time: 0.663s,  192.92/s  (0.660s,  194.07/s)  LR: 5.918e-04  Data: 0.038 (0.039)
Train: 80 [ 650/989 ( 66%)]  Loss: 3.02 (2.89)  Time: 0.634s,  202.04/s  (0.660s,  193.89/s)  LR: 5.918e-04  Data: 0.022 (0.039)
Train: 80 [ 700/989 ( 71%)]  Loss: 3.16 (2.91)  Time: 0.660s,  193.91/s  (0.660s,  193.84/s)  LR: 5.918e-04  Data: 0.022 (0.039)
Train: 80 [ 750/989 ( 76%)]  Loss: 2.73 (2.90)  Time: 0.620s,  206.46/s  (0.661s,  193.76/s)  LR: 5.918e-04  Data: 0.016 (0.039)
Train: 80 [ 800/989 ( 81%)]  Loss: 3.41 (2.93)  Time: 0.660s,  193.84/s  (0.661s,  193.51/s)  LR: 5.918e-04  Data: 0.042 (0.038)
Train: 80 [ 850/989 ( 86%)]  Loss: 3.44 (2.96)  Time: 0.683s,  187.52/s  (0.663s,  193.19/s)  LR: 5.918e-04  Data: 0.016 (0.038)
Train: 80 [ 900/989 ( 91%)]  Loss: 2.84 (2.95)  Time: 0.765s,  167.22/s  (0.664s,  192.86/s)  LR: 5.918e-04  Data: 0.025 (0.038)
Train: 80 [ 950/989 ( 96%)]  Loss: 2.84 (2.95)  Time: 0.642s,  199.49/s  (0.665s,  192.51/s)  LR: 5.918e-04  Data: 0.034 (0.038)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.269 (1.269)  Loss:   1.015 ( 1.015)  Acc@1:  79.688 ( 79.688)  Acc@5:  92.188 ( 92.188)
Test: [  39/39]  Time: 0.054 (0.323)  Loss:   0.430 ( 0.909)  Acc@1:  87.500 ( 79.280)  Acc@5: 100.000 ( 94.600)
Current checkpoints:
 ('./output/train/Upd_Exp3_Image100/checkpoint-80.pth.tar', 79.28)
 ('./output/train/Upd_Exp3_Image100/checkpoint-77.pth.tar', 78.48)
 ('./output/train/Upd_Exp3_Image100/checkpoint-79.pth.tar', 78.32)
 ('./output/train/Upd_Exp3_Image100/checkpoint-71.pth.tar', 78.22)
 ('./output/train/Upd_Exp3_Image100/checkpoint-74.pth.tar', 78.2)
 ('./output/train/Upd_Exp3_Image100/checkpoint-76.pth.tar', 78.18)
 ('./output/train/Upd_Exp3_Image100/checkpoint-70.pth.tar', 78.1)
 ('./output/train/Upd_Exp3_Image100/checkpoint-69.pth.tar', 78.04)
 ('./output/train/Upd_Exp3_Image100/checkpoint-78.pth.tar', 77.92)
 ('./output/train/Upd_Exp3_Image100/checkpoint-73.pth.tar', 77.84)

Train: 81 [   0/989 (  0%)]  Loss: 2.23 (2.23)  Time: 1.996s,   64.12/s  (1.996s,   64.12/s)  LR: 5.891e-04  Data: 1.281 (1.281)
Train: 81 [  50/989 (  5%)]  Loss: 2.38 (2.31)  Time: 0.633s,  202.12/s  (0.687s,  186.29/s)  LR: 5.891e-04  Data: 0.015 (0.062)
Train: 81 [ 100/989 ( 10%)]  Loss: 2.65 (2.42)  Time: 0.649s,  197.32/s  (0.668s,  191.51/s)  LR: 5.891e-04  Data: 0.020 (0.049)
Train: 81 [ 150/989 ( 15%)]  Loss: 2.24 (2.37)  Time: 0.688s,  186.18/s  (0.663s,  192.94/s)  LR: 5.891e-04  Data: 0.051 (0.044)
Train: 81 [ 200/989 ( 20%)]  Loss: 2.50 (2.40)  Time: 0.656s,  195.26/s  (0.661s,  193.66/s)  LR: 5.891e-04  Data: 0.055 (0.042)
Train: 81 [ 250/989 ( 25%)]  Loss: 2.97 (2.49)  Time: 0.562s,  227.66/s  (0.660s,  193.85/s)  LR: 5.891e-04  Data: 0.035 (0.041)
Train: 81 [ 300/989 ( 30%)]  Loss: 2.60 (2.51)  Time: 0.655s,  195.30/s  (0.659s,  194.20/s)  LR: 5.891e-04  Data: 0.022 (0.040)
Train: 81 [ 350/989 ( 35%)]  Loss: 2.91 (2.56)  Time: 0.641s,  199.81/s  (0.658s,  194.58/s)  LR: 5.891e-04  Data: 0.044 (0.040)
Train: 81 [ 400/989 ( 40%)]  Loss: 2.82 (2.59)  Time: 0.703s,  181.98/s  (0.658s,  194.55/s)  LR: 5.891e-04  Data: 0.046 (0.039)
Train: 81 [ 450/989 ( 46%)]  Loss: 2.41 (2.57)  Time: 0.654s,  195.74/s  (0.658s,  194.40/s)  LR: 5.891e-04  Data: 0.033 (0.039)
Train: 81 [ 500/989 ( 51%)]  Loss: 2.26 (2.54)  Time: 0.649s,  197.25/s  (0.659s,  194.32/s)  LR: 5.891e-04  Data: 0.050 (0.038)
Train: 81 [ 550/989 ( 56%)]  Loss: 2.98 (2.58)  Time: 0.662s,  193.30/s  (0.659s,  194.34/s)  LR: 5.891e-04  Data: 0.044 (0.039)
Train: 81 [ 600/989 ( 61%)]  Loss: 2.94 (2.61)  Time: 0.649s,  197.12/s  (0.658s,  194.42/s)  LR: 5.891e-04  Data: 0.039 (0.038)
Train: 81 [ 650/989 ( 66%)]  Loss: 2.64 (2.61)  Time: 0.661s,  193.66/s  (0.659s,  194.36/s)  LR: 5.891e-04  Data: 0.045 (0.038)
Train: 81 [ 700/989 ( 71%)]  Loss: 2.78 (2.62)  Time: 0.644s,  198.76/s  (0.659s,  194.26/s)  LR: 5.891e-04  Data: 0.019 (0.038)
Train: 81 [ 750/989 ( 76%)]  Loss: 3.26 (2.66)  Time: 0.633s,  202.19/s  (0.660s,  194.08/s)  LR: 5.891e-04  Data: 0.032 (0.038)
Train: 81 [ 800/989 ( 81%)]  Loss: 3.47 (2.71)  Time: 0.647s,  197.78/s  (0.660s,  193.94/s)  LR: 5.891e-04  Data: 0.034 (0.038)
Train: 81 [ 850/989 ( 86%)]  Loss: 2.73 (2.71)  Time: 0.725s,  176.57/s  (0.661s,  193.62/s)  LR: 5.891e-04  Data: 0.035 (0.038)
Train: 81 [ 900/989 ( 91%)]  Loss: 3.07 (2.73)  Time: 0.685s,  186.91/s  (0.662s,  193.28/s)  LR: 5.891e-04  Data: 0.050 (0.037)
Train: 81 [ 950/989 ( 96%)]  Loss: 2.62 (2.72)  Time: 0.680s,  188.25/s  (0.663s,  192.97/s)  LR: 5.891e-04  Data: 0.017 (0.037)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.257 (1.257)  Loss:   0.845 ( 0.845)  Acc@1:  83.594 ( 83.594)  Acc@5:  93.750 ( 93.750)
Test: [  39/39]  Time: 0.049 (0.321)  Loss:   0.605 ( 0.969)  Acc@1:  87.500 ( 78.000)  Acc@5: 100.000 ( 94.340)
Current checkpoints:
 ('./output/train/Upd_Exp3_Image100/checkpoint-80.pth.tar', 79.28)
 ('./output/train/Upd_Exp3_Image100/checkpoint-77.pth.tar', 78.48)
 ('./output/train/Upd_Exp3_Image100/checkpoint-79.pth.tar', 78.32)
 ('./output/train/Upd_Exp3_Image100/checkpoint-71.pth.tar', 78.22)
 ('./output/train/Upd_Exp3_Image100/checkpoint-74.pth.tar', 78.2)
 ('./output/train/Upd_Exp3_Image100/checkpoint-76.pth.tar', 78.18)
 ('./output/train/Upd_Exp3_Image100/checkpoint-70.pth.tar', 78.1)
 ('./output/train/Upd_Exp3_Image100/checkpoint-69.pth.tar', 78.04)
 ('./output/train/Upd_Exp3_Image100/checkpoint-81.pth.tar', 78.0)
 ('./output/train/Upd_Exp3_Image100/checkpoint-78.pth.tar', 77.92)

Train: 82 [   0/989 (  0%)]  Loss: 2.22 (2.22)  Time: 1.625s,   78.77/s  (1.625s,   78.77/s)  LR: 5.863e-04  Data: 0.865 (0.865)
Train: 82 [  50/989 (  5%)]  Loss: 2.91 (2.57)  Time: 0.644s,  198.85/s  (0.680s,  188.13/s)  LR: 5.863e-04  Data: 0.022 (0.055)
Train: 82 [ 100/989 ( 10%)]  Loss: 3.28 (2.80)  Time: 0.644s,  198.66/s  (0.666s,  192.24/s)  LR: 5.863e-04  Data: 0.056 (0.045)
Train: 82 [ 150/989 ( 15%)]  Loss: 2.17 (2.65)  Time: 0.656s,  195.05/s  (0.662s,  193.42/s)  LR: 5.863e-04  Data: 0.042 (0.042)
Train: 82 [ 200/989 ( 20%)]  Loss: 3.10 (2.74)  Time: 0.676s,  189.36/s  (0.660s,  194.00/s)  LR: 5.863e-04  Data: 0.044 (0.041)
Train: 82 [ 250/989 ( 25%)]  Loss: 2.68 (2.73)  Time: 0.661s,  193.73/s  (0.659s,  194.33/s)  LR: 5.863e-04  Data: 0.026 (0.041)
Train: 82 [ 300/989 ( 30%)]  Loss: 2.56 (2.70)  Time: 0.620s,  206.38/s  (0.658s,  194.55/s)  LR: 5.863e-04  Data: 0.054 (0.040)
Train: 82 [ 350/989 ( 35%)]  Loss: 2.53 (2.68)  Time: 0.643s,  199.01/s  (0.657s,  194.71/s)  LR: 5.863e-04  Data: 0.039 (0.040)
Train: 82 [ 400/989 ( 40%)]  Loss: 2.66 (2.68)  Time: 0.644s,  198.69/s  (0.657s,  194.76/s)  LR: 5.863e-04  Data: 0.038 (0.040)
Train: 82 [ 450/989 ( 46%)]  Loss: 3.43 (2.75)  Time: 0.638s,  200.50/s  (0.657s,  194.73/s)  LR: 5.863e-04  Data: 0.016 (0.040)
Train: 82 [ 500/989 ( 51%)]  Loss: 2.59 (2.74)  Time: 0.643s,  199.06/s  (0.657s,  194.74/s)  LR: 5.863e-04  Data: 0.042 (0.040)
Train: 82 [ 550/989 ( 56%)]  Loss: 2.10 (2.69)  Time: 0.661s,  193.64/s  (0.657s,  194.79/s)  LR: 5.863e-04  Data: 0.052 (0.039)
Train: 82 [ 600/989 ( 61%)]  Loss: 2.26 (2.65)  Time: 0.657s,  194.79/s  (0.657s,  194.71/s)  LR: 5.863e-04  Data: 0.018 (0.039)
Train: 82 [ 650/989 ( 66%)]  Loss: 3.12 (2.69)  Time: 0.621s,  206.05/s  (0.658s,  194.61/s)  LR: 5.863e-04  Data: 0.036 (0.039)
Train: 82 [ 700/989 ( 71%)]  Loss: 2.87 (2.70)  Time: 0.663s,  193.06/s  (0.658s,  194.50/s)  LR: 5.863e-04  Data: 0.039 (0.039)
Train: 82 [ 750/989 ( 76%)]  Loss: 3.06 (2.72)  Time: 0.614s,  208.41/s  (0.659s,  194.37/s)  LR: 5.863e-04  Data: 0.025 (0.038)
Train: 82 [ 800/989 ( 81%)]  Loss: 2.26 (2.69)  Time: 0.682s,  187.63/s  (0.660s,  194.08/s)  LR: 5.863e-04  Data: 0.036 (0.038)
Train: 82 [ 850/989 ( 86%)]  Loss: 2.13 (2.66)  Time: 0.670s,  190.96/s  (0.661s,  193.77/s)  LR: 5.863e-04  Data: 0.043 (0.038)
Train: 82 [ 900/989 ( 91%)]  Loss: 2.95 (2.68)  Time: 0.690s,  185.53/s  (0.662s,  193.39/s)  LR: 5.863e-04  Data: 0.043 (0.038)
Train: 82 [ 950/989 ( 96%)]  Loss: 2.93 (2.69)  Time: 0.668s,  191.69/s  (0.663s,  192.95/s)  LR: 5.863e-04  Data: 0.010 (0.038)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.398 (1.398)  Loss:   0.901 ( 0.901)  Acc@1:  81.250 ( 81.250)  Acc@5:  92.969 ( 92.969)
Test: [  39/39]  Time: 0.049 (0.324)  Loss:   0.467 ( 0.887)  Acc@1:  87.500 ( 78.880)  Acc@5: 100.000 ( 94.300)
Current checkpoints:
 ('./output/train/Upd_Exp3_Image100/checkpoint-80.pth.tar', 79.28)
 ('./output/train/Upd_Exp3_Image100/checkpoint-82.pth.tar', 78.88)
 ('./output/train/Upd_Exp3_Image100/checkpoint-77.pth.tar', 78.48)
 ('./output/train/Upd_Exp3_Image100/checkpoint-79.pth.tar', 78.32)
 ('./output/train/Upd_Exp3_Image100/checkpoint-71.pth.tar', 78.22)
 ('./output/train/Upd_Exp3_Image100/checkpoint-74.pth.tar', 78.2)
 ('./output/train/Upd_Exp3_Image100/checkpoint-76.pth.tar', 78.18)
 ('./output/train/Upd_Exp3_Image100/checkpoint-70.pth.tar', 78.1)
 ('./output/train/Upd_Exp3_Image100/checkpoint-69.pth.tar', 78.04)
 ('./output/train/Upd_Exp3_Image100/checkpoint-81.pth.tar', 78.0)

Train: 83 [   0/989 (  0%)]  Loss: 2.02 (2.02)  Time: 1.776s,   72.09/s  (1.776s,   72.09/s)  LR: 5.835e-04  Data: 1.006 (1.006)
Train: 83 [  50/989 (  5%)]  Loss: 3.19 (2.60)  Time: 0.702s,  182.34/s  (0.679s,  188.44/s)  LR: 5.835e-04  Data: 0.039 (0.055)
Train: 83 [ 100/989 ( 10%)]  Loss: 2.98 (2.73)  Time: 0.639s,  200.45/s  (0.664s,  192.70/s)  LR: 5.835e-04  Data: 0.019 (0.045)
Train: 83 [ 150/989 ( 15%)]  Loss: 2.87 (2.77)  Time: 0.678s,  188.82/s  (0.660s,  194.08/s)  LR: 5.835e-04  Data: 0.024 (0.041)
Train: 83 [ 200/989 ( 20%)]  Loss: 2.69 (2.75)  Time: 0.614s,  208.52/s  (0.658s,  194.65/s)  LR: 5.835e-04  Data: 0.045 (0.041)
Train: 83 [ 250/989 ( 25%)]  Loss: 1.94 (2.61)  Time: 0.636s,  201.38/s  (0.657s,  194.97/s)  LR: 5.835e-04  Data: 0.015 (0.040)
Train: 83 [ 300/989 ( 30%)]  Loss: 3.20 (2.70)  Time: 0.722s,  177.35/s  (0.656s,  195.25/s)  LR: 5.835e-04  Data: 0.035 (0.039)
Train: 83 [ 350/989 ( 35%)]  Loss: 3.50 (2.80)  Time: 0.655s,  195.41/s  (0.656s,  195.26/s)  LR: 5.835e-04  Data: 0.036 (0.039)
Train: 83 [ 400/989 ( 40%)]  Loss: 2.58 (2.77)  Time: 0.652s,  196.24/s  (0.655s,  195.33/s)  LR: 5.835e-04  Data: 0.033 (0.039)
Train: 83 [ 450/989 ( 46%)]  Loss: 2.87 (2.78)  Time: 0.702s,  182.33/s  (0.656s,  195.18/s)  LR: 5.835e-04  Data: 0.048 (0.039)
Train: 83 [ 500/989 ( 51%)]  Loss: 2.08 (2.72)  Time: 0.643s,  199.16/s  (0.656s,  195.11/s)  LR: 5.835e-04  Data: 0.016 (0.038)
Train: 83 [ 550/989 ( 56%)]  Loss: 2.57 (2.71)  Time: 0.654s,  195.84/s  (0.656s,  195.11/s)  LR: 5.835e-04  Data: 0.015 (0.038)
Train: 83 [ 600/989 ( 61%)]  Loss: 2.23 (2.67)  Time: 0.669s,  191.43/s  (0.657s,  194.97/s)  LR: 5.835e-04  Data: 0.044 (0.038)
Train: 83 [ 650/989 ( 66%)]  Loss: 2.82 (2.68)  Time: 0.589s,  217.32/s  (0.657s,  194.76/s)  LR: 5.835e-04  Data: 0.010 (0.038)
Train: 83 [ 700/989 ( 71%)]  Loss: 2.22 (2.65)  Time: 0.689s,  185.74/s  (0.658s,  194.51/s)  LR: 5.835e-04  Data: 0.035 (0.038)
Train: 83 [ 750/989 ( 76%)]  Loss: 2.75 (2.66)  Time: 0.661s,  193.59/s  (0.659s,  194.27/s)  LR: 5.835e-04  Data: 0.036 (0.038)
Train: 83 [ 800/989 ( 81%)]  Loss: 2.24 (2.63)  Time: 0.671s,  190.68/s  (0.660s,  194.06/s)  LR: 5.835e-04  Data: 0.015 (0.037)
Train: 83 [ 850/989 ( 86%)]  Loss: 2.89 (2.65)  Time: 0.680s,  188.25/s  (0.661s,  193.73/s)  LR: 5.835e-04  Data: 0.045 (0.037)
Train: 83 [ 900/989 ( 91%)]  Loss: 3.15 (2.67)  Time: 0.691s,  185.12/s  (0.662s,  193.40/s)  LR: 5.835e-04  Data: 0.039 (0.037)
Train: 83 [ 950/989 ( 96%)]  Loss: 2.35 (2.66)  Time: 0.717s,  178.60/s  (0.663s,  192.98/s)  LR: 5.835e-04  Data: 0.043 (0.037)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.244 (1.244)  Loss:   1.028 ( 1.028)  Acc@1:  77.344 ( 77.344)  Acc@5:  91.406 ( 91.406)
Test: [  39/39]  Time: 0.043 (0.322)  Loss:   0.231 ( 0.899)  Acc@1:  87.500 ( 79.100)  Acc@5: 100.000 ( 94.320)
Current checkpoints:
 ('./output/train/Upd_Exp3_Image100/checkpoint-80.pth.tar', 79.28)
 ('./output/train/Upd_Exp3_Image100/checkpoint-83.pth.tar', 79.1)
 ('./output/train/Upd_Exp3_Image100/checkpoint-82.pth.tar', 78.88)
 ('./output/train/Upd_Exp3_Image100/checkpoint-77.pth.tar', 78.48)
 ('./output/train/Upd_Exp3_Image100/checkpoint-79.pth.tar', 78.32)
 ('./output/train/Upd_Exp3_Image100/checkpoint-71.pth.tar', 78.22)
 ('./output/train/Upd_Exp3_Image100/checkpoint-74.pth.tar', 78.2)
 ('./output/train/Upd_Exp3_Image100/checkpoint-76.pth.tar', 78.18)
 ('./output/train/Upd_Exp3_Image100/checkpoint-70.pth.tar', 78.1)
 ('./output/train/Upd_Exp3_Image100/checkpoint-69.pth.tar', 78.04)

Train: 84 [   0/989 (  0%)]  Loss: 3.46 (3.46)  Time: 1.747s,   73.27/s  (1.747s,   73.27/s)  LR: 5.807e-04  Data: 1.027 (1.027)
Train: 84 [  50/989 (  5%)]  Loss: 2.62 (3.04)  Time: 0.652s,  196.26/s  (0.686s,  186.63/s)  LR: 5.807e-04  Data: 0.037 (0.055)
Train: 84 [ 100/989 ( 10%)]  Loss: 2.60 (2.89)  Time: 0.618s,  207.25/s  (0.671s,  190.73/s)  LR: 5.807e-04  Data: 0.030 (0.046)
Train: 84 [ 150/989 ( 15%)]  Loss: 1.96 (2.66)  Time: 0.633s,  202.25/s  (0.664s,  192.84/s)  LR: 5.807e-04  Data: 0.035 (0.044)
Train: 84 [ 200/989 ( 20%)]  Loss: 2.87 (2.70)  Time: 0.650s,  196.89/s  (0.662s,  193.38/s)  LR: 5.807e-04  Data: 0.047 (0.042)
Train: 84 [ 250/989 ( 25%)]  Loss: 2.48 (2.67)  Time: 0.640s,  199.99/s  (0.661s,  193.79/s)  LR: 5.807e-04  Data: 0.037 (0.041)
Train: 84 [ 300/989 ( 30%)]  Loss: 2.83 (2.69)  Time: 0.691s,  185.22/s  (0.660s,  193.84/s)  LR: 5.807e-04  Data: 0.024 (0.041)
Train: 84 [ 350/989 ( 35%)]  Loss: 2.57 (2.68)  Time: 0.604s,  211.83/s  (0.661s,  193.67/s)  LR: 5.807e-04  Data: 0.016 (0.040)
Train: 84 [ 400/989 ( 40%)]  Loss: 2.42 (2.65)  Time: 0.760s,  168.42/s  (0.661s,  193.64/s)  LR: 5.807e-04  Data: 0.020 (0.040)
Train: 84 [ 450/989 ( 46%)]  Loss: 2.39 (2.62)  Time: 0.661s,  193.55/s  (0.661s,  193.67/s)  LR: 5.807e-04  Data: 0.055 (0.039)
Train: 84 [ 500/989 ( 51%)]  Loss: 3.08 (2.66)  Time: 0.695s,  184.14/s  (0.661s,  193.61/s)  LR: 5.807e-04  Data: 0.031 (0.039)
Train: 84 [ 550/989 ( 56%)]  Loss: 3.20 (2.71)  Time: 0.655s,  195.55/s  (0.661s,  193.52/s)  LR: 5.807e-04  Data: 0.019 (0.039)
Train: 84 [ 600/989 ( 61%)]  Loss: 2.84 (2.72)  Time: 0.662s,  193.27/s  (0.661s,  193.60/s)  LR: 5.807e-04  Data: 0.054 (0.039)
Train: 84 [ 650/989 ( 66%)]  Loss: 2.55 (2.71)  Time: 0.623s,  205.32/s  (0.661s,  193.71/s)  LR: 5.807e-04  Data: 0.021 (0.039)
Train: 84 [ 700/989 ( 71%)]  Loss: 2.42 (2.69)  Time: 0.662s,  193.39/s  (0.661s,  193.57/s)  LR: 5.807e-04  Data: 0.019 (0.039)
Train: 84 [ 750/989 ( 76%)]  Loss: 3.18 (2.72)  Time: 0.685s,  186.89/s  (0.662s,  193.41/s)  LR: 5.807e-04  Data: 0.043 (0.039)
Train: 84 [ 800/989 ( 81%)]  Loss: 2.03 (2.68)  Time: 0.685s,  186.75/s  (0.663s,  193.21/s)  LR: 5.807e-04  Data: 0.027 (0.038)
Train: 84 [ 850/989 ( 86%)]  Loss: 2.59 (2.67)  Time: 0.668s,  191.67/s  (0.663s,  192.93/s)  LR: 5.807e-04  Data: 0.036 (0.038)
Train: 84 [ 900/989 ( 91%)]  Loss: 2.73 (2.68)  Time: 0.796s,  160.79/s  (0.664s,  192.67/s)  LR: 5.807e-04  Data: 0.049 (0.038)
Train: 84 [ 950/989 ( 96%)]  Loss: 1.97 (2.64)  Time: 0.693s,  184.75/s  (0.666s,  192.31/s)  LR: 5.807e-04  Data: 0.047 (0.037)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.261 (1.261)  Loss:   1.147 ( 1.147)  Acc@1:  80.469 ( 80.469)  Acc@5:  92.969 ( 92.969)
Test: [  39/39]  Time: 0.054 (0.325)  Loss:   0.614 ( 0.945)  Acc@1:  87.500 ( 78.960)  Acc@5: 100.000 ( 94.620)
Current checkpoints:
 ('./output/train/Upd_Exp3_Image100/checkpoint-80.pth.tar', 79.28)
 ('./output/train/Upd_Exp3_Image100/checkpoint-83.pth.tar', 79.1)
 ('./output/train/Upd_Exp3_Image100/checkpoint-84.pth.tar', 78.96)
 ('./output/train/Upd_Exp3_Image100/checkpoint-82.pth.tar', 78.88)
 ('./output/train/Upd_Exp3_Image100/checkpoint-77.pth.tar', 78.48)
 ('./output/train/Upd_Exp3_Image100/checkpoint-79.pth.tar', 78.32)
 ('./output/train/Upd_Exp3_Image100/checkpoint-71.pth.tar', 78.22)
 ('./output/train/Upd_Exp3_Image100/checkpoint-74.pth.tar', 78.2)
 ('./output/train/Upd_Exp3_Image100/checkpoint-76.pth.tar', 78.18)
 ('./output/train/Upd_Exp3_Image100/checkpoint-70.pth.tar', 78.1)

Train: 85 [   0/989 (  0%)]  Loss: 3.16 (3.16)  Time: 1.870s,   68.44/s  (1.870s,   68.44/s)  LR: 5.779e-04  Data: 1.141 (1.141)
Train: 85 [  50/989 (  5%)]  Loss: 2.60 (2.88)  Time: 0.635s,  201.69/s  (0.685s,  186.82/s)  LR: 5.779e-04  Data: 0.038 (0.058)
Train: 85 [ 100/989 ( 10%)]  Loss: 2.66 (2.81)  Time: 0.715s,  179.14/s  (0.671s,  190.81/s)  LR: 5.779e-04  Data: 0.051 (0.048)
Train: 85 [ 150/989 ( 15%)]  Loss: 2.77 (2.80)  Time: 0.663s,  192.95/s  (0.666s,  192.19/s)  LR: 5.779e-04  Data: 0.046 (0.045)
Train: 85 [ 200/989 ( 20%)]  Loss: 2.36 (2.71)  Time: 0.651s,  196.59/s  (0.664s,  192.74/s)  LR: 5.779e-04  Data: 0.043 (0.043)
Train: 85 [ 250/989 ( 25%)]  Loss: 3.23 (2.80)  Time: 0.664s,  192.76/s  (0.663s,  193.09/s)  LR: 5.779e-04  Data: 0.047 (0.042)
Train: 85 [ 300/989 ( 30%)]  Loss: 2.82 (2.80)  Time: 0.644s,  198.63/s  (0.662s,  193.42/s)  LR: 5.779e-04  Data: 0.042 (0.041)
Train: 85 [ 350/989 ( 35%)]  Loss: 2.65 (2.78)  Time: 0.681s,  188.09/s  (0.661s,  193.75/s)  LR: 5.779e-04  Data: 0.054 (0.040)
Train: 85 [ 400/989 ( 40%)]  Loss: 2.99 (2.80)  Time: 0.661s,  193.60/s  (0.660s,  193.85/s)  LR: 5.779e-04  Data: 0.045 (0.039)
Train: 85 [ 450/989 ( 46%)]  Loss: 2.99 (2.82)  Time: 0.651s,  196.57/s  (0.660s,  193.91/s)  LR: 5.779e-04  Data: 0.026 (0.039)
Train: 85 [ 500/989 ( 51%)]  Loss: 2.89 (2.83)  Time: 0.682s,  187.59/s  (0.659s,  194.12/s)  LR: 5.779e-04  Data: 0.048 (0.039)
Train: 85 [ 550/989 ( 56%)]  Loss: 2.58 (2.81)  Time: 0.611s,  209.64/s  (0.659s,  194.21/s)  LR: 5.779e-04  Data: 0.005 (0.038)
Train: 85 [ 600/989 ( 61%)]  Loss: 1.85 (2.73)  Time: 0.647s,  197.96/s  (0.659s,  194.17/s)  LR: 5.779e-04  Data: 0.046 (0.038)
Train: 85 [ 650/989 ( 66%)]  Loss: 3.13 (2.76)  Time: 0.654s,  195.60/s  (0.659s,  194.16/s)  LR: 5.779e-04  Data: 0.025 (0.038)
Train: 85 [ 700/989 ( 71%)]  Loss: 2.21 (2.73)  Time: 0.688s,  186.04/s  (0.659s,  194.14/s)  LR: 5.779e-04  Data: 0.044 (0.038)
Train: 85 [ 750/989 ( 76%)]  Loss: 2.76 (2.73)  Time: 0.604s,  212.05/s  (0.659s,  194.10/s)  LR: 5.779e-04  Data: 0.020 (0.038)
Train: 85 [ 800/989 ( 81%)]  Loss: 2.54 (2.72)  Time: 0.646s,  198.06/s  (0.660s,  193.86/s)  LR: 5.779e-04  Data: 0.037 (0.038)
Train: 85 [ 850/989 ( 86%)]  Loss: 2.10 (2.68)  Time: 0.724s,  176.70/s  (0.661s,  193.57/s)  LR: 5.779e-04  Data: 0.043 (0.037)
Train: 85 [ 900/989 ( 91%)]  Loss: 2.44 (2.67)  Time: 0.668s,  191.74/s  (0.663s,  193.17/s)  LR: 5.779e-04  Data: 0.034 (0.037)
Train: 85 [ 950/989 ( 96%)]  Loss: 2.71 (2.67)  Time: 0.680s,  188.17/s  (0.664s,  192.89/s)  LR: 5.779e-04  Data: 0.021 (0.037)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.265 (1.265)  Loss:   1.083 ( 1.083)  Acc@1:  80.469 ( 80.469)  Acc@5:  90.625 ( 90.625)
Test: [  39/39]  Time: 0.049 (0.324)  Loss:   0.439 ( 0.899)  Acc@1:  87.500 ( 80.120)  Acc@5: 100.000 ( 94.440)
Current checkpoints:
 ('./output/train/Upd_Exp3_Image100/checkpoint-85.pth.tar', 80.12)
 ('./output/train/Upd_Exp3_Image100/checkpoint-80.pth.tar', 79.28)
 ('./output/train/Upd_Exp3_Image100/checkpoint-83.pth.tar', 79.1)
 ('./output/train/Upd_Exp3_Image100/checkpoint-84.pth.tar', 78.96)
 ('./output/train/Upd_Exp3_Image100/checkpoint-82.pth.tar', 78.88)
 ('./output/train/Upd_Exp3_Image100/checkpoint-77.pth.tar', 78.48)
 ('./output/train/Upd_Exp3_Image100/checkpoint-79.pth.tar', 78.32)
 ('./output/train/Upd_Exp3_Image100/checkpoint-71.pth.tar', 78.22)
 ('./output/train/Upd_Exp3_Image100/checkpoint-74.pth.tar', 78.2)
 ('./output/train/Upd_Exp3_Image100/checkpoint-76.pth.tar', 78.18)

Train: 86 [   0/989 (  0%)]  Loss: 3.08 (3.08)  Time: 2.010s,   63.69/s  (2.010s,   63.69/s)  LR: 5.751e-04  Data: 1.270 (1.270)
Train: 86 [  50/989 (  5%)]  Loss: 2.13 (2.61)  Time: 0.636s,  201.34/s  (0.686s,  186.68/s)  LR: 5.751e-04  Data: 0.015 (0.061)
Train: 86 [ 100/989 ( 10%)]  Loss: 3.09 (2.77)  Time: 0.658s,  194.64/s  (0.674s,  189.99/s)  LR: 5.751e-04  Data: 0.043 (0.048)
Train: 86 [ 150/989 ( 15%)]  Loss: 2.68 (2.74)  Time: 0.663s,  193.03/s  (0.667s,  191.76/s)  LR: 5.751e-04  Data: 0.051 (0.046)
Train: 86 [ 200/989 ( 20%)]  Loss: 3.05 (2.81)  Time: 0.680s,  188.35/s  (0.667s,  192.01/s)  LR: 5.751e-04  Data: 0.041 (0.044)
Train: 86 [ 250/989 ( 25%)]  Loss: 2.92 (2.83)  Time: 0.651s,  196.74/s  (0.664s,  192.83/s)  LR: 5.751e-04  Data: 0.047 (0.042)
Train: 86 [ 300/989 ( 30%)]  Loss: 2.82 (2.82)  Time: 0.634s,  202.02/s  (0.662s,  193.29/s)  LR: 5.751e-04  Data: 0.012 (0.041)
Train: 86 [ 350/989 ( 35%)]  Loss: 3.12 (2.86)  Time: 0.651s,  196.52/s  (0.661s,  193.74/s)  LR: 5.751e-04  Data: 0.038 (0.040)
Train: 86 [ 400/989 ( 40%)]  Loss: 2.55 (2.83)  Time: 0.709s,  180.43/s  (0.660s,  193.88/s)  LR: 5.751e-04  Data: 0.019 (0.040)
Train: 86 [ 450/989 ( 46%)]  Loss: 2.50 (2.79)  Time: 0.643s,  198.92/s  (0.659s,  194.11/s)  LR: 5.751e-04  Data: 0.045 (0.040)
Train: 86 [ 500/989 ( 51%)]  Loss: 2.78 (2.79)  Time: 0.666s,  192.25/s  (0.659s,  194.13/s)  LR: 5.751e-04  Data: 0.017 (0.039)
Train: 86 [ 550/989 ( 56%)]  Loss: 3.29 (2.83)  Time: 0.639s,  200.28/s  (0.659s,  194.17/s)  LR: 5.751e-04  Data: 0.033 (0.039)
Train: 86 [ 600/989 ( 61%)]  Loss: 3.31 (2.87)  Time: 0.658s,  194.44/s  (0.659s,  194.21/s)  LR: 5.751e-04  Data: 0.052 (0.039)
Train: 86 [ 650/989 ( 66%)]  Loss: 3.11 (2.89)  Time: 0.673s,  190.27/s  (0.659s,  194.24/s)  LR: 5.751e-04  Data: 0.041 (0.039)
Train: 86 [ 700/989 ( 71%)]  Loss: 3.20 (2.91)  Time: 0.647s,  197.88/s  (0.659s,  194.12/s)  LR: 5.751e-04  Data: 0.034 (0.039)
Train: 86 [ 750/989 ( 76%)]  Loss: 2.58 (2.89)  Time: 0.801s,  159.78/s  (0.660s,  194.01/s)  LR: 5.751e-04  Data: 0.024 (0.039)
Train: 86 [ 800/989 ( 81%)]  Loss: 2.58 (2.87)  Time: 0.650s,  196.99/s  (0.660s,  193.84/s)  LR: 5.751e-04  Data: 0.013 (0.038)
Train: 86 [ 850/989 ( 86%)]  Loss: 2.62 (2.86)  Time: 0.691s,  185.14/s  (0.661s,  193.52/s)  LR: 5.751e-04  Data: 0.031 (0.038)
Train: 86 [ 900/989 ( 91%)]  Loss: 2.66 (2.85)  Time: 0.548s,  233.49/s  (0.662s,  193.26/s)  LR: 5.751e-04  Data: 0.016 (0.038)
Train: 86 [ 950/989 ( 96%)]  Loss: 2.88 (2.85)  Time: 0.677s,  189.15/s  (0.664s,  192.87/s)  LR: 5.751e-04  Data: 0.051 (0.038)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.275 (1.275)  Loss:   1.288 ( 1.288)  Acc@1:  73.438 ( 73.438)  Acc@5:  92.188 ( 92.188)
Test: [  39/39]  Time: 0.048 (0.325)  Loss:   0.738 ( 0.940)  Acc@1:  87.500 ( 79.120)  Acc@5: 100.000 ( 94.680)
Current checkpoints:
 ('./output/train/Upd_Exp3_Image100/checkpoint-85.pth.tar', 80.12)
 ('./output/train/Upd_Exp3_Image100/checkpoint-80.pth.tar', 79.28)
 ('./output/train/Upd_Exp3_Image100/checkpoint-86.pth.tar', 79.12)
 ('./output/train/Upd_Exp3_Image100/checkpoint-83.pth.tar', 79.1)
 ('./output/train/Upd_Exp3_Image100/checkpoint-84.pth.tar', 78.96)
 ('./output/train/Upd_Exp3_Image100/checkpoint-82.pth.tar', 78.88)
 ('./output/train/Upd_Exp3_Image100/checkpoint-77.pth.tar', 78.48)
 ('./output/train/Upd_Exp3_Image100/checkpoint-79.pth.tar', 78.32)
 ('./output/train/Upd_Exp3_Image100/checkpoint-71.pth.tar', 78.22)
 ('./output/train/Upd_Exp3_Image100/checkpoint-74.pth.tar', 78.2)

Train: 87 [   0/989 (  0%)]  Loss: 2.90 (2.90)  Time: 1.737s,   73.69/s  (1.737s,   73.69/s)  LR: 5.722e-04  Data: 1.013 (1.013)
Train: 87 [  50/989 (  5%)]  Loss: 2.90 (2.90)  Time: 0.701s,  182.59/s  (0.689s,  185.79/s)  LR: 5.722e-04  Data: 0.015 (0.059)
Train: 87 [ 100/989 ( 10%)]  Loss: 2.53 (2.78)  Time: 0.682s,  187.57/s  (0.671s,  190.84/s)  LR: 5.722e-04  Data: 0.059 (0.047)
Train: 87 [ 150/989 ( 15%)]  Loss: 3.36 (2.92)  Time: 0.666s,  192.16/s  (0.665s,  192.59/s)  LR: 5.722e-04  Data: 0.043 (0.044)
Train: 87 [ 200/989 ( 20%)]  Loss: 2.27 (2.79)  Time: 0.661s,  193.77/s  (0.662s,  193.39/s)  LR: 5.722e-04  Data: 0.025 (0.042)
Train: 87 [ 250/989 ( 25%)]  Loss: 3.09 (2.84)  Time: 0.604s,  211.83/s  (0.661s,  193.69/s)  LR: 5.722e-04  Data: 0.022 (0.041)
Train: 87 [ 300/989 ( 30%)]  Loss: 1.86 (2.70)  Time: 0.662s,  193.35/s  (0.660s,  193.82/s)  LR: 5.722e-04  Data: 0.042 (0.041)
Train: 87 [ 350/989 ( 35%)]  Loss: 2.61 (2.69)  Time: 0.668s,  191.61/s  (0.660s,  193.89/s)  LR: 5.722e-04  Data: 0.046 (0.041)
Train: 87 [ 400/989 ( 40%)]  Loss: 2.06 (2.62)  Time: 0.637s,  201.02/s  (0.660s,  194.01/s)  LR: 5.722e-04  Data: 0.015 (0.040)
Train: 87 [ 450/989 ( 46%)]  Loss: 2.22 (2.58)  Time: 0.704s,  181.85/s  (0.660s,  194.06/s)  LR: 5.722e-04  Data: 0.075 (0.040)
Train: 87 [ 500/989 ( 51%)]  Loss: 2.88 (2.61)  Time: 0.672s,  190.40/s  (0.659s,  194.13/s)  LR: 5.722e-04  Data: 0.043 (0.040)
Train: 87 [ 550/989 ( 56%)]  Loss: 2.09 (2.57)  Time: 0.663s,  193.13/s  (0.659s,  194.19/s)  LR: 5.722e-04  Data: 0.041 (0.040)
Train: 87 [ 600/989 ( 61%)]  Loss: 2.75 (2.58)  Time: 0.642s,  199.34/s  (0.659s,  194.27/s)  LR: 5.722e-04  Data: 0.042 (0.039)
Train: 87 [ 650/989 ( 66%)]  Loss: 2.48 (2.57)  Time: 0.677s,  188.99/s  (0.659s,  194.12/s)  LR: 5.722e-04  Data: 0.041 (0.039)
Train: 87 [ 700/989 ( 71%)]  Loss: 3.41 (2.63)  Time: 0.655s,  195.29/s  (0.660s,  193.98/s)  LR: 5.722e-04  Data: 0.047 (0.040)
Train: 87 [ 750/989 ( 76%)]  Loss: 3.25 (2.67)  Time: 0.669s,  191.27/s  (0.661s,  193.78/s)  LR: 5.722e-04  Data: 0.052 (0.039)
Train: 87 [ 800/989 ( 81%)]  Loss: 2.94 (2.68)  Time: 0.701s,  182.51/s  (0.661s,  193.59/s)  LR: 5.722e-04  Data: 0.038 (0.039)
Train: 87 [ 850/989 ( 86%)]  Loss: 2.25 (2.66)  Time: 0.674s,  189.79/s  (0.663s,  193.18/s)  LR: 5.722e-04  Data: 0.037 (0.039)
Train: 87 [ 900/989 ( 91%)]  Loss: 2.59 (2.66)  Time: 0.687s,  186.27/s  (0.664s,  192.79/s)  LR: 5.722e-04  Data: 0.020 (0.039)
Train: 87 [ 950/989 ( 96%)]  Loss: 2.77 (2.66)  Time: 0.727s,  175.95/s  (0.665s,  192.42/s)  LR: 5.722e-04  Data: 0.031 (0.038)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.266 (1.266)  Loss:   1.057 ( 1.057)  Acc@1:  79.688 ( 79.688)  Acc@5:  89.844 ( 89.844)
Test: [  39/39]  Time: 0.047 (0.324)  Loss:   0.365 ( 0.929)  Acc@1:  87.500 ( 79.740)  Acc@5: 100.000 ( 94.720)
Current checkpoints:
 ('./output/train/Upd_Exp3_Image100/checkpoint-85.pth.tar', 80.12)
 ('./output/train/Upd_Exp3_Image100/checkpoint-87.pth.tar', 79.74)
 ('./output/train/Upd_Exp3_Image100/checkpoint-80.pth.tar', 79.28)
 ('./output/train/Upd_Exp3_Image100/checkpoint-86.pth.tar', 79.12)
 ('./output/train/Upd_Exp3_Image100/checkpoint-83.pth.tar', 79.1)
 ('./output/train/Upd_Exp3_Image100/checkpoint-84.pth.tar', 78.96)
 ('./output/train/Upd_Exp3_Image100/checkpoint-82.pth.tar', 78.88)
 ('./output/train/Upd_Exp3_Image100/checkpoint-77.pth.tar', 78.48)
 ('./output/train/Upd_Exp3_Image100/checkpoint-79.pth.tar', 78.32)
 ('./output/train/Upd_Exp3_Image100/checkpoint-71.pth.tar', 78.22)

Train: 88 [   0/989 (  0%)]  Loss: 2.60 (2.60)  Time: 1.802s,   71.02/s  (1.802s,   71.02/s)  LR: 5.693e-04  Data: 1.057 (1.057)
Train: 88 [  50/989 (  5%)]  Loss: 3.09 (2.85)  Time: 0.666s,  192.16/s  (0.686s,  186.53/s)  LR: 5.693e-04  Data: 0.050 (0.057)
Train: 88 [ 100/989 ( 10%)]  Loss: 2.23 (2.64)  Time: 0.672s,  190.43/s  (0.671s,  190.73/s)  LR: 5.693e-04  Data: 0.055 (0.048)
Train: 88 [ 150/989 ( 15%)]  Loss: 3.39 (2.83)  Time: 0.707s,  181.09/s  (0.665s,  192.53/s)  LR: 5.693e-04  Data: 0.018 (0.044)
Train: 88 [ 200/989 ( 20%)]  Loss: 2.18 (2.70)  Time: 0.657s,  194.79/s  (0.663s,  192.95/s)  LR: 5.693e-04  Data: 0.048 (0.043)
Train: 88 [ 250/989 ( 25%)]  Loss: 2.53 (2.67)  Time: 0.654s,  195.71/s  (0.662s,  193.41/s)  LR: 5.693e-04  Data: 0.035 (0.042)
Train: 88 [ 300/989 ( 30%)]  Loss: 3.14 (2.74)  Time: 0.643s,  199.00/s  (0.661s,  193.56/s)  LR: 5.693e-04  Data: 0.022 (0.041)
Train: 88 [ 350/989 ( 35%)]  Loss: 2.60 (2.72)  Time: 0.638s,  200.51/s  (0.660s,  193.83/s)  LR: 5.693e-04  Data: 0.037 (0.040)
Train: 88 [ 400/989 ( 40%)]  Loss: 2.11 (2.65)  Time: 0.675s,  189.60/s  (0.661s,  193.75/s)  LR: 5.693e-04  Data: 0.035 (0.040)
Train: 88 [ 450/989 ( 46%)]  Loss: 2.69 (2.66)  Time: 0.699s,  183.19/s  (0.661s,  193.77/s)  LR: 5.693e-04  Data: 0.045 (0.040)
Train: 88 [ 500/989 ( 51%)]  Loss: 2.29 (2.62)  Time: 0.661s,  193.50/s  (0.660s,  193.84/s)  LR: 5.693e-04  Data: 0.020 (0.039)
Train: 88 [ 550/989 ( 56%)]  Loss: 2.56 (2.62)  Time: 0.638s,  200.71/s  (0.660s,  193.84/s)  LR: 5.693e-04  Data: 0.036 (0.039)
Train: 88 [ 600/989 ( 61%)]  Loss: 2.65 (2.62)  Time: 0.723s,  177.05/s  (0.660s,  193.85/s)  LR: 5.693e-04  Data: 0.043 (0.039)
Train: 88 [ 650/989 ( 66%)]  Loss: 2.91 (2.64)  Time: 0.652s,  196.33/s  (0.660s,  193.90/s)  LR: 5.693e-04  Data: 0.039 (0.039)
Train: 88 [ 700/989 ( 71%)]  Loss: 2.41 (2.63)  Time: 0.695s,  184.21/s  (0.660s,  193.84/s)  LR: 5.693e-04  Data: 0.046 (0.039)
Train: 88 [ 750/989 ( 76%)]  Loss: 2.61 (2.62)  Time: 0.651s,  196.48/s  (0.661s,  193.66/s)  LR: 5.693e-04  Data: 0.018 (0.039)
Train: 88 [ 800/989 ( 81%)]  Loss: 2.76 (2.63)  Time: 0.661s,  193.64/s  (0.662s,  193.48/s)  LR: 5.693e-04  Data: 0.026 (0.039)
Train: 88 [ 850/989 ( 86%)]  Loss: 2.58 (2.63)  Time: 0.658s,  194.40/s  (0.662s,  193.24/s)  LR: 5.693e-04  Data: 0.005 (0.038)
Train: 88 [ 900/989 ( 91%)]  Loss: 3.10 (2.65)  Time: 0.681s,  187.88/s  (0.664s,  192.92/s)  LR: 5.693e-04  Data: 0.035 (0.038)
Train: 88 [ 950/989 ( 96%)]  Loss: 2.60 (2.65)  Time: 0.660s,  193.82/s  (0.665s,  192.59/s)  LR: 5.693e-04  Data: 0.016 (0.038)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.236 (1.236)  Loss:   1.014 ( 1.014)  Acc@1:  78.906 ( 78.906)  Acc@5:  92.969 ( 92.969)
Test: [  39/39]  Time: 0.053 (0.320)  Loss:   0.376 ( 0.907)  Acc@1: 100.000 ( 79.100)  Acc@5: 100.000 ( 94.360)
Current checkpoints:
 ('./output/train/Upd_Exp3_Image100/checkpoint-85.pth.tar', 80.12)
 ('./output/train/Upd_Exp3_Image100/checkpoint-87.pth.tar', 79.74)
 ('./output/train/Upd_Exp3_Image100/checkpoint-80.pth.tar', 79.28)
 ('./output/train/Upd_Exp3_Image100/checkpoint-86.pth.tar', 79.12)
 ('./output/train/Upd_Exp3_Image100/checkpoint-83.pth.tar', 79.1)
 ('./output/train/Upd_Exp3_Image100/checkpoint-88.pth.tar', 79.1)
 ('./output/train/Upd_Exp3_Image100/checkpoint-84.pth.tar', 78.96)
 ('./output/train/Upd_Exp3_Image100/checkpoint-82.pth.tar', 78.88)
 ('./output/train/Upd_Exp3_Image100/checkpoint-77.pth.tar', 78.48)
 ('./output/train/Upd_Exp3_Image100/checkpoint-79.pth.tar', 78.32)

Train: 89 [   0/989 (  0%)]  Loss: 3.04 (3.04)  Time: 1.686s,   75.92/s  (1.686s,   75.92/s)  LR: 5.664e-04  Data: 0.971 (0.971)
Train: 89 [  50/989 (  5%)]  Loss: 2.65 (2.85)  Time: 0.633s,  202.18/s  (0.680s,  188.20/s)  LR: 5.664e-04  Data: 0.037 (0.056)
Train: 89 [ 100/989 ( 10%)]  Loss: 3.01 (2.90)  Time: 0.655s,  195.32/s  (0.666s,  192.13/s)  LR: 5.664e-04  Data: 0.039 (0.046)
Train: 89 [ 150/989 ( 15%)]  Loss: 2.20 (2.73)  Time: 0.636s,  201.19/s  (0.662s,  193.27/s)  LR: 5.664e-04  Data: 0.035 (0.043)
Train: 89 [ 200/989 ( 20%)]  Loss: 2.85 (2.75)  Time: 0.654s,  195.81/s  (0.661s,  193.64/s)  LR: 5.664e-04  Data: 0.034 (0.041)
Train: 89 [ 250/989 ( 25%)]  Loss: 3.39 (2.86)  Time: 0.654s,  195.68/s  (0.660s,  193.85/s)  LR: 5.664e-04  Data: 0.042 (0.040)
Train: 89 [ 300/989 ( 30%)]  Loss: 3.05 (2.89)  Time: 0.648s,  197.54/s  (0.659s,  194.29/s)  LR: 5.664e-04  Data: 0.021 (0.040)
Train: 89 [ 350/989 ( 35%)]  Loss: 2.88 (2.88)  Time: 0.625s,  204.65/s  (0.659s,  194.37/s)  LR: 5.664e-04  Data: 0.046 (0.039)
Train: 89 [ 400/989 ( 40%)]  Loss: 2.49 (2.84)  Time: 0.630s,  203.32/s  (0.659s,  194.35/s)  LR: 5.664e-04  Data: 0.056 (0.040)
Train: 89 [ 450/989 ( 46%)]  Loss: 3.32 (2.89)  Time: 0.650s,  196.92/s  (0.659s,  194.22/s)  LR: 5.664e-04  Data: 0.038 (0.040)
Train: 89 [ 500/989 ( 51%)]  Loss: 2.99 (2.90)  Time: 0.685s,  186.96/s  (0.659s,  194.13/s)  LR: 5.664e-04  Data: 0.030 (0.040)
Train: 89 [ 550/989 ( 56%)]  Loss: 2.88 (2.90)  Time: 0.642s,  199.44/s  (0.659s,  194.19/s)  LR: 5.664e-04  Data: 0.041 (0.040)
Train: 89 [ 600/989 ( 61%)]  Loss: 2.41 (2.86)  Time: 0.686s,  186.62/s  (0.659s,  194.28/s)  LR: 5.664e-04  Data: 0.046 (0.040)
Train: 89 [ 650/989 ( 66%)]  Loss: 2.11 (2.80)  Time: 0.685s,  186.77/s  (0.659s,  194.11/s)  LR: 5.664e-04  Data: 0.049 (0.040)
Train: 89 [ 700/989 ( 71%)]  Loss: 3.00 (2.82)  Time: 0.678s,  188.89/s  (0.660s,  194.01/s)  LR: 5.664e-04  Data: 0.036 (0.040)
Train: 89 [ 750/989 ( 76%)]  Loss: 3.23 (2.84)  Time: 0.716s,  178.68/s  (0.660s,  193.91/s)  LR: 5.664e-04  Data: 0.054 (0.040)
Train: 89 [ 800/989 ( 81%)]  Loss: 2.56 (2.83)  Time: 0.639s,  200.21/s  (0.661s,  193.75/s)  LR: 5.664e-04  Data: 0.029 (0.040)
Train: 89 [ 850/989 ( 86%)]  Loss: 3.14 (2.84)  Time: 0.702s,  182.35/s  (0.662s,  193.50/s)  LR: 5.664e-04  Data: 0.033 (0.040)
Train: 89 [ 900/989 ( 91%)]  Loss: 2.75 (2.84)  Time: 0.657s,  194.83/s  (0.662s,  193.26/s)  LR: 5.664e-04  Data: 0.025 (0.040)
Train: 89 [ 950/989 ( 96%)]  Loss: 2.86 (2.84)  Time: 0.677s,  189.08/s  (0.663s,  193.00/s)  LR: 5.664e-04  Data: 0.038 (0.039)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.260 (1.260)  Loss:   0.966 ( 0.966)  Acc@1:  80.469 ( 80.469)  Acc@5:  91.406 ( 91.406)
Test: [  39/39]  Time: 0.049 (0.321)  Loss:   0.474 ( 0.896)  Acc@1:  87.500 ( 79.920)  Acc@5: 100.000 ( 94.820)
Current checkpoints:
 ('./output/train/Upd_Exp3_Image100/checkpoint-85.pth.tar', 80.12)
 ('./output/train/Upd_Exp3_Image100/checkpoint-89.pth.tar', 79.92)
 ('./output/train/Upd_Exp3_Image100/checkpoint-87.pth.tar', 79.74)
 ('./output/train/Upd_Exp3_Image100/checkpoint-80.pth.tar', 79.28)
 ('./output/train/Upd_Exp3_Image100/checkpoint-86.pth.tar', 79.12)
 ('./output/train/Upd_Exp3_Image100/checkpoint-83.pth.tar', 79.1)
 ('./output/train/Upd_Exp3_Image100/checkpoint-88.pth.tar', 79.1)
 ('./output/train/Upd_Exp3_Image100/checkpoint-84.pth.tar', 78.96)
 ('./output/train/Upd_Exp3_Image100/checkpoint-82.pth.tar', 78.88)
 ('./output/train/Upd_Exp3_Image100/checkpoint-77.pth.tar', 78.48)

Train: 90 [   0/989 (  0%)]  Loss: 3.33 (3.33)  Time: 1.944s,   65.83/s  (1.944s,   65.83/s)  LR: 5.634e-04  Data: 1.218 (1.218)
Train: 90 [  50/989 (  5%)]  Loss: 2.53 (2.93)  Time: 0.664s,  192.80/s  (0.686s,  186.57/s)  LR: 5.634e-04  Data: 0.064 (0.066)
Train: 90 [ 100/989 ( 10%)]  Loss: 3.39 (3.08)  Time: 0.658s,  194.53/s  (0.669s,  191.26/s)  LR: 5.634e-04  Data: 0.030 (0.052)
Train: 90 [ 150/989 ( 15%)]  Loss: 1.97 (2.80)  Time: 0.650s,  196.96/s  (0.664s,  192.88/s)  LR: 5.634e-04  Data: 0.037 (0.048)
Train: 90 [ 200/989 ( 20%)]  Loss: 3.18 (2.88)  Time: 0.651s,  196.68/s  (0.662s,  193.48/s)  LR: 5.634e-04  Data: 0.041 (0.046)
Train: 90 [ 250/989 ( 25%)]  Loss: 2.74 (2.86)  Time: 0.646s,  198.24/s  (0.661s,  193.63/s)  LR: 5.634e-04  Data: 0.033 (0.045)
Train: 90 [ 300/989 ( 30%)]  Loss: 3.06 (2.88)  Time: 0.659s,  194.32/s  (0.661s,  193.64/s)  LR: 5.634e-04  Data: 0.039 (0.045)
Train: 90 [ 350/989 ( 35%)]  Loss: 2.75 (2.87)  Time: 0.655s,  195.27/s  (0.661s,  193.64/s)  LR: 5.634e-04  Data: 0.040 (0.045)
Train: 90 [ 400/989 ( 40%)]  Loss: 2.77 (2.86)  Time: 0.658s,  194.57/s  (0.661s,  193.77/s)  LR: 5.634e-04  Data: 0.028 (0.044)
Train: 90 [ 450/989 ( 46%)]  Loss: 2.82 (2.85)  Time: 0.675s,  189.75/s  (0.660s,  193.92/s)  LR: 5.634e-04  Data: 0.029 (0.043)
Train: 90 [ 500/989 ( 51%)]  Loss: 2.24 (2.80)  Time: 0.690s,  185.40/s  (0.660s,  193.96/s)  LR: 5.634e-04  Data: 0.044 (0.043)
Train: 90 [ 550/989 ( 56%)]  Loss: 2.27 (2.75)  Time: 0.661s,  193.59/s  (0.660s,  193.99/s)  LR: 5.634e-04  Data: 0.041 (0.042)
Train: 90 [ 600/989 ( 61%)]  Loss: 3.22 (2.79)  Time: 0.737s,  173.72/s  (0.660s,  193.94/s)  LR: 5.634e-04  Data: 0.037 (0.042)
Train: 90 [ 650/989 ( 66%)]  Loss: 2.65 (2.78)  Time: 0.706s,  181.34/s  (0.660s,  193.94/s)  LR: 5.634e-04  Data: 0.039 (0.042)
Train: 90 [ 700/989 ( 71%)]  Loss: 3.17 (2.80)  Time: 0.697s,  183.73/s  (0.660s,  193.93/s)  LR: 5.634e-04  Data: 0.048 (0.041)
Train: 90 [ 750/989 ( 76%)]  Loss: 3.18 (2.83)  Time: 0.708s,  180.84/s  (0.660s,  193.84/s)  LR: 5.634e-04  Data: 0.056 (0.041)
Train: 90 [ 800/989 ( 81%)]  Loss: 3.31 (2.86)  Time: 0.685s,  186.80/s  (0.661s,  193.71/s)  LR: 5.634e-04  Data: 0.046 (0.041)
Train: 90 [ 850/989 ( 86%)]  Loss: 2.48 (2.83)  Time: 0.684s,  187.21/s  (0.661s,  193.53/s)  LR: 5.634e-04  Data: 0.046 (0.041)
Train: 90 [ 900/989 ( 91%)]  Loss: 2.36 (2.81)  Time: 0.682s,  187.75/s  (0.662s,  193.29/s)  LR: 5.634e-04  Data: 0.011 (0.040)
Train: 90 [ 950/989 ( 96%)]  Loss: 1.90 (2.76)  Time: 0.654s,  195.73/s  (0.663s,  193.03/s)  LR: 5.634e-04  Data: 0.032 (0.040)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.229 (1.229)  Loss:   1.346 ( 1.346)  Acc@1:  67.969 ( 67.969)  Acc@5:  89.844 ( 89.844)
Test: [  39/39]  Time: 0.046 (0.321)  Loss:   0.581 ( 0.960)  Acc@1:  87.500 ( 79.680)  Acc@5: 100.000 ( 94.780)
Current checkpoints:
 ('./output/train/Upd_Exp3_Image100/checkpoint-85.pth.tar', 80.12)
 ('./output/train/Upd_Exp3_Image100/checkpoint-89.pth.tar', 79.92)
 ('./output/train/Upd_Exp3_Image100/checkpoint-87.pth.tar', 79.74)
 ('./output/train/Upd_Exp3_Image100/checkpoint-90.pth.tar', 79.68)
 ('./output/train/Upd_Exp3_Image100/checkpoint-80.pth.tar', 79.28)
 ('./output/train/Upd_Exp3_Image100/checkpoint-86.pth.tar', 79.12)
 ('./output/train/Upd_Exp3_Image100/checkpoint-83.pth.tar', 79.1)
 ('./output/train/Upd_Exp3_Image100/checkpoint-88.pth.tar', 79.1)
 ('./output/train/Upd_Exp3_Image100/checkpoint-84.pth.tar', 78.96)
 ('./output/train/Upd_Exp3_Image100/checkpoint-82.pth.tar', 78.88)

Train: 91 [   0/989 (  0%)]  Loss: 3.03 (3.03)  Time: 1.866s,   68.60/s  (1.866s,   68.60/s)  LR: 5.605e-04  Data: 1.127 (1.127)
Train: 91 [  50/989 (  5%)]  Loss: 2.98 (3.01)  Time: 0.622s,  205.83/s  (0.684s,  187.09/s)  LR: 5.605e-04  Data: 0.032 (0.064)
Train: 91 [ 100/989 ( 10%)]  Loss: 2.63 (2.88)  Time: 0.641s,  199.70/s  (0.669s,  191.47/s)  LR: 5.605e-04  Data: 0.041 (0.052)
Train: 91 [ 150/989 ( 15%)]  Loss: 2.57 (2.80)  Time: 0.643s,  199.16/s  (0.665s,  192.57/s)  LR: 5.605e-04  Data: 0.037 (0.049)
Train: 91 [ 200/989 ( 20%)]  Loss: 2.57 (2.76)  Time: 0.675s,  189.61/s  (0.663s,  193.18/s)  LR: 5.605e-04  Data: 0.049 (0.047)
Train: 91 [ 250/989 ( 25%)]  Loss: 2.98 (2.79)  Time: 0.630s,  203.33/s  (0.662s,  193.21/s)  LR: 5.605e-04  Data: 0.036 (0.045)
Train: 91 [ 300/989 ( 30%)]  Loss: 1.97 (2.68)  Time: 0.635s,  201.65/s  (0.662s,  193.42/s)  LR: 5.605e-04  Data: 0.032 (0.045)
Train: 91 [ 350/989 ( 35%)]  Loss: 2.34 (2.64)  Time: 0.651s,  196.76/s  (0.661s,  193.54/s)  LR: 5.605e-04  Data: 0.035 (0.044)
Train: 91 [ 400/989 ( 40%)]  Loss: 3.22 (2.70)  Time: 0.631s,  203.00/s  (0.662s,  193.50/s)  LR: 5.605e-04  Data: 0.044 (0.044)
Train: 91 [ 450/989 ( 46%)]  Loss: 2.19 (2.65)  Time: 0.642s,  199.31/s  (0.661s,  193.58/s)  LR: 5.605e-04  Data: 0.028 (0.043)
Train: 91 [ 500/989 ( 51%)]  Loss: 3.15 (2.69)  Time: 0.648s,  197.49/s  (0.661s,  193.69/s)  LR: 5.605e-04  Data: 0.039 (0.043)
Train: 91 [ 550/989 ( 56%)]  Loss: 2.90 (2.71)  Time: 0.664s,  192.69/s  (0.661s,  193.65/s)  LR: 5.605e-04  Data: 0.056 (0.043)
Train: 91 [ 600/989 ( 61%)]  Loss: 2.05 (2.66)  Time: 0.700s,  182.94/s  (0.661s,  193.66/s)  LR: 5.605e-04  Data: 0.061 (0.043)
Train: 91 [ 650/989 ( 66%)]  Loss: 1.88 (2.61)  Time: 0.670s,  190.96/s  (0.661s,  193.66/s)  LR: 5.605e-04  Data: 0.034 (0.042)
Train: 91 [ 700/989 ( 71%)]  Loss: 2.96 (2.63)  Time: 0.630s,  203.18/s  (0.661s,  193.66/s)  LR: 5.605e-04  Data: 0.045 (0.042)
Train: 91 [ 750/989 ( 76%)]  Loss: 3.05 (2.66)  Time: 0.686s,  186.48/s  (0.661s,  193.53/s)  LR: 5.605e-04  Data: 0.040 (0.042)
Train: 91 [ 800/989 ( 81%)]  Loss: 2.47 (2.64)  Time: 0.631s,  202.81/s  (0.662s,  193.40/s)  LR: 5.605e-04  Data: 0.049 (0.042)
Train: 91 [ 850/989 ( 86%)]  Loss: 2.41 (2.63)  Time: 0.790s,  161.93/s  (0.663s,  193.02/s)  LR: 5.605e-04  Data: 0.072 (0.042)
Train: 91 [ 900/989 ( 91%)]  Loss: 3.04 (2.65)  Time: 0.757s,  169.07/s  (0.664s,  192.79/s)  LR: 5.605e-04  Data: 0.043 (0.041)
Train: 91 [ 950/989 ( 96%)]  Loss: 1.93 (2.62)  Time: 0.658s,  194.57/s  (0.665s,  192.50/s)  LR: 5.605e-04  Data: 0.029 (0.041)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.231 (1.231)  Loss:   0.945 ( 0.945)  Acc@1:  82.812 ( 82.812)  Acc@5:  92.969 ( 92.969)
Test: [  39/39]  Time: 0.054 (0.319)  Loss:   0.242 ( 0.878)  Acc@1: 100.000 ( 79.940)  Acc@5: 100.000 ( 94.660)
Current checkpoints:
 ('./output/train/Upd_Exp3_Image100/checkpoint-85.pth.tar', 80.12)
 ('./output/train/Upd_Exp3_Image100/checkpoint-91.pth.tar', 79.94)
 ('./output/train/Upd_Exp3_Image100/checkpoint-89.pth.tar', 79.92)
 ('./output/train/Upd_Exp3_Image100/checkpoint-87.pth.tar', 79.74)
 ('./output/train/Upd_Exp3_Image100/checkpoint-90.pth.tar', 79.68)
 ('./output/train/Upd_Exp3_Image100/checkpoint-80.pth.tar', 79.28)
 ('./output/train/Upd_Exp3_Image100/checkpoint-86.pth.tar', 79.12)
 ('./output/train/Upd_Exp3_Image100/checkpoint-83.pth.tar', 79.1)
 ('./output/train/Upd_Exp3_Image100/checkpoint-88.pth.tar', 79.1)
 ('./output/train/Upd_Exp3_Image100/checkpoint-84.pth.tar', 78.96)

Train: 92 [   0/989 (  0%)]  Loss: 3.10 (3.10)  Time: 1.697s,   75.43/s  (1.697s,   75.43/s)  LR: 5.575e-04  Data: 0.983 (0.983)
Train: 92 [  50/989 (  5%)]  Loss: 2.53 (2.82)  Time: 0.630s,  203.22/s  (0.684s,  187.26/s)  LR: 5.575e-04  Data: 0.028 (0.057)
Train: 92 [ 100/989 ( 10%)]  Loss: 3.12 (2.92)  Time: 0.631s,  202.95/s  (0.668s,  191.55/s)  LR: 5.575e-04  Data: 0.038 (0.049)
Train: 92 [ 150/989 ( 15%)]  Loss: 2.85 (2.90)  Time: 0.642s,  199.38/s  (0.665s,  192.56/s)  LR: 5.575e-04  Data: 0.038 (0.047)
Train: 92 [ 200/989 ( 20%)]  Loss: 2.61 (2.84)  Time: 0.694s,  184.38/s  (0.662s,  193.39/s)  LR: 5.575e-04  Data: 0.056 (0.045)
Train: 92 [ 250/989 ( 25%)]  Loss: 2.69 (2.82)  Time: 0.662s,  193.27/s  (0.661s,  193.66/s)  LR: 5.575e-04  Data: 0.053 (0.044)
Train: 92 [ 300/989 ( 30%)]  Loss: 2.74 (2.81)  Time: 0.669s,  191.21/s  (0.660s,  194.03/s)  LR: 5.575e-04  Data: 0.047 (0.044)
Train: 92 [ 350/989 ( 35%)]  Loss: 3.34 (2.88)  Time: 0.660s,  194.06/s  (0.659s,  194.23/s)  LR: 5.575e-04  Data: 0.029 (0.043)
Train: 92 [ 400/989 ( 40%)]  Loss: 2.49 (2.83)  Time: 0.652s,  196.45/s  (0.659s,  194.32/s)  LR: 5.575e-04  Data: 0.052 (0.043)
Train: 92 [ 450/989 ( 46%)]  Loss: 2.93 (2.84)  Time: 0.602s,  212.69/s  (0.658s,  194.55/s)  LR: 5.575e-04  Data: 0.035 (0.042)
Train: 92 [ 500/989 ( 51%)]  Loss: 3.17 (2.87)  Time: 0.639s,  200.41/s  (0.658s,  194.50/s)  LR: 5.575e-04  Data: 0.041 (0.042)
Train: 92 [ 550/989 ( 56%)]  Loss: 2.27 (2.82)  Time: 0.638s,  200.70/s  (0.658s,  194.47/s)  LR: 5.575e-04  Data: 0.049 (0.042)
Train: 92 [ 600/989 ( 61%)]  Loss: 2.47 (2.79)  Time: 0.634s,  201.98/s  (0.658s,  194.48/s)  LR: 5.575e-04  Data: 0.032 (0.042)
Train: 92 [ 650/989 ( 66%)]  Loss: 2.76 (2.79)  Time: 0.652s,  196.22/s  (0.658s,  194.46/s)  LR: 5.575e-04  Data: 0.039 (0.042)
Train: 92 [ 700/989 ( 71%)]  Loss: 3.15 (2.82)  Time: 0.648s,  197.49/s  (0.658s,  194.41/s)  LR: 5.575e-04  Data: 0.039 (0.042)
Train: 92 [ 750/989 ( 76%)]  Loss: 2.83 (2.82)  Time: 0.665s,  192.51/s  (0.659s,  194.28/s)  LR: 5.575e-04  Data: 0.035 (0.041)
Train: 92 [ 800/989 ( 81%)]  Loss: 2.50 (2.80)  Time: 0.656s,  195.06/s  (0.659s,  194.22/s)  LR: 5.575e-04  Data: 0.039 (0.041)
Train: 92 [ 850/989 ( 86%)]  Loss: 2.41 (2.78)  Time: 0.665s,  192.36/s  (0.660s,  194.03/s)  LR: 5.575e-04  Data: 0.042 (0.040)
Train: 92 [ 900/989 ( 91%)]  Loss: 2.25 (2.75)  Time: 0.670s,  191.18/s  (0.660s,  193.81/s)  LR: 5.575e-04  Data: 0.033 (0.040)
Train: 92 [ 950/989 ( 96%)]  Loss: 3.13 (2.77)  Time: 0.749s,  170.90/s  (0.662s,  193.50/s)  LR: 5.575e-04  Data: 0.041 (0.040)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.209 (1.209)  Loss:   0.952 ( 0.952)  Acc@1:  82.031 ( 82.031)  Acc@5:  93.750 ( 93.750)
Test: [  39/39]  Time: 0.046 (0.318)  Loss:   0.503 ( 0.948)  Acc@1:  87.500 ( 79.420)  Acc@5: 100.000 ( 94.500)
Current checkpoints:
 ('./output/train/Upd_Exp3_Image100/checkpoint-85.pth.tar', 80.12)
 ('./output/train/Upd_Exp3_Image100/checkpoint-91.pth.tar', 79.94)
 ('./output/train/Upd_Exp3_Image100/checkpoint-89.pth.tar', 79.92)
 ('./output/train/Upd_Exp3_Image100/checkpoint-87.pth.tar', 79.74)
 ('./output/train/Upd_Exp3_Image100/checkpoint-90.pth.tar', 79.68)
 ('./output/train/Upd_Exp3_Image100/checkpoint-92.pth.tar', 79.42)
 ('./output/train/Upd_Exp3_Image100/checkpoint-80.pth.tar', 79.28)
 ('./output/train/Upd_Exp3_Image100/checkpoint-86.pth.tar', 79.12)
 ('./output/train/Upd_Exp3_Image100/checkpoint-83.pth.tar', 79.1)
 ('./output/train/Upd_Exp3_Image100/checkpoint-88.pth.tar', 79.1)

Train: 93 [   0/989 (  0%)]  Loss: 2.66 (2.66)  Time: 1.894s,   67.59/s  (1.894s,   67.59/s)  LR: 5.545e-04  Data: 1.080 (1.080)
Train: 93 [  50/989 (  5%)]  Loss: 2.60 (2.63)  Time: 0.654s,  195.68/s  (0.685s,  186.79/s)  LR: 5.545e-04  Data: 0.051 (0.061)
Train: 93 [ 100/989 ( 10%)]  Loss: 2.37 (2.55)  Time: 0.650s,  196.87/s  (0.671s,  190.66/s)  LR: 5.545e-04  Data: 0.042 (0.052)
Train: 93 [ 150/989 ( 15%)]  Loss: 3.29 (2.73)  Time: 0.630s,  203.16/s  (0.665s,  192.41/s)  LR: 5.545e-04  Data: 0.030 (0.046)
Train: 93 [ 200/989 ( 20%)]  Loss: 2.10 (2.60)  Time: 0.648s,  197.39/s  (0.663s,  192.96/s)  LR: 5.545e-04  Data: 0.022 (0.044)
Train: 93 [ 250/989 ( 25%)]  Loss: 3.50 (2.75)  Time: 0.634s,  201.74/s  (0.661s,  193.63/s)  LR: 5.545e-04  Data: 0.032 (0.044)
Train: 93 [ 300/989 ( 30%)]  Loss: 2.47 (2.71)  Time: 0.658s,  194.66/s  (0.660s,  193.92/s)  LR: 5.545e-04  Data: 0.030 (0.043)
Train: 93 [ 350/989 ( 35%)]  Loss: 3.13 (2.77)  Time: 0.638s,  200.51/s  (0.659s,  194.24/s)  LR: 5.545e-04  Data: 0.047 (0.042)
Train: 93 [ 400/989 ( 40%)]  Loss: 2.21 (2.70)  Time: 0.647s,  197.89/s  (0.659s,  194.36/s)  LR: 5.545e-04  Data: 0.039 (0.042)
Train: 93 [ 450/989 ( 46%)]  Loss: 2.58 (2.69)  Time: 0.654s,  195.69/s  (0.659s,  194.35/s)  LR: 5.545e-04  Data: 0.044 (0.042)
Train: 93 [ 500/989 ( 51%)]  Loss: 2.61 (2.68)  Time: 0.649s,  197.16/s  (0.658s,  194.48/s)  LR: 5.545e-04  Data: 0.044 (0.041)
Train: 93 [ 550/989 ( 56%)]  Loss: 3.25 (2.73)  Time: 0.664s,  192.71/s  (0.659s,  194.37/s)  LR: 5.545e-04  Data: 0.045 (0.041)
Train: 93 [ 600/989 ( 61%)]  Loss: 3.21 (2.77)  Time: 0.674s,  190.04/s  (0.659s,  194.34/s)  LR: 5.545e-04  Data: 0.029 (0.041)
Train: 93 [ 650/989 ( 66%)]  Loss: 2.18 (2.73)  Time: 0.651s,  196.68/s  (0.659s,  194.26/s)  LR: 5.545e-04  Data: 0.034 (0.041)
Train: 93 [ 700/989 ( 71%)]  Loss: 1.79 (2.66)  Time: 0.626s,  204.61/s  (0.659s,  194.17/s)  LR: 5.545e-04  Data: 0.043 (0.041)
Train: 93 [ 750/989 ( 76%)]  Loss: 2.16 (2.63)  Time: 0.647s,  197.88/s  (0.660s,  194.03/s)  LR: 5.545e-04  Data: 0.028 (0.041)
Train: 93 [ 800/989 ( 81%)]  Loss: 3.07 (2.66)  Time: 0.674s,  189.97/s  (0.660s,  193.81/s)  LR: 5.545e-04  Data: 0.037 (0.040)
Train: 93 [ 850/989 ( 86%)]  Loss: 2.94 (2.67)  Time: 0.681s,  187.87/s  (0.661s,  193.59/s)  LR: 5.545e-04  Data: 0.038 (0.040)
Train: 93 [ 900/989 ( 91%)]  Loss: 2.50 (2.66)  Time: 0.654s,  195.86/s  (0.662s,  193.33/s)  LR: 5.545e-04  Data: 0.028 (0.040)
Train: 93 [ 950/989 ( 96%)]  Loss: 2.64 (2.66)  Time: 0.684s,  187.14/s  (0.663s,  193.10/s)  LR: 5.545e-04  Data: 0.023 (0.040)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.224 (1.224)  Loss:   1.016 ( 1.016)  Acc@1:  84.375 ( 84.375)  Acc@5:  91.406 ( 91.406)
Test: [  39/39]  Time: 0.050 (0.316)  Loss:   0.613 ( 1.021)  Acc@1:  87.500 ( 80.000)  Acc@5: 100.000 ( 94.680)
Current checkpoints:
 ('./output/train/Upd_Exp3_Image100/checkpoint-85.pth.tar', 80.12)
 ('./output/train/Upd_Exp3_Image100/checkpoint-93.pth.tar', 80.0)
 ('./output/train/Upd_Exp3_Image100/checkpoint-91.pth.tar', 79.94)
 ('./output/train/Upd_Exp3_Image100/checkpoint-89.pth.tar', 79.92)
 ('./output/train/Upd_Exp3_Image100/checkpoint-87.pth.tar', 79.74)
 ('./output/train/Upd_Exp3_Image100/checkpoint-90.pth.tar', 79.68)
 ('./output/train/Upd_Exp3_Image100/checkpoint-92.pth.tar', 79.42)
 ('./output/train/Upd_Exp3_Image100/checkpoint-80.pth.tar', 79.28)
 ('./output/train/Upd_Exp3_Image100/checkpoint-86.pth.tar', 79.12)
 ('./output/train/Upd_Exp3_Image100/checkpoint-83.pth.tar', 79.1)

Train: 94 [   0/989 (  0%)]  Loss: 2.41 (2.41)  Time: 1.898s,   67.43/s  (1.898s,   67.43/s)  LR: 5.514e-04  Data: 1.141 (1.141)
Train: 94 [  50/989 (  5%)]  Loss: 1.77 (2.09)  Time: 0.642s,  199.41/s  (0.686s,  186.72/s)  LR: 5.514e-04  Data: 0.015 (0.059)
Train: 94 [ 100/989 ( 10%)]  Loss: 3.04 (2.41)  Time: 0.662s,  193.35/s  (0.668s,  191.68/s)  LR: 5.514e-04  Data: 0.050 (0.049)
Train: 94 [ 150/989 ( 15%)]  Loss: 1.99 (2.30)  Time: 0.645s,  198.51/s  (0.662s,  193.27/s)  LR: 5.514e-04  Data: 0.028 (0.046)
Train: 94 [ 200/989 ( 20%)]  Loss: 2.28 (2.30)  Time: 0.657s,  194.89/s  (0.659s,  194.29/s)  LR: 5.514e-04  Data: 0.027 (0.045)
Train: 94 [ 250/989 ( 25%)]  Loss: 3.17 (2.44)  Time: 0.658s,  194.56/s  (0.657s,  194.82/s)  LR: 5.514e-04  Data: 0.035 (0.043)
Train: 94 [ 300/989 ( 30%)]  Loss: 3.44 (2.59)  Time: 0.672s,  190.45/s  (0.656s,  195.14/s)  LR: 5.514e-04  Data: 0.028 (0.042)
Train: 94 [ 350/989 ( 35%)]  Loss: 2.13 (2.53)  Time: 0.647s,  197.68/s  (0.656s,  195.14/s)  LR: 5.514e-04  Data: 0.026 (0.042)
Train: 94 [ 400/989 ( 40%)]  Loss: 3.17 (2.60)  Time: 0.646s,  198.03/s  (0.656s,  195.25/s)  LR: 5.514e-04  Data: 0.027 (0.041)
Train: 94 [ 450/989 ( 46%)]  Loss: 3.31 (2.67)  Time: 0.666s,  192.27/s  (0.655s,  195.29/s)  LR: 5.514e-04  Data: 0.050 (0.041)
Train: 94 [ 500/989 ( 51%)]  Loss: 2.37 (2.64)  Time: 0.656s,  195.21/s  (0.655s,  195.37/s)  LR: 5.514e-04  Data: 0.053 (0.041)
Train: 94 [ 550/989 ( 56%)]  Loss: 2.46 (2.63)  Time: 0.624s,  205.15/s  (0.655s,  195.43/s)  LR: 5.514e-04  Data: 0.021 (0.040)
Train: 94 [ 600/989 ( 61%)]  Loss: 2.68 (2.63)  Time: 0.660s,  193.86/s  (0.655s,  195.43/s)  LR: 5.514e-04  Data: 0.053 (0.040)
Train: 94 [ 650/989 ( 66%)]  Loss: 3.41 (2.69)  Time: 0.649s,  197.34/s  (0.655s,  195.30/s)  LR: 5.514e-04  Data: 0.035 (0.040)
Train: 94 [ 700/989 ( 71%)]  Loss: 3.29 (2.73)  Time: 0.665s,  192.36/s  (0.656s,  195.13/s)  LR: 5.514e-04  Data: 0.036 (0.040)
Train: 94 [ 750/989 ( 76%)]  Loss: 2.88 (2.74)  Time: 0.632s,  202.48/s  (0.656s,  195.08/s)  LR: 5.514e-04  Data: 0.044 (0.040)
Train: 94 [ 800/989 ( 81%)]  Loss: 2.26 (2.71)  Time: 0.658s,  194.53/s  (0.657s,  194.92/s)  LR: 5.514e-04  Data: 0.024 (0.040)
Train: 94 [ 850/989 ( 86%)]  Loss: 2.71 (2.71)  Time: 0.682s,  187.63/s  (0.657s,  194.75/s)  LR: 5.514e-04  Data: 0.034 (0.039)
Train: 94 [ 900/989 ( 91%)]  Loss: 2.99 (2.72)  Time: 0.663s,  193.07/s  (0.658s,  194.44/s)  LR: 5.514e-04  Data: 0.045 (0.039)
Train: 94 [ 950/989 ( 96%)]  Loss: 2.93 (2.73)  Time: 0.679s,  188.62/s  (0.660s,  194.03/s)  LR: 5.514e-04  Data: 0.039 (0.039)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.226 (1.226)  Loss:   1.165 ( 1.165)  Acc@1:  74.219 ( 74.219)  Acc@5:  89.844 ( 89.844)
Test: [  39/39]  Time: 0.049 (0.318)  Loss:   0.288 ( 0.912)  Acc@1: 100.000 ( 79.840)  Acc@5: 100.000 ( 94.860)
Current checkpoints:
 ('./output/train/Upd_Exp3_Image100/checkpoint-85.pth.tar', 80.12)
 ('./output/train/Upd_Exp3_Image100/checkpoint-93.pth.tar', 80.0)
 ('./output/train/Upd_Exp3_Image100/checkpoint-91.pth.tar', 79.94)
 ('./output/train/Upd_Exp3_Image100/checkpoint-89.pth.tar', 79.92)
 ('./output/train/Upd_Exp3_Image100/checkpoint-94.pth.tar', 79.84)
 ('./output/train/Upd_Exp3_Image100/checkpoint-87.pth.tar', 79.74)
 ('./output/train/Upd_Exp3_Image100/checkpoint-90.pth.tar', 79.68)
 ('./output/train/Upd_Exp3_Image100/checkpoint-92.pth.tar', 79.42)
 ('./output/train/Upd_Exp3_Image100/checkpoint-80.pth.tar', 79.28)
 ('./output/train/Upd_Exp3_Image100/checkpoint-86.pth.tar', 79.12)

Train: 95 [   0/989 (  0%)]  Loss: 2.39 (2.39)  Time: 2.051s,   62.41/s  (2.051s,   62.41/s)  LR: 5.484e-04  Data: 1.302 (1.302)
Train: 95 [  50/989 (  5%)]  Loss: 3.02 (2.71)  Time: 0.670s,  191.11/s  (0.695s,  184.27/s)  LR: 5.484e-04  Data: 0.060 (0.067)
Train: 95 [ 100/989 ( 10%)]  Loss: 2.32 (2.58)  Time: 0.648s,  197.54/s  (0.677s,  188.93/s)  LR: 5.484e-04  Data: 0.039 (0.056)
Train: 95 [ 150/989 ( 15%)]  Loss: 2.61 (2.58)  Time: 0.663s,  193.05/s  (0.671s,  190.72/s)  LR: 5.484e-04  Data: 0.050 (0.051)
Train: 95 [ 200/989 ( 20%)]  Loss: 2.64 (2.60)  Time: 0.690s,  185.43/s  (0.669s,  191.43/s)  LR: 5.484e-04  Data: 0.054 (0.049)
Train: 95 [ 250/989 ( 25%)]  Loss: 2.01 (2.50)  Time: 0.674s,  189.80/s  (0.667s,  191.89/s)  LR: 5.484e-04  Data: 0.041 (0.047)
Train: 95 [ 300/989 ( 30%)]  Loss: 1.73 (2.39)  Time: 0.640s,  199.88/s  (0.666s,  192.12/s)  LR: 5.484e-04  Data: 0.030 (0.046)
Train: 95 [ 350/989 ( 35%)]  Loss: 1.76 (2.31)  Time: 0.651s,  196.74/s  (0.666s,  192.30/s)  LR: 5.484e-04  Data: 0.037 (0.046)
Train: 95 [ 400/989 ( 40%)]  Loss: 2.93 (2.38)  Time: 0.676s,  189.42/s  (0.665s,  192.53/s)  LR: 5.484e-04  Data: 0.039 (0.045)
Train: 95 [ 450/989 ( 46%)]  Loss: 2.46 (2.39)  Time: 0.635s,  201.60/s  (0.664s,  192.74/s)  LR: 5.484e-04  Data: 0.017 (0.045)
Train: 95 [ 500/989 ( 51%)]  Loss: 2.89 (2.43)  Time: 0.639s,  200.24/s  (0.664s,  192.87/s)  LR: 5.484e-04  Data: 0.028 (0.044)
Train: 95 [ 550/989 ( 56%)]  Loss: 3.29 (2.50)  Time: 0.635s,  201.59/s  (0.663s,  193.02/s)  LR: 5.484e-04  Data: 0.028 (0.043)
Train: 95 [ 600/989 ( 61%)]  Loss: 2.25 (2.48)  Time: 0.667s,  191.84/s  (0.663s,  193.17/s)  LR: 5.484e-04  Data: 0.041 (0.043)
Train: 95 [ 650/989 ( 66%)]  Loss: 2.01 (2.45)  Time: 0.678s,  188.87/s  (0.663s,  193.16/s)  LR: 5.484e-04  Data: 0.042 (0.043)
Train: 95 [ 700/989 ( 71%)]  Loss: 3.33 (2.51)  Time: 0.661s,  193.53/s  (0.662s,  193.25/s)  LR: 5.484e-04  Data: 0.037 (0.042)
Train: 95 [ 750/989 ( 76%)]  Loss: 1.97 (2.48)  Time: 0.655s,  195.33/s  (0.663s,  193.20/s)  LR: 5.484e-04  Data: 0.044 (0.042)
Train: 95 [ 800/989 ( 81%)]  Loss: 3.17 (2.52)  Time: 0.595s,  215.05/s  (0.663s,  193.19/s)  LR: 5.484e-04  Data: 0.035 (0.042)
slurmstepd: error: *** JOB 2007123 ON i06 CANCELLED AT 2024-03-05T21:32:17 DUE TO TIME LIMIT ***
