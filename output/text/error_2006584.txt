[2024-03-02 19:46:43,142] torch.distributed.run: [WARNING] 
[2024-03-02 19:46:43,142] torch.distributed.run: [WARNING] *****************************************
[2024-03-02 19:46:43,142] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-03-02 19:46:43,142] torch.distributed.run: [WARNING] *****************************************
Training in distributed mode with multiple processes, 1 device per process.Process 0, total 2, device cuda:0.
WARNING: No pretrained configuration specified for MONet_T model. Using a default. Please add a config to the model pretrained_cfg registry or pass explicitly.
Training in distributed mode with multiple processes, 1 device per process.Process 1, total 2, device cuda:1.
WARNING: No pretrained configuration specified for MONet_T model. Using a default. Please add a config to the model pretrained_cfg registry or pass explicitly.
Model MONet_T created, param count:10165736
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.875
	crop_mode: center
AMP not enabled. Training in float32.
Restoring model state from checkpoint...
Restoring optimizer state from checkpoint...
Loaded checkpoint '/home/sharipov/monet/output/train/Exp4_imagenet100/model_best.pth.tar' (epoch 27)
Using native Torch DistributedDataParallel.
/home/sharipov/monet/venv/lib64/python3.9/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/sharipov/monet/venv/lib64/python3.9/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Scheduled epochs: 90. LR stepped per epoch.
Train: 28 [   0/494 (  0%)]  Loss: 2.10 (2.10)  Time: 14.165s,   18.07/s  (14.165s,   18.07/s)  LR: 7.796e-05  Data: 5.885 (5.885)
Train: 28 [  50/494 ( 10%)]  Loss: 2.37 (2.24)  Time: 1.146s,  223.30/s  (1.434s,  178.49/s)  LR: 7.796e-05  Data: 0.055 (0.179)
Train: 28 [ 100/494 ( 20%)]  Loss: 2.15 (2.21)  Time: 1.261s,  202.96/s  (1.301s,  196.83/s)  LR: 7.796e-05  Data: 0.090 (0.122)
Train: 28 [ 150/494 ( 30%)]  Loss: 2.07 (2.17)  Time: 1.160s,  220.70/s  (1.254s,  204.15/s)  LR: 7.796e-05  Data: 0.063 (0.103)
Train: 28 [ 200/494 ( 41%)]  Loss: 2.05 (2.15)  Time: 1.168s,  219.11/s  (1.232s,  207.87/s)  LR: 7.796e-05  Data: 0.061 (0.094)
Train: 28 [ 250/494 ( 51%)]  Loss: 2.10 (2.14)  Time: 1.192s,  214.71/s  (1.218s,  210.10/s)  LR: 7.796e-05  Data: 0.086 (0.089)
Train: 28 [ 300/494 ( 61%)]  Loss: 2.10 (2.14)  Time: 1.167s,  219.32/s  (1.209s,  211.67/s)  LR: 7.796e-05  Data: 0.071 (0.086)
Train: 28 [ 350/494 ( 71%)]  Loss: 2.16 (2.14)  Time: 1.144s,  223.83/s  (1.202s,  212.95/s)  LR: 7.796e-05  Data: 0.064 (0.083)
Train: 28 [ 400/494 ( 81%)]  Loss: 2.15 (2.14)  Time: 1.152s,  222.13/s  (1.197s,  213.81/s)  LR: 7.796e-05  Data: 0.068 (0.082)
Train: 28 [ 450/494 ( 91%)]  Loss: 2.36 (2.16)  Time: 1.136s,  225.28/s  (1.194s,  214.48/s)  LR: 7.796e-05  Data: 0.065 (0.080)
Distributing BatchNorm running means and vars
Test: [   0/19]  Time: 6.273 (6.273)  Loss:   1.505 ( 1.505)  Acc@1:  56.250 ( 56.250)  Acc@5:  87.109 ( 87.109)
Test: [  19/19]  Time: 0.333 (0.806)  Loss:   1.373 ( 1.485)  Acc@1:  57.353 ( 61.100)  Acc@5:  88.971 ( 85.700)
Current checkpoints:
 ('./output/train/Exp5_imagenet100/checkpoint-28.pth.tar', 61.10000006103515)

Train: 29 [   0/494 (  0%)]  Loss: 2.00 (2.00)  Time: 5.431s,   47.14/s  (5.431s,   47.14/s)  LR: 7.650e-05  Data: 3.318 (3.318)
Train: 29 [  50/494 ( 10%)]  Loss: 2.11 (2.05)  Time: 1.149s,  222.76/s  (1.240s,  206.46/s)  LR: 7.650e-05  Data: 0.062 (0.134)
Train: 29 [ 100/494 ( 20%)]  Loss: 2.15 (2.09)  Time: 1.142s,  224.11/s  (1.194s,  214.47/s)  LR: 7.650e-05  Data: 0.073 (0.102)
Train: 29 [ 150/494 ( 30%)]  Loss: 2.17 (2.11)  Time: 1.136s,  225.31/s  (1.175s,  217.87/s)  LR: 7.650e-05  Data: 0.074 (0.091)
Train: 29 [ 200/494 ( 41%)]  Loss: 2.25 (2.14)  Time: 1.123s,  227.93/s  (1.167s,  219.44/s)  LR: 7.650e-05  Data: 0.075 (0.086)
Train: 29 [ 250/494 ( 51%)]  Loss: 2.16 (2.14)  Time: 1.163s,  220.19/s  (1.162s,  220.34/s)  LR: 7.650e-05  Data: 0.084 (0.082)
Train: 29 [ 300/494 ( 61%)]  Loss: 2.13 (2.14)  Time: 1.150s,  222.67/s  (1.159s,  220.91/s)  LR: 7.650e-05  Data: 0.071 (0.080)
Train: 29 [ 350/494 ( 71%)]  Loss: 2.11 (2.14)  Time: 1.163s,  220.07/s  (1.157s,  221.31/s)  LR: 7.650e-05  Data: 0.076 (0.078)
Train: 29 [ 400/494 ( 81%)]  Loss: 2.17 (2.14)  Time: 1.135s,  225.60/s  (1.156s,  221.40/s)  LR: 7.650e-05  Data: 0.062 (0.077)
Train: 29 [ 450/494 ( 91%)]  Loss: 2.18 (2.14)  Time: 1.161s,  220.53/s  (1.156s,  221.39/s)  LR: 7.650e-05  Data: 0.054 (0.076)
Distributing BatchNorm running means and vars
Test: [   0/19]  Time: 3.112 (3.112)  Loss:   1.410 ( 1.410)  Acc@1:  59.375 ( 59.375)  Acc@5:  85.547 ( 85.547)
Test: [  19/19]  Time: 0.169 (0.641)  Loss:   1.368 ( 1.450)  Acc@1:  61.765 ( 62.580)  Acc@5:  90.441 ( 86.340)
Current checkpoints:
 ('./output/train/Exp5_imagenet100/checkpoint-29.pth.tar', 62.579999993896486)
 ('./output/train/Exp5_imagenet100/checkpoint-28.pth.tar', 61.10000006103515)

Train: 30 [   0/494 (  0%)]  Loss: 2.15 (2.15)  Time: 5.108s,   50.12/s  (5.108s,   50.12/s)  LR: 7.500e-05  Data: 3.264 (3.264)
Train: 30 [  50/494 ( 10%)]  Loss: 2.33 (2.24)  Time: 1.128s,  226.96/s  (1.230s,  208.09/s)  LR: 7.500e-05  Data: 0.066 (0.134)
Train: 30 [ 100/494 ( 20%)]  Loss: 2.14 (2.20)  Time: 1.140s,  224.63/s  (1.183s,  216.47/s)  LR: 7.500e-05  Data: 0.067 (0.102)
slurmstepd: error: *** JOB 2006584 ON i45 CANCELLED AT 2024-03-02T20:08:48 ***
