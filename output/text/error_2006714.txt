[2024-03-03 10:54:04,539] torch.distributed.run: [WARNING] 
[2024-03-03 10:54:04,539] torch.distributed.run: [WARNING] *****************************************
[2024-03-03 10:54:04,539] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-03-03 10:54:04,539] torch.distributed.run: [WARNING] *****************************************
Training in distributed mode with multiple processes, 1 device per process.Process 0, total 2, device cuda:0.
WARNING: No pretrained configuration specified for MONet_T model. Using a default. Please add a config to the model pretrained_cfg registry or pass explicitly.
Training in distributed mode with multiple processes, 1 device per process.Process 1, total 2, device cuda:1.
WARNING: No pretrained configuration specified for MONet_T model. Using a default. Please add a config to the model pretrained_cfg registry or pass explicitly.
Model MONet_T created, param count:10165736
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.875
	crop_mode: center
Learning rate (0.0007071067811865476) calculated from base learning rate (0.001) and effective global batch size (128) with sqrt scaling.
AMP not enabled. Training in float32.
Restoring model state from checkpoint...
Restoring optimizer state from checkpoint...
Loaded checkpoint '/home/sharipov/monet/output/train/Upd_Exp1_Image100/model_best.pth.tar' (epoch 31)
Using native Torch DistributedDataParallel.
/home/sharipov/monet/venv/lib64/python3.9/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/sharipov/monet/venv/lib64/python3.9/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Scheduled epochs: 300. LR stepped per epoch.
Train: 32 [   0/989 (  0%)]  Loss: 3.22 (3.22)  Time: 7.195s,   17.79/s  (7.195s,   17.79/s)  LR: 6.877e-04  Data: 3.198 (3.198)
Train: 32 [  50/989 (  5%)]  Loss: 2.59 (2.90)  Time: 0.695s,  184.09/s  (0.795s,  161.03/s)  LR: 6.877e-04  Data: 0.044 (0.096)
Train: 32 [ 100/989 ( 10%)]  Loss: 3.34 (3.05)  Time: 0.663s,  193.12/s  (0.727s,  175.99/s)  LR: 6.877e-04  Data: 0.045 (0.066)
Train: 32 [ 150/989 ( 15%)]  Loss: 3.27 (3.11)  Time: 0.671s,  190.69/s  (0.704s,  181.75/s)  LR: 6.877e-04  Data: 0.047 (0.056)
Train: 32 [ 200/989 ( 20%)]  Loss: 3.61 (3.21)  Time: 0.715s,  179.04/s  (0.702s,  182.30/s)  LR: 6.877e-04  Data: 0.029 (0.050)
Train: 32 [ 250/989 ( 25%)]  Loss: 3.20 (3.21)  Time: 0.771s,  165.98/s  (0.703s,  181.95/s)  LR: 6.877e-04  Data: 0.050 (0.047)
Train: 32 [ 300/989 ( 30%)]  Loss: 3.09 (3.19)  Time: 0.617s,  207.62/s  (0.705s,  181.57/s)  LR: 6.877e-04  Data: 0.020 (0.044)
Train: 32 [ 350/989 ( 35%)]  Loss: 2.91 (3.15)  Time: 0.764s,  167.54/s  (0.706s,  181.22/s)  LR: 6.877e-04  Data: 0.052 (0.042)
Train: 32 [ 400/989 ( 40%)]  Loss: 3.46 (3.19)  Time: 0.684s,  187.25/s  (0.707s,  181.00/s)  LR: 6.877e-04  Data: 0.055 (0.041)
Train: 32 [ 450/989 ( 46%)]  Loss: 2.42 (3.11)  Time: 0.805s,  159.06/s  (0.708s,  180.79/s)  LR: 6.877e-04  Data: 0.010 (0.040)
Train: 32 [ 500/989 ( 51%)]  Loss: 3.68 (3.16)  Time: 0.676s,  189.24/s  (0.708s,  180.79/s)  LR: 6.877e-04  Data: 0.020 (0.039)
Train: 32 [ 550/989 ( 56%)]  Loss: 2.82 (3.13)  Time: 0.646s,  198.07/s  (0.709s,  180.58/s)  LR: 6.877e-04  Data: 0.018 (0.039)
Train: 32 [ 600/989 ( 61%)]  Loss: 3.51 (3.16)  Time: 0.744s,  172.06/s  (0.710s,  180.26/s)  LR: 6.877e-04  Data: 0.040 (0.038)
Train: 32 [ 650/989 ( 66%)]  Loss: 3.50 (3.19)  Time: 0.688s,  186.11/s  (0.711s,  180.08/s)  LR: 6.877e-04  Data: 0.033 (0.038)
Train: 32 [ 700/989 ( 71%)]  Loss: 3.54 (3.21)  Time: 0.676s,  189.44/s  (0.712s,  179.88/s)  LR: 6.877e-04  Data: 0.011 (0.037)
Train: 32 [ 750/989 ( 76%)]  Loss: 3.62 (3.24)  Time: 0.738s,  173.35/s  (0.713s,  179.60/s)  LR: 6.877e-04  Data: 0.020 (0.037)
Train: 32 [ 800/989 ( 81%)]  Loss: 3.59 (3.26)  Time: 0.733s,  174.51/s  (0.714s,  179.32/s)  LR: 6.877e-04  Data: 0.026 (0.036)
Train: 32 [ 850/989 ( 86%)]  Loss: 2.95 (3.24)  Time: 0.720s,  177.89/s  (0.715s,  179.05/s)  LR: 6.877e-04  Data: 0.026 (0.035)
Train: 32 [ 900/989 ( 91%)]  Loss: 2.52 (3.20)  Time: 0.705s,  181.56/s  (0.716s,  178.75/s)  LR: 6.877e-04  Data: 0.015 (0.035)
Train: 32 [ 950/989 ( 96%)]  Loss: 3.55 (3.22)  Time: 0.683s,  187.31/s  (0.717s,  178.51/s)  LR: 6.877e-04  Data: 0.020 (0.034)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 4.653 (4.653)  Loss:   1.376 ( 1.376)  Acc@1:  68.750 ( 68.750)  Acc@5:  86.719 ( 86.719)
Test: [  39/39]  Time: 0.869 (0.428)  Loss:   0.836 ( 1.304)  Acc@1:  87.500 ( 67.100)  Acc@5: 100.000 ( 89.200)
Current checkpoints:
 ('./output/train/Upd_Exp2_Image100/checkpoint-32.pth.tar', 67.1)

Train: 33 [   0/989 (  0%)]  Loss: 4.04 (4.04)  Time: 2.286s,   55.98/s  (2.286s,   55.98/s)  LR: 6.865e-04  Data: 1.193 (1.193)
Train: 33 [  50/989 (  5%)]  Loss: 3.76 (3.90)  Time: 0.635s,  201.65/s  (0.687s,  186.44/s)  LR: 6.865e-04  Data: 0.044 (0.058)
Train: 33 [ 100/989 ( 10%)]  Loss: 2.93 (3.58)  Time: 0.645s,  198.49/s  (0.671s,  190.72/s)  LR: 6.865e-04  Data: 0.045 (0.048)
Train: 33 [ 150/989 ( 15%)]  Loss: 3.01 (3.43)  Time: 0.684s,  187.02/s  (0.667s,  191.90/s)  LR: 6.865e-04  Data: 0.035 (0.045)
Train: 33 [ 200/989 ( 20%)]  Loss: 3.32 (3.41)  Time: 0.617s,  207.61/s  (0.665s,  192.59/s)  LR: 6.865e-04  Data: 0.015 (0.043)
Train: 33 [ 250/989 ( 25%)]  Loss: 2.97 (3.34)  Time: 0.625s,  204.89/s  (0.663s,  193.01/s)  LR: 6.865e-04  Data: 0.035 (0.042)
Train: 33 [ 300/989 ( 30%)]  Loss: 3.46 (3.36)  Time: 0.651s,  196.60/s  (0.662s,  193.44/s)  LR: 6.865e-04  Data: 0.027 (0.040)
Train: 33 [ 350/989 ( 35%)]  Loss: 2.92 (3.30)  Time: 0.662s,  193.44/s  (0.661s,  193.62/s)  LR: 6.865e-04  Data: 0.055 (0.040)
Train: 33 [ 400/989 ( 40%)]  Loss: 3.23 (3.29)  Time: 0.722s,  177.35/s  (0.661s,  193.57/s)  LR: 6.865e-04  Data: 0.032 (0.039)
Train: 33 [ 450/989 ( 46%)]  Loss: 3.52 (3.32)  Time: 0.644s,  198.70/s  (0.661s,  193.70/s)  LR: 6.865e-04  Data: 0.039 (0.039)
Train: 33 [ 500/989 ( 51%)]  Loss: 2.91 (3.28)  Time: 0.653s,  196.12/s  (0.661s,  193.66/s)  LR: 6.865e-04  Data: 0.041 (0.038)
Train: 33 [ 550/989 ( 56%)]  Loss: 3.68 (3.31)  Time: 0.644s,  198.82/s  (0.661s,  193.52/s)  LR: 6.865e-04  Data: 0.031 (0.038)
Train: 33 [ 600/989 ( 61%)]  Loss: 3.80 (3.35)  Time: 0.677s,  189.07/s  (0.662s,  193.42/s)  LR: 6.865e-04  Data: 0.032 (0.038)
Train: 33 [ 650/989 ( 66%)]  Loss: 3.45 (3.36)  Time: 0.650s,  196.94/s  (0.662s,  193.33/s)  LR: 6.865e-04  Data: 0.044 (0.038)
Train: 33 [ 700/989 ( 71%)]  Loss: 3.00 (3.33)  Time: 0.692s,  185.01/s  (0.662s,  193.23/s)  LR: 6.865e-04  Data: 0.029 (0.037)
Train: 33 [ 750/989 ( 76%)]  Loss: 2.50 (3.28)  Time: 0.707s,  181.09/s  (0.663s,  193.01/s)  LR: 6.865e-04  Data: 0.040 (0.037)
Train: 33 [ 800/989 ( 81%)]  Loss: 3.22 (3.28)  Time: 0.740s,  173.04/s  (0.665s,  192.56/s)  LR: 6.865e-04  Data: 0.052 (0.037)
Train: 33 [ 850/989 ( 86%)]  Loss: 3.38 (3.28)  Time: 0.739s,  173.18/s  (0.666s,  192.12/s)  LR: 6.865e-04  Data: 0.053 (0.037)
Train: 33 [ 900/989 ( 91%)]  Loss: 3.27 (3.28)  Time: 0.703s,  182.04/s  (0.668s,  191.58/s)  LR: 6.865e-04  Data: 0.047 (0.037)
Train: 33 [ 950/989 ( 96%)]  Loss: 3.01 (3.27)  Time: 0.793s,  161.38/s  (0.671s,  190.87/s)  LR: 6.865e-04  Data: 0.033 (0.037)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.241 (1.241)  Loss:   1.329 ( 1.329)  Acc@1:  72.656 ( 72.656)  Acc@5:  91.406 ( 91.406)
Test: [  39/39]  Time: 0.048 (0.318)  Loss:   0.632 ( 1.351)  Acc@1:  87.500 ( 68.400)  Acc@5: 100.000 ( 89.400)
Current checkpoints:
 ('./output/train/Upd_Exp2_Image100/checkpoint-33.pth.tar', 68.4)
 ('./output/train/Upd_Exp2_Image100/checkpoint-32.pth.tar', 67.1)

Train: 34 [   0/989 (  0%)]  Loss: 3.34 (3.34)  Time: 1.858s,   68.90/s  (1.858s,   68.90/s)  LR: 6.852e-04  Data: 1.075 (1.075)
Train: 34 [  50/989 (  5%)]  Loss: 2.67 (3.01)  Time: 0.654s,  195.80/s  (0.686s,  186.55/s)  LR: 6.852e-04  Data: 0.039 (0.058)
Train: 34 [ 100/989 ( 10%)]  Loss: 2.58 (2.86)  Time: 0.664s,  192.71/s  (0.670s,  190.95/s)  LR: 6.852e-04  Data: 0.046 (0.047)
Train: 34 [ 150/989 ( 15%)]  Loss: 3.02 (2.90)  Time: 0.640s,  199.99/s  (0.665s,  192.34/s)  LR: 6.852e-04  Data: 0.032 (0.044)
Train: 34 [ 200/989 ( 20%)]  Loss: 2.99 (2.92)  Time: 0.649s,  197.27/s  (0.663s,  193.05/s)  LR: 6.852e-04  Data: 0.026 (0.043)
Train: 34 [ 250/989 ( 25%)]  Loss: 3.57 (3.03)  Time: 0.663s,  192.95/s  (0.661s,  193.78/s)  LR: 6.852e-04  Data: 0.040 (0.040)
Train: 34 [ 300/989 ( 30%)]  Loss: 2.76 (2.99)  Time: 0.637s,  200.81/s  (0.660s,  193.98/s)  LR: 6.852e-04  Data: 0.034 (0.040)
Train: 34 [ 350/989 ( 35%)]  Loss: 2.94 (2.98)  Time: 0.641s,  199.74/s  (0.659s,  194.22/s)  LR: 6.852e-04  Data: 0.023 (0.039)
Train: 34 [ 400/989 ( 40%)]  Loss: 2.53 (2.93)  Time: 0.684s,  187.02/s  (0.658s,  194.52/s)  LR: 6.852e-04  Data: 0.024 (0.038)
Train: 34 [ 450/989 ( 46%)]  Loss: 3.55 (2.99)  Time: 0.656s,  194.97/s  (0.658s,  194.60/s)  LR: 6.852e-04  Data: 0.047 (0.038)
Train: 34 [ 500/989 ( 51%)]  Loss: 3.64 (3.05)  Time: 0.663s,  193.05/s  (0.657s,  194.80/s)  LR: 6.852e-04  Data: 0.043 (0.037)
Train: 34 [ 550/989 ( 56%)]  Loss: 3.62 (3.10)  Time: 0.699s,  183.01/s  (0.656s,  195.05/s)  LR: 6.852e-04  Data: 0.008 (0.037)
Train: 34 [ 600/989 ( 61%)]  Loss: 3.20 (3.11)  Time: 0.677s,  188.95/s  (0.656s,  195.11/s)  LR: 6.852e-04  Data: 0.051 (0.036)
Train: 34 [ 650/989 ( 66%)]  Loss: 2.99 (3.10)  Time: 0.642s,  199.43/s  (0.656s,  195.01/s)  LR: 6.852e-04  Data: 0.017 (0.036)
Train: 34 [ 700/989 ( 71%)]  Loss: 3.36 (3.12)  Time: 0.710s,  180.41/s  (0.657s,  194.89/s)  LR: 6.852e-04  Data: 0.044 (0.036)
Train: 34 [ 750/989 ( 76%)]  Loss: 3.40 (3.13)  Time: 0.712s,  179.73/s  (0.657s,  194.80/s)  LR: 6.852e-04  Data: 0.016 (0.036)
Train: 34 [ 800/989 ( 81%)]  Loss: 3.40 (3.15)  Time: 0.641s,  199.55/s  (0.658s,  194.58/s)  LR: 6.852e-04  Data: 0.011 (0.036)
Train: 34 [ 850/989 ( 86%)]  Loss: 3.29 (3.16)  Time: 0.678s,  188.70/s  (0.659s,  194.17/s)  LR: 6.852e-04  Data: 0.017 (0.036)
Train: 34 [ 900/989 ( 91%)]  Loss: 3.49 (3.18)  Time: 0.666s,  192.20/s  (0.661s,  193.73/s)  LR: 6.852e-04  Data: 0.043 (0.036)
Train: 34 [ 950/989 ( 96%)]  Loss: 3.29 (3.18)  Time: 0.674s,  189.98/s  (0.662s,  193.41/s)  LR: 6.852e-04  Data: 0.033 (0.035)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.226 (1.226)  Loss:   1.357 ( 1.357)  Acc@1:  67.969 ( 67.969)  Acc@5:  92.188 ( 92.188)
Test: [  39/39]  Time: 0.047 (0.313)  Loss:   0.378 ( 1.343)  Acc@1: 100.000 ( 67.800)  Acc@5: 100.000 ( 89.560)
Current checkpoints:
 ('./output/train/Upd_Exp2_Image100/checkpoint-33.pth.tar', 68.4)
 ('./output/train/Upd_Exp2_Image100/checkpoint-34.pth.tar', 67.8)
 ('./output/train/Upd_Exp2_Image100/checkpoint-32.pth.tar', 67.1)

Train: 35 [   0/989 (  0%)]  Loss: 3.83 (3.83)  Time: 1.845s,   69.38/s  (1.845s,   69.38/s)  LR: 6.840e-04  Data: 1.123 (1.123)
Train: 35 [  50/989 (  5%)]  Loss: 2.80 (3.31)  Time: 0.646s,  198.26/s  (0.672s,  190.48/s)  LR: 6.840e-04  Data: 0.026 (0.050)
Train: 35 [ 100/989 ( 10%)]  Loss: 2.52 (3.05)  Time: 0.636s,  201.34/s  (0.656s,  195.03/s)  LR: 6.840e-04  Data: 0.018 (0.040)
Train: 35 [ 150/989 ( 15%)]  Loss: 3.61 (3.19)  Time: 0.641s,  199.61/s  (0.651s,  196.58/s)  LR: 6.840e-04  Data: 0.017 (0.035)
Train: 35 [ 200/989 ( 20%)]  Loss: 3.21 (3.19)  Time: 0.620s,  206.48/s  (0.649s,  197.27/s)  LR: 6.840e-04  Data: 0.026 (0.034)
Train: 35 [ 250/989 ( 25%)]  Loss: 3.40 (3.23)  Time: 0.638s,  200.58/s  (0.648s,  197.56/s)  LR: 6.840e-04  Data: 0.018 (0.033)
Train: 35 [ 300/989 ( 30%)]  Loss: 3.70 (3.30)  Time: 0.662s,  193.46/s  (0.648s,  197.68/s)  LR: 6.840e-04  Data: 0.046 (0.032)
Train: 35 [ 350/989 ( 35%)]  Loss: 3.22 (3.29)  Time: 0.642s,  199.23/s  (0.647s,  197.79/s)  LR: 6.840e-04  Data: 0.016 (0.032)
Train: 35 [ 400/989 ( 40%)]  Loss: 3.15 (3.27)  Time: 0.635s,  201.73/s  (0.647s,  197.81/s)  LR: 6.840e-04  Data: 0.023 (0.032)
Train: 35 [ 450/989 ( 46%)]  Loss: 3.12 (3.26)  Time: 0.628s,  203.84/s  (0.647s,  197.86/s)  LR: 6.840e-04  Data: 0.015 (0.032)
Train: 35 [ 500/989 ( 51%)]  Loss: 3.00 (3.23)  Time: 0.661s,  193.60/s  (0.647s,  197.73/s)  LR: 6.840e-04  Data: 0.044 (0.032)
Train: 35 [ 550/989 ( 56%)]  Loss: 3.00 (3.21)  Time: 0.641s,  199.75/s  (0.648s,  197.50/s)  LR: 6.840e-04  Data: 0.025 (0.032)
Train: 35 [ 600/989 ( 61%)]  Loss: 3.51 (3.24)  Time: 0.657s,  194.68/s  (0.649s,  197.23/s)  LR: 6.840e-04  Data: 0.049 (0.033)
Train: 35 [ 650/989 ( 66%)]  Loss: 2.71 (3.20)  Time: 0.650s,  196.98/s  (0.650s,  196.95/s)  LR: 6.840e-04  Data: 0.051 (0.033)
Train: 35 [ 700/989 ( 71%)]  Loss: 3.08 (3.19)  Time: 0.652s,  196.31/s  (0.651s,  196.58/s)  LR: 6.840e-04  Data: 0.040 (0.033)
Train: 35 [ 750/989 ( 76%)]  Loss: 3.58 (3.21)  Time: 0.635s,  201.66/s  (0.652s,  196.36/s)  LR: 6.840e-04  Data: 0.023 (0.033)
Train: 35 [ 800/989 ( 81%)]  Loss: 2.78 (3.19)  Time: 0.664s,  192.64/s  (0.653s,  196.11/s)  LR: 6.840e-04  Data: 0.039 (0.034)
Train: 35 [ 850/989 ( 86%)]  Loss: 2.86 (3.17)  Time: 0.679s,  188.54/s  (0.654s,  195.83/s)  LR: 6.840e-04  Data: 0.019 (0.034)
Train: 35 [ 900/989 ( 91%)]  Loss: 2.80 (3.15)  Time: 0.691s,  185.34/s  (0.655s,  195.36/s)  LR: 6.840e-04  Data: 0.031 (0.034)
Train: 35 [ 950/989 ( 96%)]  Loss: 2.53 (3.12)  Time: 0.680s,  188.19/s  (0.657s,  194.97/s)  LR: 6.840e-04  Data: 0.041 (0.034)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.186 (1.186)  Loss:   1.652 ( 1.652)  Acc@1:  58.594 ( 58.594)  Acc@5:  85.938 ( 85.938)
Test: [  39/39]  Time: 0.100 (0.313)  Loss:   0.598 ( 1.308)  Acc@1:  87.500 ( 69.660)  Acc@5: 100.000 ( 90.160)
Current checkpoints:
 ('./output/train/Upd_Exp2_Image100/checkpoint-35.pth.tar', 69.66)
 ('./output/train/Upd_Exp2_Image100/checkpoint-33.pth.tar', 68.4)
 ('./output/train/Upd_Exp2_Image100/checkpoint-34.pth.tar', 67.8)
 ('./output/train/Upd_Exp2_Image100/checkpoint-32.pth.tar', 67.1)

Train: 36 [   0/989 (  0%)]  Loss: 3.48 (3.48)  Time: 1.831s,   69.89/s  (1.831s,   69.89/s)  LR: 6.826e-04  Data: 1.128 (1.128)
Train: 36 [  50/989 (  5%)]  Loss: 3.11 (3.30)  Time: 0.700s,  182.81/s  (0.683s,  187.34/s)  LR: 6.826e-04  Data: 0.064 (0.060)
Train: 36 [ 100/989 ( 10%)]  Loss: 3.35 (3.32)  Time: 0.633s,  202.12/s  (0.663s,  192.97/s)  LR: 6.826e-04  Data: 0.032 (0.048)
Train: 36 [ 150/989 ( 15%)]  Loss: 3.35 (3.32)  Time: 0.626s,  204.40/s  (0.658s,  194.45/s)  LR: 6.826e-04  Data: 0.028 (0.044)
Train: 36 [ 200/989 ( 20%)]  Loss: 2.49 (3.16)  Time: 0.648s,  197.41/s  (0.656s,  195.17/s)  LR: 6.826e-04  Data: 0.031 (0.042)
Train: 36 [ 250/989 ( 25%)]  Loss: 2.58 (3.06)  Time: 0.669s,  191.32/s  (0.655s,  195.49/s)  LR: 6.826e-04  Data: 0.054 (0.040)
Train: 36 [ 300/989 ( 30%)]  Loss: 3.67 (3.15)  Time: 0.646s,  198.15/s  (0.654s,  195.68/s)  LR: 6.826e-04  Data: 0.030 (0.040)
Train: 36 [ 350/989 ( 35%)]  Loss: 2.97 (3.13)  Time: 0.600s,  213.39/s  (0.654s,  195.72/s)  LR: 6.826e-04  Data: 0.019 (0.040)
Train: 36 [ 400/989 ( 40%)]  Loss: 2.66 (3.07)  Time: 0.647s,  197.74/s  (0.654s,  195.78/s)  LR: 6.826e-04  Data: 0.043 (0.039)
Train: 36 [ 450/989 ( 46%)]  Loss: 3.52 (3.12)  Time: 0.653s,  196.03/s  (0.654s,  195.79/s)  LR: 6.826e-04  Data: 0.041 (0.039)
Train: 36 [ 500/989 ( 51%)]  Loss: 3.18 (3.12)  Time: 0.658s,  194.41/s  (0.654s,  195.73/s)  LR: 6.826e-04  Data: 0.039 (0.039)
Train: 36 [ 550/989 ( 56%)]  Loss: 2.97 (3.11)  Time: 0.638s,  200.53/s  (0.654s,  195.80/s)  LR: 6.826e-04  Data: 0.027 (0.039)
Train: 36 [ 600/989 ( 61%)]  Loss: 2.95 (3.10)  Time: 0.664s,  192.84/s  (0.654s,  195.80/s)  LR: 6.826e-04  Data: 0.034 (0.039)
Train: 36 [ 650/989 ( 66%)]  Loss: 3.60 (3.13)  Time: 0.687s,  186.21/s  (0.654s,  195.71/s)  LR: 6.826e-04  Data: 0.043 (0.039)
Train: 36 [ 700/989 ( 71%)]  Loss: 2.89 (3.12)  Time: 0.648s,  197.66/s  (0.654s,  195.63/s)  LR: 6.826e-04  Data: 0.031 (0.039)
Train: 36 [ 750/989 ( 76%)]  Loss: 3.51 (3.14)  Time: 0.651s,  196.64/s  (0.654s,  195.63/s)  LR: 6.826e-04  Data: 0.048 (0.039)
Train: 36 [ 800/989 ( 81%)]  Loss: 2.35 (3.10)  Time: 0.683s,  187.35/s  (0.655s,  195.48/s)  LR: 6.826e-04  Data: 0.037 (0.038)
Train: 36 [ 850/989 ( 86%)]  Loss: 2.97 (3.09)  Time: 0.797s,  160.60/s  (0.656s,  195.16/s)  LR: 6.826e-04  Data: 0.049 (0.038)
Train: 36 [ 900/989 ( 91%)]  Loss: 2.64 (3.07)  Time: 0.682s,  187.66/s  (0.657s,  194.76/s)  LR: 6.826e-04  Data: 0.047 (0.038)
Train: 36 [ 950/989 ( 96%)]  Loss: 3.50 (3.09)  Time: 0.681s,  188.01/s  (0.659s,  194.31/s)  LR: 6.826e-04  Data: 0.053 (0.039)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.216 (1.216)  Loss:   1.681 ( 1.681)  Acc@1:  60.156 ( 60.156)  Acc@5:  87.500 ( 87.500)
Test: [  39/39]  Time: 0.048 (0.310)  Loss:   0.961 ( 1.380)  Acc@1:  75.000 ( 69.120)  Acc@5: 100.000 ( 90.620)
Current checkpoints:
 ('./output/train/Upd_Exp2_Image100/checkpoint-35.pth.tar', 69.66)
 ('./output/train/Upd_Exp2_Image100/checkpoint-36.pth.tar', 69.12)
 ('./output/train/Upd_Exp2_Image100/checkpoint-33.pth.tar', 68.4)
 ('./output/train/Upd_Exp2_Image100/checkpoint-34.pth.tar', 67.8)
 ('./output/train/Upd_Exp2_Image100/checkpoint-32.pth.tar', 67.1)

Train: 37 [   0/989 (  0%)]  Loss: 3.60 (3.60)  Time: 1.583s,   80.88/s  (1.583s,   80.88/s)  LR: 6.813e-04  Data: 0.868 (0.868)
Train: 37 [  50/989 (  5%)]  Loss: 3.37 (3.49)  Time: 0.630s,  203.21/s  (0.678s,  188.91/s)  LR: 6.813e-04  Data: 0.037 (0.054)
Train: 37 [ 100/989 ( 10%)]  Loss: 2.68 (3.22)  Time: 0.659s,  194.33/s  (0.662s,  193.32/s)  LR: 6.813e-04  Data: 0.034 (0.045)
Train: 37 [ 150/989 ( 15%)]  Loss: 2.90 (3.14)  Time: 0.636s,  201.31/s  (0.657s,  194.89/s)  LR: 6.813e-04  Data: 0.043 (0.042)
Train: 37 [ 200/989 ( 20%)]  Loss: 3.76 (3.26)  Time: 0.643s,  199.00/s  (0.655s,  195.43/s)  LR: 6.813e-04  Data: 0.015 (0.042)
Train: 37 [ 250/989 ( 25%)]  Loss: 3.45 (3.30)  Time: 0.643s,  198.93/s  (0.654s,  195.86/s)  LR: 6.813e-04  Data: 0.029 (0.040)
Train: 37 [ 300/989 ( 30%)]  Loss: 3.53 (3.33)  Time: 0.636s,  201.33/s  (0.652s,  196.18/s)  LR: 6.813e-04  Data: 0.025 (0.040)
Train: 37 [ 350/989 ( 35%)]  Loss: 2.63 (3.24)  Time: 0.653s,  195.98/s  (0.652s,  196.22/s)  LR: 6.813e-04  Data: 0.039 (0.040)
Train: 37 [ 400/989 ( 40%)]  Loss: 3.22 (3.24)  Time: 0.642s,  199.49/s  (0.652s,  196.27/s)  LR: 6.813e-04  Data: 0.039 (0.039)
Train: 37 [ 450/989 ( 46%)]  Loss: 2.80 (3.20)  Time: 0.640s,  199.97/s  (0.652s,  196.29/s)  LR: 6.813e-04  Data: 0.033 (0.039)
Train: 37 [ 500/989 ( 51%)]  Loss: 3.33 (3.21)  Time: 0.644s,  198.82/s  (0.652s,  196.23/s)  LR: 6.813e-04  Data: 0.037 (0.039)
Train: 37 [ 550/989 ( 56%)]  Loss: 3.12 (3.20)  Time: 0.631s,  202.92/s  (0.652s,  196.21/s)  LR: 6.813e-04  Data: 0.022 (0.039)
Train: 37 [ 600/989 ( 61%)]  Loss: 3.39 (3.21)  Time: 0.676s,  189.25/s  (0.653s,  196.00/s)  LR: 6.813e-04  Data: 0.050 (0.039)
Train: 37 [ 650/989 ( 66%)]  Loss: 3.49 (3.23)  Time: 0.671s,  190.69/s  (0.654s,  195.80/s)  LR: 6.813e-04  Data: 0.047 (0.039)
Train: 37 [ 700/989 ( 71%)]  Loss: 3.75 (3.27)  Time: 0.653s,  196.08/s  (0.654s,  195.61/s)  LR: 6.813e-04  Data: 0.043 (0.039)
Train: 37 [ 750/989 ( 76%)]  Loss: 3.22 (3.27)  Time: 0.657s,  194.69/s  (0.655s,  195.50/s)  LR: 6.813e-04  Data: 0.036 (0.039)
Train: 37 [ 800/989 ( 81%)]  Loss: 2.86 (3.24)  Time: 0.661s,  193.63/s  (0.655s,  195.37/s)  LR: 6.813e-04  Data: 0.028 (0.039)
Train: 37 [ 850/989 ( 86%)]  Loss: 2.94 (3.23)  Time: 0.638s,  200.58/s  (0.656s,  195.12/s)  LR: 6.813e-04  Data: 0.044 (0.039)
Train: 37 [ 900/989 ( 91%)]  Loss: 2.55 (3.19)  Time: 0.711s,  180.09/s  (0.657s,  194.83/s)  LR: 6.813e-04  Data: 0.051 (0.039)
Train: 37 [ 950/989 ( 96%)]  Loss: 2.69 (3.16)  Time: 0.681s,  188.01/s  (0.658s,  194.51/s)  LR: 6.813e-04  Data: 0.045 (0.039)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.255 (1.255)  Loss:   1.596 ( 1.596)  Acc@1:  66.406 ( 66.406)  Acc@5:  85.938 ( 85.938)
Test: [  39/39]  Time: 0.054 (0.310)  Loss:   0.571 ( 1.281)  Acc@1:  87.500 ( 69.740)  Acc@5: 100.000 ( 90.220)
Current checkpoints:
 ('./output/train/Upd_Exp2_Image100/checkpoint-37.pth.tar', 69.74)
 ('./output/train/Upd_Exp2_Image100/checkpoint-35.pth.tar', 69.66)
 ('./output/train/Upd_Exp2_Image100/checkpoint-36.pth.tar', 69.12)
 ('./output/train/Upd_Exp2_Image100/checkpoint-33.pth.tar', 68.4)
 ('./output/train/Upd_Exp2_Image100/checkpoint-34.pth.tar', 67.8)
 ('./output/train/Upd_Exp2_Image100/checkpoint-32.pth.tar', 67.1)

Train: 38 [   0/989 (  0%)]  Loss: 2.91 (2.91)  Time: 1.805s,   70.90/s  (1.805s,   70.90/s)  LR: 6.799e-04  Data: 1.083 (1.083)
Train: 38 [  50/989 (  5%)]  Loss: 3.28 (3.10)  Time: 0.682s,  187.81/s  (0.681s,  187.98/s)  LR: 6.799e-04  Data: 0.035 (0.060)
Train: 38 [ 100/989 ( 10%)]  Loss: 3.33 (3.18)  Time: 0.649s,  197.21/s  (0.663s,  193.17/s)  LR: 6.799e-04  Data: 0.043 (0.049)
Train: 38 [ 150/989 ( 15%)]  Loss: 3.08 (3.15)  Time: 0.637s,  201.06/s  (0.657s,  194.80/s)  LR: 6.799e-04  Data: 0.036 (0.046)
Train: 38 [ 200/989 ( 20%)]  Loss: 2.88 (3.10)  Time: 0.625s,  204.76/s  (0.655s,  195.53/s)  LR: 6.799e-04  Data: 0.052 (0.044)
Train: 38 [ 250/989 ( 25%)]  Loss: 2.37 (2.98)  Time: 0.638s,  200.69/s  (0.654s,  195.72/s)  LR: 6.799e-04  Data: 0.034 (0.043)
Train: 38 [ 300/989 ( 30%)]  Loss: 3.16 (3.00)  Time: 0.680s,  188.36/s  (0.654s,  195.81/s)  LR: 6.799e-04  Data: 0.059 (0.042)
Train: 38 [ 350/989 ( 35%)]  Loss: 2.47 (2.94)  Time: 0.650s,  196.89/s  (0.654s,  195.68/s)  LR: 6.799e-04  Data: 0.042 (0.042)
Train: 38 [ 400/989 ( 40%)]  Loss: 2.95 (2.94)  Time: 0.656s,  195.05/s  (0.654s,  195.60/s)  LR: 6.799e-04  Data: 0.049 (0.042)
Train: 38 [ 450/989 ( 46%)]  Loss: 3.68 (3.01)  Time: 0.595s,  215.26/s  (0.654s,  195.57/s)  LR: 6.799e-04  Data: 0.035 (0.041)
Train: 38 [ 500/989 ( 51%)]  Loss: 2.90 (3.00)  Time: 0.665s,  192.36/s  (0.654s,  195.57/s)  LR: 6.799e-04  Data: 0.048 (0.041)
Train: 38 [ 550/989 ( 56%)]  Loss: 2.77 (2.98)  Time: 0.655s,  195.52/s  (0.655s,  195.54/s)  LR: 6.799e-04  Data: 0.046 (0.041)
Train: 38 [ 600/989 ( 61%)]  Loss: 3.10 (2.99)  Time: 0.648s,  197.52/s  (0.655s,  195.45/s)  LR: 6.799e-04  Data: 0.049 (0.041)
Train: 38 [ 650/989 ( 66%)]  Loss: 3.36 (3.02)  Time: 0.636s,  201.39/s  (0.655s,  195.34/s)  LR: 6.799e-04  Data: 0.033 (0.041)
Train: 38 [ 700/989 ( 71%)]  Loss: 2.99 (3.02)  Time: 0.702s,  182.24/s  (0.655s,  195.31/s)  LR: 6.799e-04  Data: 0.036 (0.041)
Train: 38 [ 750/989 ( 76%)]  Loss: 3.51 (3.05)  Time: 0.677s,  189.18/s  (0.656s,  195.24/s)  LR: 6.799e-04  Data: 0.033 (0.040)
Train: 38 [ 800/989 ( 81%)]  Loss: 3.23 (3.06)  Time: 0.649s,  197.20/s  (0.656s,  195.08/s)  LR: 6.799e-04  Data: 0.040 (0.040)
Train: 38 [ 850/989 ( 86%)]  Loss: 3.00 (3.05)  Time: 0.675s,  189.73/s  (0.657s,  194.78/s)  LR: 6.799e-04  Data: 0.054 (0.040)
Train: 38 [ 900/989 ( 91%)]  Loss: 3.01 (3.05)  Time: 0.692s,  184.97/s  (0.658s,  194.49/s)  LR: 6.799e-04  Data: 0.029 (0.040)
Train: 38 [ 950/989 ( 96%)]  Loss: 3.11 (3.05)  Time: 0.709s,  180.66/s  (0.659s,  194.23/s)  LR: 6.799e-04  Data: 0.060 (0.040)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.253 (1.253)  Loss:   1.546 ( 1.546)  Acc@1:  63.281 ( 63.281)  Acc@5:  87.500 ( 87.500)
Test: [  39/39]  Time: 0.047 (0.312)  Loss:   0.538 ( 1.219)  Acc@1:  87.500 ( 70.640)  Acc@5: 100.000 ( 90.960)
Current checkpoints:
 ('./output/train/Upd_Exp2_Image100/checkpoint-38.pth.tar', 70.64)
 ('./output/train/Upd_Exp2_Image100/checkpoint-37.pth.tar', 69.74)
 ('./output/train/Upd_Exp2_Image100/checkpoint-35.pth.tar', 69.66)
 ('./output/train/Upd_Exp2_Image100/checkpoint-36.pth.tar', 69.12)
 ('./output/train/Upd_Exp2_Image100/checkpoint-33.pth.tar', 68.4)
 ('./output/train/Upd_Exp2_Image100/checkpoint-34.pth.tar', 67.8)
 ('./output/train/Upd_Exp2_Image100/checkpoint-32.pth.tar', 67.1)

Train: 39 [   0/989 (  0%)]  Loss: 3.37 (3.37)  Time: 1.963s,   65.21/s  (1.963s,   65.21/s)  LR: 6.784e-04  Data: 1.239 (1.239)
Train: 39 [  50/989 (  5%)]  Loss: 3.60 (3.48)  Time: 0.646s,  198.23/s  (0.681s,  187.95/s)  LR: 6.784e-04  Data: 0.058 (0.063)
Train: 39 [ 100/989 ( 10%)]  Loss: 3.01 (3.33)  Time: 0.644s,  198.72/s  (0.666s,  192.08/s)  LR: 6.784e-04  Data: 0.035 (0.049)
Train: 39 [ 150/989 ( 15%)]  Loss: 3.76 (3.43)  Time: 0.675s,  189.59/s  (0.661s,  193.63/s)  LR: 6.784e-04  Data: 0.050 (0.046)
Train: 39 [ 200/989 ( 20%)]  Loss: 3.38 (3.42)  Time: 0.611s,  209.61/s  (0.658s,  194.59/s)  LR: 6.784e-04  Data: 0.052 (0.045)
Train: 39 [ 250/989 ( 25%)]  Loss: 2.99 (3.35)  Time: 0.645s,  198.49/s  (0.657s,  194.79/s)  LR: 6.784e-04  Data: 0.008 (0.043)
Train: 39 [ 300/989 ( 30%)]  Loss: 3.69 (3.40)  Time: 0.643s,  199.08/s  (0.655s,  195.28/s)  LR: 6.784e-04  Data: 0.033 (0.042)
Train: 39 [ 350/989 ( 35%)]  Loss: 2.42 (3.28)  Time: 0.706s,  181.33/s  (0.655s,  195.48/s)  LR: 6.784e-04  Data: 0.028 (0.041)
Train: 39 [ 400/989 ( 40%)]  Loss: 3.04 (3.25)  Time: 0.664s,  192.83/s  (0.655s,  195.56/s)  LR: 6.784e-04  Data: 0.041 (0.041)
Train: 39 [ 450/989 ( 46%)]  Loss: 2.56 (3.18)  Time: 0.663s,  193.02/s  (0.654s,  195.81/s)  LR: 6.784e-04  Data: 0.040 (0.040)
Train: 39 [ 500/989 ( 51%)]  Loss: 2.81 (3.15)  Time: 0.688s,  185.94/s  (0.654s,  195.74/s)  LR: 6.784e-04  Data: 0.050 (0.040)
Train: 39 [ 550/989 ( 56%)]  Loss: 2.93 (3.13)  Time: 0.675s,  189.50/s  (0.654s,  195.73/s)  LR: 6.784e-04  Data: 0.046 (0.040)
Train: 39 [ 600/989 ( 61%)]  Loss: 3.48 (3.16)  Time: 0.645s,  198.56/s  (0.654s,  195.61/s)  LR: 6.784e-04  Data: 0.033 (0.040)
Train: 39 [ 650/989 ( 66%)]  Loss: 2.92 (3.14)  Time: 0.653s,  196.14/s  (0.654s,  195.58/s)  LR: 6.784e-04  Data: 0.037 (0.040)
Train: 39 [ 700/989 ( 71%)]  Loss: 3.16 (3.14)  Time: 0.657s,  194.78/s  (0.655s,  195.48/s)  LR: 6.784e-04  Data: 0.032 (0.040)
Train: 39 [ 750/989 ( 76%)]  Loss: 2.58 (3.11)  Time: 0.625s,  204.90/s  (0.655s,  195.31/s)  LR: 6.784e-04  Data: 0.043 (0.040)
Train: 39 [ 800/989 ( 81%)]  Loss: 2.75 (3.09)  Time: 0.758s,  168.87/s  (0.656s,  195.11/s)  LR: 6.784e-04  Data: 0.050 (0.040)
Train: 39 [ 850/989 ( 86%)]  Loss: 2.47 (3.05)  Time: 0.658s,  194.51/s  (0.657s,  194.88/s)  LR: 6.784e-04  Data: 0.036 (0.040)
Train: 39 [ 900/989 ( 91%)]  Loss: 2.84 (3.04)  Time: 0.579s,  221.11/s  (0.658s,  194.65/s)  LR: 6.784e-04  Data: 0.035 (0.040)
Train: 39 [ 950/989 ( 96%)]  Loss: 2.41 (3.01)  Time: 0.663s,  193.16/s  (0.659s,  194.22/s)  LR: 6.784e-04  Data: 0.014 (0.040)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.215 (1.215)  Loss:   1.423 ( 1.423)  Acc@1:  69.531 ( 69.531)  Acc@5:  87.500 ( 87.500)
Test: [  39/39]  Time: 0.109 (0.315)  Loss:   0.305 ( 1.283)  Acc@1: 100.000 ( 69.820)  Acc@5: 100.000 ( 90.440)
Current checkpoints:
 ('./output/train/Upd_Exp2_Image100/checkpoint-38.pth.tar', 70.64)
 ('./output/train/Upd_Exp2_Image100/checkpoint-39.pth.tar', 69.82)
 ('./output/train/Upd_Exp2_Image100/checkpoint-37.pth.tar', 69.74)
 ('./output/train/Upd_Exp2_Image100/checkpoint-35.pth.tar', 69.66)
 ('./output/train/Upd_Exp2_Image100/checkpoint-36.pth.tar', 69.12)
 ('./output/train/Upd_Exp2_Image100/checkpoint-33.pth.tar', 68.4)
 ('./output/train/Upd_Exp2_Image100/checkpoint-34.pth.tar', 67.8)
 ('./output/train/Upd_Exp2_Image100/checkpoint-32.pth.tar', 67.1)

Train: 40 [   0/989 (  0%)]  Loss: 3.30 (3.30)  Time: 1.632s,   78.42/s  (1.632s,   78.42/s)  LR: 6.770e-04  Data: 0.922 (0.922)
Train: 40 [  50/989 (  5%)]  Loss: 2.54 (2.92)  Time: 0.651s,  196.66/s  (0.674s,  189.90/s)  LR: 6.770e-04  Data: 0.040 (0.053)
Train: 40 [ 100/989 ( 10%)]  Loss: 3.70 (3.18)  Time: 0.592s,  216.30/s  (0.660s,  193.84/s)  LR: 6.770e-04  Data: 0.033 (0.047)
Train: 40 [ 150/989 ( 15%)]  Loss: 3.50 (3.26)  Time: 0.653s,  195.89/s  (0.657s,  194.72/s)  LR: 6.770e-04  Data: 0.046 (0.045)
Train: 40 [ 200/989 ( 20%)]  Loss: 2.76 (3.16)  Time: 0.633s,  202.06/s  (0.655s,  195.32/s)  LR: 6.770e-04  Data: 0.041 (0.043)
Train: 40 [ 250/989 ( 25%)]  Loss: 3.40 (3.20)  Time: 0.666s,  192.19/s  (0.654s,  195.75/s)  LR: 6.770e-04  Data: 0.054 (0.043)
Train: 40 [ 300/989 ( 30%)]  Loss: 3.59 (3.26)  Time: 0.639s,  200.33/s  (0.653s,  195.92/s)  LR: 6.770e-04  Data: 0.043 (0.042)
Train: 40 [ 350/989 ( 35%)]  Loss: 2.54 (3.17)  Time: 0.636s,  201.33/s  (0.653s,  196.11/s)  LR: 6.770e-04  Data: 0.047 (0.041)
Train: 40 [ 400/989 ( 40%)]  Loss: 3.62 (3.22)  Time: 0.715s,  179.05/s  (0.653s,  195.88/s)  LR: 6.770e-04  Data: 0.048 (0.041)
Train: 40 [ 450/989 ( 46%)]  Loss: 2.80 (3.17)  Time: 0.650s,  196.94/s  (0.653s,  195.95/s)  LR: 6.770e-04  Data: 0.043 (0.041)
Train: 40 [ 500/989 ( 51%)]  Loss: 3.50 (3.20)  Time: 0.638s,  200.56/s  (0.653s,  195.90/s)  LR: 6.770e-04  Data: 0.028 (0.041)
Train: 40 [ 550/989 ( 56%)]  Loss: 3.39 (3.22)  Time: 0.665s,  192.50/s  (0.654s,  195.77/s)  LR: 6.770e-04  Data: 0.030 (0.040)
Train: 40 [ 600/989 ( 61%)]  Loss: 2.18 (3.14)  Time: 0.611s,  209.64/s  (0.654s,  195.78/s)  LR: 6.770e-04  Data: 0.035 (0.040)
Train: 40 [ 650/989 ( 66%)]  Loss: 3.02 (3.13)  Time: 0.698s,  183.37/s  (0.654s,  195.79/s)  LR: 6.770e-04  Data: 0.055 (0.041)
Train: 40 [ 700/989 ( 71%)]  Loss: 3.05 (3.13)  Time: 0.677s,  189.04/s  (0.654s,  195.62/s)  LR: 6.770e-04  Data: 0.056 (0.041)
Train: 40 [ 750/989 ( 76%)]  Loss: 3.62 (3.16)  Time: 0.660s,  193.87/s  (0.655s,  195.45/s)  LR: 6.770e-04  Data: 0.036 (0.041)
Train: 40 [ 800/989 ( 81%)]  Loss: 3.04 (3.15)  Time: 0.655s,  195.36/s  (0.656s,  195.25/s)  LR: 6.770e-04  Data: 0.035 (0.041)
Train: 40 [ 850/989 ( 86%)]  Loss: 2.42 (3.11)  Time: 0.627s,  204.02/s  (0.657s,  194.91/s)  LR: 6.770e-04  Data: 0.042 (0.040)
Train: 40 [ 900/989 ( 91%)]  Loss: 2.83 (3.09)  Time: 0.661s,  193.51/s  (0.658s,  194.59/s)  LR: 6.770e-04  Data: 0.026 (0.040)
Train: 40 [ 950/989 ( 96%)]  Loss: 3.02 (3.09)  Time: 0.673s,  190.23/s  (0.659s,  194.36/s)  LR: 6.770e-04  Data: 0.034 (0.040)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.363 (1.363)  Loss:   1.298 ( 1.298)  Acc@1:  67.188 ( 67.188)  Acc@5:  89.844 ( 89.844)
Test: [  39/39]  Time: 0.050 (0.313)  Loss:   0.882 ( 1.173)  Acc@1:  75.000 ( 70.840)  Acc@5: 100.000 ( 91.140)
Current checkpoints:
 ('./output/train/Upd_Exp2_Image100/checkpoint-40.pth.tar', 70.84)
 ('./output/train/Upd_Exp2_Image100/checkpoint-38.pth.tar', 70.64)
 ('./output/train/Upd_Exp2_Image100/checkpoint-39.pth.tar', 69.82)
 ('./output/train/Upd_Exp2_Image100/checkpoint-37.pth.tar', 69.74)
 ('./output/train/Upd_Exp2_Image100/checkpoint-35.pth.tar', 69.66)
 ('./output/train/Upd_Exp2_Image100/checkpoint-36.pth.tar', 69.12)
 ('./output/train/Upd_Exp2_Image100/checkpoint-33.pth.tar', 68.4)
 ('./output/train/Upd_Exp2_Image100/checkpoint-34.pth.tar', 67.8)
 ('./output/train/Upd_Exp2_Image100/checkpoint-32.pth.tar', 67.1)

Train: 41 [   0/989 (  0%)]  Loss: 2.71 (2.71)  Time: 1.926s,   66.45/s  (1.926s,   66.45/s)  LR: 6.755e-04  Data: 1.120 (1.120)
Train: 41 [  50/989 (  5%)]  Loss: 2.50 (2.61)  Time: 0.617s,  207.37/s  (0.680s,  188.21/s)  LR: 6.755e-04  Data: 0.023 (0.058)
Train: 41 [ 100/989 ( 10%)]  Loss: 3.68 (2.97)  Time: 0.637s,  200.79/s  (0.664s,  192.65/s)  LR: 6.755e-04  Data: 0.017 (0.048)
Train: 41 [ 150/989 ( 15%)]  Loss: 3.56 (3.12)  Time: 0.661s,  193.73/s  (0.660s,  194.07/s)  LR: 6.755e-04  Data: 0.042 (0.045)
Train: 41 [ 200/989 ( 20%)]  Loss: 3.15 (3.12)  Time: 0.634s,  202.04/s  (0.657s,  194.89/s)  LR: 6.755e-04  Data: 0.030 (0.044)
Train: 41 [ 250/989 ( 25%)]  Loss: 2.92 (3.09)  Time: 0.642s,  199.42/s  (0.655s,  195.50/s)  LR: 6.755e-04  Data: 0.023 (0.042)
Train: 41 [ 300/989 ( 30%)]  Loss: 3.25 (3.11)  Time: 0.629s,  203.40/s  (0.654s,  195.67/s)  LR: 6.755e-04  Data: 0.030 (0.042)
Train: 41 [ 350/989 ( 35%)]  Loss: 3.10 (3.11)  Time: 0.626s,  204.55/s  (0.653s,  195.87/s)  LR: 6.755e-04  Data: 0.034 (0.040)
Train: 41 [ 400/989 ( 40%)]  Loss: 2.92 (3.09)  Time: 0.586s,  218.56/s  (0.653s,  196.10/s)  LR: 6.755e-04  Data: 0.051 (0.040)
Train: 41 [ 450/989 ( 46%)]  Loss: 3.46 (3.13)  Time: 0.634s,  202.03/s  (0.654s,  195.85/s)  LR: 6.755e-04  Data: 0.052 (0.040)
Train: 41 [ 500/989 ( 51%)]  Loss: 3.14 (3.13)  Time: 0.625s,  204.81/s  (0.653s,  195.90/s)  LR: 6.755e-04  Data: 0.029 (0.040)
Train: 41 [ 550/989 ( 56%)]  Loss: 3.56 (3.16)  Time: 0.655s,  195.53/s  (0.653s,  195.89/s)  LR: 6.755e-04  Data: 0.031 (0.040)
Train: 41 [ 600/989 ( 61%)]  Loss: 2.96 (3.15)  Time: 0.661s,  193.73/s  (0.654s,  195.83/s)  LR: 6.755e-04  Data: 0.050 (0.040)
Train: 41 [ 650/989 ( 66%)]  Loss: 3.01 (3.14)  Time: 0.697s,  183.55/s  (0.654s,  195.73/s)  LR: 6.755e-04  Data: 0.045 (0.039)
Train: 41 [ 700/989 ( 71%)]  Loss: 3.33 (3.15)  Time: 0.666s,  192.09/s  (0.655s,  195.56/s)  LR: 6.755e-04  Data: 0.050 (0.040)
Train: 41 [ 750/989 ( 76%)]  Loss: 3.04 (3.14)  Time: 0.684s,  187.09/s  (0.655s,  195.48/s)  LR: 6.755e-04  Data: 0.056 (0.039)
Train: 41 [ 800/989 ( 81%)]  Loss: 2.67 (3.12)  Time: 0.682s,  187.65/s  (0.656s,  195.19/s)  LR: 6.755e-04  Data: 0.052 (0.039)
Train: 41 [ 850/989 ( 86%)]  Loss: 2.57 (3.08)  Time: 0.670s,  191.01/s  (0.657s,  194.86/s)  LR: 6.755e-04  Data: 0.046 (0.039)
Train: 41 [ 900/989 ( 91%)]  Loss: 2.27 (3.04)  Time: 0.663s,  192.93/s  (0.658s,  194.50/s)  LR: 6.755e-04  Data: 0.038 (0.039)
Train: 41 [ 950/989 ( 96%)]  Loss: 2.69 (3.02)  Time: 0.642s,  199.38/s  (0.659s,  194.18/s)  LR: 6.755e-04  Data: 0.050 (0.039)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.247 (1.247)  Loss:   1.302 ( 1.302)  Acc@1:  75.000 ( 75.000)  Acc@5:  87.500 ( 87.500)
Test: [  39/39]  Time: 0.046 (0.315)  Loss:   0.944 ( 1.205)  Acc@1:  75.000 ( 71.020)  Acc@5: 100.000 ( 91.160)
Current checkpoints:
 ('./output/train/Upd_Exp2_Image100/checkpoint-41.pth.tar', 71.02)
 ('./output/train/Upd_Exp2_Image100/checkpoint-40.pth.tar', 70.84)
 ('./output/train/Upd_Exp2_Image100/checkpoint-38.pth.tar', 70.64)
 ('./output/train/Upd_Exp2_Image100/checkpoint-39.pth.tar', 69.82)
 ('./output/train/Upd_Exp2_Image100/checkpoint-37.pth.tar', 69.74)
 ('./output/train/Upd_Exp2_Image100/checkpoint-35.pth.tar', 69.66)
 ('./output/train/Upd_Exp2_Image100/checkpoint-36.pth.tar', 69.12)
 ('./output/train/Upd_Exp2_Image100/checkpoint-33.pth.tar', 68.4)
 ('./output/train/Upd_Exp2_Image100/checkpoint-34.pth.tar', 67.8)
 ('./output/train/Upd_Exp2_Image100/checkpoint-32.pth.tar', 67.1)

Train: 42 [   0/989 (  0%)]  Loss: 3.14 (3.14)  Time: 1.918s,   66.72/s  (1.918s,   66.72/s)  LR: 6.739e-04  Data: 1.134 (1.134)
Train: 42 [  50/989 (  5%)]  Loss: 3.15 (3.15)  Time: 0.649s,  197.36/s  (0.684s,  187.16/s)  LR: 6.739e-04  Data: 0.043 (0.054)
Train: 42 [ 100/989 ( 10%)]  Loss: 3.34 (3.21)  Time: 0.650s,  197.05/s  (0.668s,  191.66/s)  LR: 6.739e-04  Data: 0.025 (0.047)
Train: 42 [ 150/989 ( 15%)]  Loss: 2.82 (3.11)  Time: 0.671s,  190.69/s  (0.663s,  193.09/s)  LR: 6.739e-04  Data: 0.050 (0.044)
Train: 42 [ 200/989 ( 20%)]  Loss: 3.20 (3.13)  Time: 0.642s,  199.34/s  (0.660s,  193.85/s)  LR: 6.739e-04  Data: 0.036 (0.042)
Train: 42 [ 250/989 ( 25%)]  Loss: 3.11 (3.13)  Time: 0.647s,  197.79/s  (0.659s,  194.35/s)  LR: 6.739e-04  Data: 0.040 (0.042)
Train: 42 [ 300/989 ( 30%)]  Loss: 3.51 (3.18)  Time: 0.646s,  198.26/s  (0.658s,  194.47/s)  LR: 6.739e-04  Data: 0.043 (0.041)
Train: 42 [ 350/989 ( 35%)]  Loss: 3.15 (3.18)  Time: 0.644s,  198.87/s  (0.658s,  194.59/s)  LR: 6.739e-04  Data: 0.035 (0.041)
Train: 42 [ 400/989 ( 40%)]  Loss: 3.24 (3.18)  Time: 0.677s,  188.99/s  (0.657s,  194.88/s)  LR: 6.739e-04  Data: 0.058 (0.040)
Train: 42 [ 450/989 ( 46%)]  Loss: 2.73 (3.14)  Time: 0.645s,  198.44/s  (0.657s,  194.90/s)  LR: 6.739e-04  Data: 0.042 (0.039)
Train: 42 [ 500/989 ( 51%)]  Loss: 2.93 (3.12)  Time: 0.653s,  196.08/s  (0.656s,  195.06/s)  LR: 6.739e-04  Data: 0.040 (0.039)
Train: 42 [ 550/989 ( 56%)]  Loss: 3.21 (3.13)  Time: 0.659s,  194.28/s  (0.656s,  195.18/s)  LR: 6.739e-04  Data: 0.042 (0.039)
Train: 42 [ 600/989 ( 61%)]  Loss: 3.46 (3.15)  Time: 0.614s,  208.57/s  (0.655s,  195.30/s)  LR: 6.739e-04  Data: 0.045 (0.039)
Train: 42 [ 650/989 ( 66%)]  Loss: 3.79 (3.20)  Time: 0.659s,  194.33/s  (0.655s,  195.28/s)  LR: 6.739e-04  Data: 0.044 (0.039)
Train: 42 [ 700/989 ( 71%)]  Loss: 2.74 (3.17)  Time: 0.641s,  199.59/s  (0.656s,  195.26/s)  LR: 6.739e-04  Data: 0.016 (0.039)
Train: 42 [ 750/989 ( 76%)]  Loss: 3.38 (3.18)  Time: 0.680s,  188.33/s  (0.656s,  195.11/s)  LR: 6.739e-04  Data: 0.038 (0.039)
Train: 42 [ 800/989 ( 81%)]  Loss: 3.18 (3.18)  Time: 0.626s,  204.51/s  (0.657s,  194.86/s)  LR: 6.739e-04  Data: 0.038 (0.039)
Train: 42 [ 850/989 ( 86%)]  Loss: 2.92 (3.17)  Time: 0.640s,  199.94/s  (0.658s,  194.66/s)  LR: 6.739e-04  Data: 0.031 (0.039)
Train: 42 [ 900/989 ( 91%)]  Loss: 3.64 (3.19)  Time: 0.624s,  204.97/s  (0.659s,  194.31/s)  LR: 6.739e-04  Data: 0.024 (0.039)
Train: 42 [ 950/989 ( 96%)]  Loss: 3.06 (3.19)  Time: 0.763s,  167.80/s  (0.660s,  193.91/s)  LR: 6.739e-04  Data: 0.046 (0.039)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.191 (1.191)  Loss:   1.511 ( 1.511)  Acc@1:  66.406 ( 66.406)  Acc@5:  90.625 ( 90.625)
Test: [  39/39]  Time: 0.049 (0.313)  Loss:   0.779 ( 1.276)  Acc@1:  87.500 ( 72.280)  Acc@5: 100.000 ( 91.720)
Current checkpoints:
 ('./output/train/Upd_Exp2_Image100/checkpoint-42.pth.tar', 72.28)
 ('./output/train/Upd_Exp2_Image100/checkpoint-41.pth.tar', 71.02)
 ('./output/train/Upd_Exp2_Image100/checkpoint-40.pth.tar', 70.84)
 ('./output/train/Upd_Exp2_Image100/checkpoint-38.pth.tar', 70.64)
 ('./output/train/Upd_Exp2_Image100/checkpoint-39.pth.tar', 69.82)
 ('./output/train/Upd_Exp2_Image100/checkpoint-37.pth.tar', 69.74)
 ('./output/train/Upd_Exp2_Image100/checkpoint-35.pth.tar', 69.66)
 ('./output/train/Upd_Exp2_Image100/checkpoint-36.pth.tar', 69.12)
 ('./output/train/Upd_Exp2_Image100/checkpoint-33.pth.tar', 68.4)
 ('./output/train/Upd_Exp2_Image100/checkpoint-34.pth.tar', 67.8)

Train: 43 [   0/989 (  0%)]  Loss: 3.66 (3.66)  Time: 1.873s,   68.35/s  (1.873s,   68.35/s)  LR: 6.724e-04  Data: 1.102 (1.102)
Train: 43 [  50/989 (  5%)]  Loss: 3.11 (3.39)  Time: 0.638s,  200.70/s  (0.679s,  188.61/s)  LR: 6.724e-04  Data: 0.035 (0.060)
Train: 43 [ 100/989 ( 10%)]  Loss: 3.33 (3.37)  Time: 0.641s,  199.65/s  (0.664s,  192.90/s)  LR: 6.724e-04  Data: 0.054 (0.049)
Train: 43 [ 150/989 ( 15%)]  Loss: 3.57 (3.42)  Time: 0.631s,  202.80/s  (0.659s,  194.37/s)  LR: 6.724e-04  Data: 0.035 (0.045)
Train: 43 [ 200/989 ( 20%)]  Loss: 2.93 (3.32)  Time: 0.647s,  197.75/s  (0.656s,  195.20/s)  LR: 6.724e-04  Data: 0.039 (0.043)
Train: 43 [ 250/989 ( 25%)]  Loss: 2.99 (3.27)  Time: 0.697s,  183.54/s  (0.655s,  195.49/s)  LR: 6.724e-04  Data: 0.032 (0.042)
Train: 43 [ 300/989 ( 30%)]  Loss: 2.55 (3.16)  Time: 0.659s,  194.38/s  (0.654s,  195.78/s)  LR: 6.724e-04  Data: 0.034 (0.042)
Train: 43 [ 350/989 ( 35%)]  Loss: 3.55 (3.21)  Time: 0.654s,  195.83/s  (0.653s,  196.06/s)  LR: 6.724e-04  Data: 0.043 (0.041)
Train: 43 [ 400/989 ( 40%)]  Loss: 3.18 (3.21)  Time: 0.656s,  195.23/s  (0.652s,  196.20/s)  LR: 6.724e-04  Data: 0.052 (0.041)
Train: 43 [ 450/989 ( 46%)]  Loss: 3.06 (3.19)  Time: 0.629s,  203.64/s  (0.652s,  196.24/s)  LR: 6.724e-04  Data: 0.051 (0.041)
Train: 43 [ 500/989 ( 51%)]  Loss: 2.92 (3.17)  Time: 0.661s,  193.71/s  (0.652s,  196.21/s)  LR: 6.724e-04  Data: 0.028 (0.040)
Train: 43 [ 550/989 ( 56%)]  Loss: 2.82 (3.14)  Time: 0.634s,  201.94/s  (0.653s,  196.10/s)  LR: 6.724e-04  Data: 0.037 (0.040)
Train: 43 [ 600/989 ( 61%)]  Loss: 2.66 (3.10)  Time: 0.637s,  200.92/s  (0.653s,  195.94/s)  LR: 6.724e-04  Data: 0.040 (0.040)
Train: 43 [ 650/989 ( 66%)]  Loss: 3.25 (3.11)  Time: 0.657s,  194.94/s  (0.654s,  195.84/s)  LR: 6.724e-04  Data: 0.036 (0.040)
Train: 43 [ 700/989 ( 71%)]  Loss: 3.41 (3.13)  Time: 0.639s,  200.25/s  (0.654s,  195.68/s)  LR: 6.724e-04  Data: 0.034 (0.040)
Train: 43 [ 750/989 ( 76%)]  Loss: 3.24 (3.14)  Time: 0.652s,  196.35/s  (0.655s,  195.44/s)  LR: 6.724e-04  Data: 0.039 (0.040)
Train: 43 [ 800/989 ( 81%)]  Loss: 2.67 (3.11)  Time: 0.647s,  197.81/s  (0.656s,  195.23/s)  LR: 6.724e-04  Data: 0.039 (0.040)
Train: 43 [ 850/989 ( 86%)]  Loss: 2.98 (3.10)  Time: 0.640s,  199.94/s  (0.657s,  194.84/s)  LR: 6.724e-04  Data: 0.040 (0.040)
Train: 43 [ 900/989 ( 91%)]  Loss: 2.88 (3.09)  Time: 0.710s,  180.21/s  (0.658s,  194.39/s)  LR: 6.724e-04  Data: 0.027 (0.040)
Train: 43 [ 950/989 ( 96%)]  Loss: 3.77 (3.13)  Time: 0.669s,  191.19/s  (0.659s,  194.11/s)  LR: 6.724e-04  Data: 0.050 (0.040)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.197 (1.197)  Loss:   1.268 ( 1.268)  Acc@1:  69.531 ( 69.531)  Acc@5:  91.406 ( 91.406)
Test: [  39/39]  Time: 0.049 (0.311)  Loss:   1.475 ( 1.177)  Acc@1:  62.500 ( 72.220)  Acc@5:  87.500 ( 91.980)
Current checkpoints:
 ('./output/train/Upd_Exp2_Image100/checkpoint-42.pth.tar', 72.28)
 ('./output/train/Upd_Exp2_Image100/checkpoint-43.pth.tar', 72.22)
 ('./output/train/Upd_Exp2_Image100/checkpoint-41.pth.tar', 71.02)
 ('./output/train/Upd_Exp2_Image100/checkpoint-40.pth.tar', 70.84)
 ('./output/train/Upd_Exp2_Image100/checkpoint-38.pth.tar', 70.64)
 ('./output/train/Upd_Exp2_Image100/checkpoint-39.pth.tar', 69.82)
 ('./output/train/Upd_Exp2_Image100/checkpoint-37.pth.tar', 69.74)
 ('./output/train/Upd_Exp2_Image100/checkpoint-35.pth.tar', 69.66)
 ('./output/train/Upd_Exp2_Image100/checkpoint-36.pth.tar', 69.12)
 ('./output/train/Upd_Exp2_Image100/checkpoint-33.pth.tar', 68.4)

Train: 44 [   0/989 (  0%)]  Loss: 3.33 (3.33)  Time: 1.895s,   67.54/s  (1.895s,   67.54/s)  LR: 6.708e-04  Data: 1.152 (1.152)
Train: 44 [  50/989 (  5%)]  Loss: 3.29 (3.31)  Time: 0.640s,  199.95/s  (0.679s,  188.54/s)  LR: 6.708e-04  Data: 0.045 (0.060)
Train: 44 [ 100/989 ( 10%)]  Loss: 3.24 (3.29)  Time: 0.645s,  198.35/s  (0.662s,  193.41/s)  LR: 6.708e-04  Data: 0.023 (0.048)
Train: 44 [ 150/989 ( 15%)]  Loss: 3.54 (3.35)  Time: 0.653s,  196.00/s  (0.656s,  195.11/s)  LR: 6.708e-04  Data: 0.048 (0.045)
Train: 44 [ 200/989 ( 20%)]  Loss: 3.06 (3.29)  Time: 0.713s,  179.59/s  (0.656s,  195.24/s)  LR: 6.708e-04  Data: 0.064 (0.042)
Train: 44 [ 250/989 ( 25%)]  Loss: 3.57 (3.34)  Time: 0.639s,  200.23/s  (0.655s,  195.28/s)  LR: 6.708e-04  Data: 0.023 (0.041)
Train: 44 [ 300/989 ( 30%)]  Loss: 2.65 (3.24)  Time: 0.651s,  196.73/s  (0.655s,  195.33/s)  LR: 6.708e-04  Data: 0.045 (0.040)
Train: 44 [ 350/989 ( 35%)]  Loss: 2.78 (3.18)  Time: 0.656s,  195.21/s  (0.655s,  195.32/s)  LR: 6.708e-04  Data: 0.033 (0.041)
Train: 44 [ 400/989 ( 40%)]  Loss: 2.87 (3.15)  Time: 0.647s,  197.87/s  (0.655s,  195.29/s)  LR: 6.708e-04  Data: 0.039 (0.040)
Train: 44 [ 450/989 ( 46%)]  Loss: 2.98 (3.13)  Time: 0.636s,  201.19/s  (0.656s,  195.24/s)  LR: 6.708e-04  Data: 0.021 (0.040)
Train: 44 [ 500/989 ( 51%)]  Loss: 2.57 (3.08)  Time: 0.641s,  199.68/s  (0.656s,  195.18/s)  LR: 6.708e-04  Data: 0.035 (0.040)
Train: 44 [ 550/989 ( 56%)]  Loss: 2.81 (3.06)  Time: 0.637s,  200.90/s  (0.656s,  195.15/s)  LR: 6.708e-04  Data: 0.043 (0.040)
Train: 44 [ 600/989 ( 61%)]  Loss: 3.49 (3.09)  Time: 0.666s,  192.22/s  (0.656s,  195.08/s)  LR: 6.708e-04  Data: 0.016 (0.040)
Train: 44 [ 650/989 ( 66%)]  Loss: 3.00 (3.08)  Time: 0.655s,  195.36/s  (0.656s,  195.08/s)  LR: 6.708e-04  Data: 0.037 (0.040)
Train: 44 [ 700/989 ( 71%)]  Loss: 2.77 (3.06)  Time: 0.724s,  176.77/s  (0.657s,  194.90/s)  LR: 6.708e-04  Data: 0.057 (0.040)
Train: 44 [ 750/989 ( 76%)]  Loss: 3.42 (3.09)  Time: 0.646s,  198.18/s  (0.657s,  194.78/s)  LR: 6.708e-04  Data: 0.031 (0.040)
Train: 44 [ 800/989 ( 81%)]  Loss: 3.33 (3.10)  Time: 0.592s,  216.05/s  (0.658s,  194.54/s)  LR: 6.708e-04  Data: 0.030 (0.039)
Train: 44 [ 850/989 ( 86%)]  Loss: 2.58 (3.07)  Time: 0.628s,  203.84/s  (0.659s,  194.28/s)  LR: 6.708e-04  Data: 0.024 (0.039)
Train: 44 [ 900/989 ( 91%)]  Loss: 2.84 (3.06)  Time: 0.665s,  192.61/s  (0.660s,  194.00/s)  LR: 6.708e-04  Data: 0.039 (0.039)
Train: 44 [ 950/989 ( 96%)]  Loss: 3.04 (3.06)  Time: 0.669s,  191.26/s  (0.661s,  193.76/s)  LR: 6.708e-04  Data: 0.036 (0.039)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.220 (1.220)  Loss:   1.273 ( 1.273)  Acc@1:  69.531 ( 69.531)  Acc@5:  88.281 ( 88.281)
Test: [  39/39]  Time: 0.059 (0.316)  Loss:   0.805 ( 1.189)  Acc@1:  62.500 ( 70.700)  Acc@5: 100.000 ( 91.060)
Current checkpoints:
 ('./output/train/Upd_Exp2_Image100/checkpoint-42.pth.tar', 72.28)
 ('./output/train/Upd_Exp2_Image100/checkpoint-43.pth.tar', 72.22)
 ('./output/train/Upd_Exp2_Image100/checkpoint-41.pth.tar', 71.02)
 ('./output/train/Upd_Exp2_Image100/checkpoint-40.pth.tar', 70.84)
 ('./output/train/Upd_Exp2_Image100/checkpoint-44.pth.tar', 70.7)
 ('./output/train/Upd_Exp2_Image100/checkpoint-38.pth.tar', 70.64)
 ('./output/train/Upd_Exp2_Image100/checkpoint-39.pth.tar', 69.82)
 ('./output/train/Upd_Exp2_Image100/checkpoint-37.pth.tar', 69.74)
 ('./output/train/Upd_Exp2_Image100/checkpoint-35.pth.tar', 69.66)
 ('./output/train/Upd_Exp2_Image100/checkpoint-36.pth.tar', 69.12)

Train: 45 [   0/989 (  0%)]  Loss: 2.80 (2.80)  Time: 1.650s,   77.58/s  (1.650s,   77.58/s)  LR: 6.691e-04  Data: 0.912 (0.912)
Train: 45 [  50/989 (  5%)]  Loss: 3.06 (2.93)  Time: 0.659s,  194.22/s  (0.678s,  188.82/s)  LR: 6.691e-04  Data: 0.039 (0.056)
Train: 45 [ 100/989 ( 10%)]  Loss: 2.87 (2.91)  Time: 0.649s,  197.28/s  (0.663s,  193.04/s)  LR: 6.691e-04  Data: 0.034 (0.048)
Train: 45 [ 150/989 ( 15%)]  Loss: 2.41 (2.79)  Time: 0.652s,  196.34/s  (0.658s,  194.55/s)  LR: 6.691e-04  Data: 0.023 (0.044)
Train: 45 [ 200/989 ( 20%)]  Loss: 2.69 (2.77)  Time: 0.639s,  200.34/s  (0.655s,  195.40/s)  LR: 6.691e-04  Data: 0.040 (0.043)
Train: 45 [ 250/989 ( 25%)]  Loss: 2.49 (2.72)  Time: 0.626s,  204.53/s  (0.654s,  195.86/s)  LR: 6.691e-04  Data: 0.038 (0.043)
Train: 45 [ 300/989 ( 30%)]  Loss: 2.73 (2.72)  Time: 0.652s,  196.27/s  (0.653s,  196.08/s)  LR: 6.691e-04  Data: 0.023 (0.042)
Train: 45 [ 350/989 ( 35%)]  Loss: 3.08 (2.77)  Time: 0.643s,  199.14/s  (0.652s,  196.25/s)  LR: 6.691e-04  Data: 0.034 (0.041)
Train: 45 [ 400/989 ( 40%)]  Loss: 3.17 (2.81)  Time: 0.654s,  195.84/s  (0.652s,  196.19/s)  LR: 6.691e-04  Data: 0.038 (0.041)
Train: 45 [ 450/989 ( 46%)]  Loss: 2.69 (2.80)  Time: 0.688s,  186.03/s  (0.652s,  196.33/s)  LR: 6.691e-04  Data: 0.039 (0.041)
Train: 45 [ 500/989 ( 51%)]  Loss: 2.26 (2.75)  Time: 0.658s,  194.66/s  (0.652s,  196.22/s)  LR: 6.691e-04  Data: 0.040 (0.041)
Train: 45 [ 550/989 ( 56%)]  Loss: 3.22 (2.79)  Time: 0.656s,  195.23/s  (0.652s,  196.20/s)  LR: 6.691e-04  Data: 0.023 (0.041)
Train: 45 [ 600/989 ( 61%)]  Loss: 2.86 (2.80)  Time: 0.647s,  197.79/s  (0.652s,  196.30/s)  LR: 6.691e-04  Data: 0.041 (0.041)
Train: 45 [ 650/989 ( 66%)]  Loss: 3.40 (2.84)  Time: 0.651s,  196.74/s  (0.652s,  196.18/s)  LR: 6.691e-04  Data: 0.030 (0.041)
Train: 45 [ 700/989 ( 71%)]  Loss: 3.07 (2.85)  Time: 0.657s,  194.77/s  (0.653s,  196.17/s)  LR: 6.691e-04  Data: 0.040 (0.040)
Train: 45 [ 750/989 ( 76%)]  Loss: 2.72 (2.85)  Time: 0.610s,  209.69/s  (0.653s,  195.99/s)  LR: 6.691e-04  Data: 0.029 (0.040)
Train: 45 [ 800/989 ( 81%)]  Loss: 2.87 (2.85)  Time: 0.783s,  163.49/s  (0.654s,  195.77/s)  LR: 6.691e-04  Data: 0.029 (0.040)
Train: 45 [ 850/989 ( 86%)]  Loss: 3.07 (2.86)  Time: 0.646s,  198.07/s  (0.655s,  195.48/s)  LR: 6.691e-04  Data: 0.040 (0.040)
Train: 45 [ 900/989 ( 91%)]  Loss: 2.87 (2.86)  Time: 0.673s,  190.28/s  (0.656s,  195.12/s)  LR: 6.691e-04  Data: 0.042 (0.039)
Train: 45 [ 950/989 ( 96%)]  Loss: 3.25 (2.88)  Time: 0.642s,  199.34/s  (0.657s,  194.81/s)  LR: 6.691e-04  Data: 0.022 (0.039)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.203 (1.203)  Loss:   1.474 ( 1.474)  Acc@1:  67.969 ( 67.969)  Acc@5:  87.500 ( 87.500)
Test: [  39/39]  Time: 0.048 (0.315)  Loss:   0.817 ( 1.173)  Acc@1:  87.500 ( 72.040)  Acc@5:  87.500 ( 91.420)
Current checkpoints:
 ('./output/train/Upd_Exp2_Image100/checkpoint-42.pth.tar', 72.28)
 ('./output/train/Upd_Exp2_Image100/checkpoint-43.pth.tar', 72.22)
 ('./output/train/Upd_Exp2_Image100/checkpoint-45.pth.tar', 72.04)
 ('./output/train/Upd_Exp2_Image100/checkpoint-41.pth.tar', 71.02)
 ('./output/train/Upd_Exp2_Image100/checkpoint-40.pth.tar', 70.84)
 ('./output/train/Upd_Exp2_Image100/checkpoint-44.pth.tar', 70.7)
 ('./output/train/Upd_Exp2_Image100/checkpoint-38.pth.tar', 70.64)
 ('./output/train/Upd_Exp2_Image100/checkpoint-39.pth.tar', 69.82)
 ('./output/train/Upd_Exp2_Image100/checkpoint-37.pth.tar', 69.74)
 ('./output/train/Upd_Exp2_Image100/checkpoint-35.pth.tar', 69.66)

Train: 46 [   0/989 (  0%)]  Loss: 2.51 (2.51)  Time: 1.965s,   65.15/s  (1.965s,   65.15/s)  LR: 6.674e-04  Data: 1.238 (1.238)
Train: 46 [  50/989 (  5%)]  Loss: 2.95 (2.73)  Time: 0.688s,  186.02/s  (0.685s,  186.90/s)  LR: 6.674e-04  Data: 0.039 (0.062)
Train: 46 [ 100/989 ( 10%)]  Loss: 3.15 (2.87)  Time: 0.629s,  203.56/s  (0.667s,  191.81/s)  LR: 6.674e-04  Data: 0.034 (0.051)
Train: 46 [ 150/989 ( 15%)]  Loss: 3.37 (3.00)  Time: 0.649s,  197.21/s  (0.661s,  193.66/s)  LR: 6.674e-04  Data: 0.044 (0.046)
Train: 46 [ 200/989 ( 20%)]  Loss: 2.60 (2.92)  Time: 0.651s,  196.63/s  (0.658s,  194.54/s)  LR: 6.674e-04  Data: 0.042 (0.045)
Train: 46 [ 250/989 ( 25%)]  Loss: 2.94 (2.92)  Time: 0.668s,  191.61/s  (0.657s,  194.93/s)  LR: 6.674e-04  Data: 0.064 (0.044)
Train: 46 [ 300/989 ( 30%)]  Loss: 3.14 (2.95)  Time: 0.639s,  200.28/s  (0.655s,  195.31/s)  LR: 6.674e-04  Data: 0.046 (0.043)
Train: 46 [ 350/989 ( 35%)]  Loss: 2.85 (2.94)  Time: 0.606s,  211.06/s  (0.655s,  195.33/s)  LR: 6.674e-04  Data: 0.031 (0.042)
Train: 46 [ 400/989 ( 40%)]  Loss: 3.65 (3.02)  Time: 0.655s,  195.56/s  (0.655s,  195.40/s)  LR: 6.674e-04  Data: 0.039 (0.042)
Train: 46 [ 450/989 ( 46%)]  Loss: 2.47 (2.96)  Time: 0.699s,  183.15/s  (0.654s,  195.62/s)  LR: 6.674e-04  Data: 0.049 (0.041)
Train: 46 [ 500/989 ( 51%)]  Loss: 3.08 (2.97)  Time: 0.645s,  198.31/s  (0.654s,  195.59/s)  LR: 6.674e-04  Data: 0.041 (0.041)
Train: 46 [ 550/989 ( 56%)]  Loss: 2.89 (2.97)  Time: 0.701s,  182.49/s  (0.654s,  195.60/s)  LR: 6.674e-04  Data: 0.043 (0.041)
Train: 46 [ 600/989 ( 61%)]  Loss: 2.51 (2.93)  Time: 0.632s,  202.47/s  (0.654s,  195.59/s)  LR: 6.674e-04  Data: 0.031 (0.041)
Train: 46 [ 650/989 ( 66%)]  Loss: 2.68 (2.91)  Time: 0.646s,  198.19/s  (0.655s,  195.54/s)  LR: 6.674e-04  Data: 0.037 (0.040)
Train: 46 [ 700/989 ( 71%)]  Loss: 2.55 (2.89)  Time: 0.677s,  188.93/s  (0.655s,  195.52/s)  LR: 6.674e-04  Data: 0.041 (0.040)
Train: 46 [ 750/989 ( 76%)]  Loss: 2.34 (2.85)  Time: 0.790s,  161.95/s  (0.655s,  195.37/s)  LR: 6.674e-04  Data: 0.043 (0.040)
Train: 46 [ 800/989 ( 81%)]  Loss: 3.20 (2.88)  Time: 0.663s,  193.03/s  (0.656s,  195.12/s)  LR: 6.674e-04  Data: 0.037 (0.040)
Train: 46 [ 850/989 ( 86%)]  Loss: 3.12 (2.89)  Time: 0.660s,  193.80/s  (0.657s,  194.90/s)  LR: 6.674e-04  Data: 0.037 (0.040)
Train: 46 [ 900/989 ( 91%)]  Loss: 2.36 (2.86)  Time: 0.655s,  195.31/s  (0.658s,  194.57/s)  LR: 6.674e-04  Data: 0.028 (0.040)
Train: 46 [ 950/989 ( 96%)]  Loss: 3.55 (2.90)  Time: 0.677s,  189.05/s  (0.659s,  194.24/s)  LR: 6.674e-04  Data: 0.034 (0.040)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.229 (1.229)  Loss:   1.473 ( 1.473)  Acc@1:  63.281 ( 63.281)  Acc@5:  88.281 ( 88.281)
Test: [  39/39]  Time: 0.048 (0.311)  Loss:   0.947 ( 1.306)  Acc@1:  75.000 ( 71.560)  Acc@5:  87.500 ( 91.360)
Current checkpoints:
 ('./output/train/Upd_Exp2_Image100/checkpoint-42.pth.tar', 72.28)
 ('./output/train/Upd_Exp2_Image100/checkpoint-43.pth.tar', 72.22)
 ('./output/train/Upd_Exp2_Image100/checkpoint-45.pth.tar', 72.04)
 ('./output/train/Upd_Exp2_Image100/checkpoint-46.pth.tar', 71.56)
 ('./output/train/Upd_Exp2_Image100/checkpoint-41.pth.tar', 71.02)
 ('./output/train/Upd_Exp2_Image100/checkpoint-40.pth.tar', 70.84)
 ('./output/train/Upd_Exp2_Image100/checkpoint-44.pth.tar', 70.7)
 ('./output/train/Upd_Exp2_Image100/checkpoint-38.pth.tar', 70.64)
 ('./output/train/Upd_Exp2_Image100/checkpoint-39.pth.tar', 69.82)
 ('./output/train/Upd_Exp2_Image100/checkpoint-37.pth.tar', 69.74)

Train: 47 [   0/989 (  0%)]  Loss: 2.78 (2.78)  Time: 2.166s,   59.09/s  (2.166s,   59.09/s)  LR: 6.657e-04  Data: 1.115 (1.115)
Train: 47 [  50/989 (  5%)]  Loss: 2.91 (2.84)  Time: 0.653s,  196.14/s  (0.682s,  187.73/s)  LR: 6.657e-04  Data: 0.039 (0.057)
Train: 47 [ 100/989 ( 10%)]  Loss: 2.23 (2.64)  Time: 0.653s,  196.14/s  (0.665s,  192.39/s)  LR: 6.657e-04  Data: 0.040 (0.047)
Train: 47 [ 150/989 ( 15%)]  Loss: 3.38 (2.82)  Time: 0.648s,  197.41/s  (0.659s,  194.13/s)  LR: 6.657e-04  Data: 0.036 (0.044)
Train: 47 [ 200/989 ( 20%)]  Loss: 2.62 (2.78)  Time: 0.662s,  193.49/s  (0.657s,  194.96/s)  LR: 6.657e-04  Data: 0.016 (0.042)
Train: 47 [ 250/989 ( 25%)]  Loss: 3.16 (2.85)  Time: 0.674s,  190.02/s  (0.655s,  195.32/s)  LR: 6.657e-04  Data: 0.038 (0.041)
Train: 47 [ 300/989 ( 30%)]  Loss: 2.21 (2.75)  Time: 0.658s,  194.39/s  (0.655s,  195.56/s)  LR: 6.657e-04  Data: 0.052 (0.041)
Train: 47 [ 350/989 ( 35%)]  Loss: 2.62 (2.74)  Time: 0.654s,  195.80/s  (0.654s,  195.67/s)  LR: 6.657e-04  Data: 0.034 (0.040)
Train: 47 [ 400/989 ( 40%)]  Loss: 3.57 (2.83)  Time: 0.622s,  205.72/s  (0.654s,  195.79/s)  LR: 6.657e-04  Data: 0.015 (0.040)
Train: 47 [ 450/989 ( 46%)]  Loss: 2.90 (2.84)  Time: 0.607s,  210.83/s  (0.654s,  195.76/s)  LR: 6.657e-04  Data: 0.037 (0.040)
Train: 47 [ 500/989 ( 51%)]  Loss: 2.88 (2.84)  Time: 0.633s,  202.29/s  (0.654s,  195.81/s)  LR: 6.657e-04  Data: 0.016 (0.040)
Train: 47 [ 550/989 ( 56%)]  Loss: 3.01 (2.85)  Time: 0.541s,  236.53/s  (0.653s,  195.94/s)  LR: 6.657e-04  Data: 0.038 (0.039)
Train: 47 [ 600/989 ( 61%)]  Loss: 3.08 (2.87)  Time: 0.716s,  178.77/s  (0.654s,  195.72/s)  LR: 6.657e-04  Data: 0.057 (0.040)
Train: 47 [ 650/989 ( 66%)]  Loss: 2.86 (2.87)  Time: 0.657s,  194.78/s  (0.654s,  195.66/s)  LR: 6.657e-04  Data: 0.044 (0.040)
Train: 47 [ 700/989 ( 71%)]  Loss: 3.33 (2.90)  Time: 0.630s,  203.11/s  (0.654s,  195.58/s)  LR: 6.657e-04  Data: 0.019 (0.040)
Train: 47 [ 750/989 ( 76%)]  Loss: 3.37 (2.93)  Time: 0.572s,  223.87/s  (0.655s,  195.41/s)  LR: 6.657e-04  Data: 0.039 (0.040)
Train: 47 [ 800/989 ( 81%)]  Loss: 3.53 (2.97)  Time: 0.710s,  180.23/s  (0.656s,  195.12/s)  LR: 6.657e-04  Data: 0.041 (0.040)
Train: 47 [ 850/989 ( 86%)]  Loss: 2.42 (2.94)  Time: 0.713s,  179.50/s  (0.657s,  194.79/s)  LR: 6.657e-04  Data: 0.045 (0.040)
Train: 47 [ 900/989 ( 91%)]  Loss: 3.47 (2.96)  Time: 0.700s,  182.75/s  (0.658s,  194.43/s)  LR: 6.657e-04  Data: 0.048 (0.040)
Train: 47 [ 950/989 ( 96%)]  Loss: 3.36 (2.98)  Time: 0.664s,  192.91/s  (0.659s,  194.17/s)  LR: 6.657e-04  Data: 0.034 (0.040)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.212 (1.212)  Loss:   1.443 ( 1.443)  Acc@1:  70.312 ( 70.312)  Acc@5:  85.938 ( 85.938)
Test: [  39/39]  Time: 0.048 (0.311)  Loss:   0.859 ( 1.171)  Acc@1:  87.500 ( 72.720)  Acc@5: 100.000 ( 91.840)
Current checkpoints:
 ('./output/train/Upd_Exp2_Image100/checkpoint-47.pth.tar', 72.72)
 ('./output/train/Upd_Exp2_Image100/checkpoint-42.pth.tar', 72.28)
 ('./output/train/Upd_Exp2_Image100/checkpoint-43.pth.tar', 72.22)
 ('./output/train/Upd_Exp2_Image100/checkpoint-45.pth.tar', 72.04)
 ('./output/train/Upd_Exp2_Image100/checkpoint-46.pth.tar', 71.56)
 ('./output/train/Upd_Exp2_Image100/checkpoint-41.pth.tar', 71.02)
 ('./output/train/Upd_Exp2_Image100/checkpoint-40.pth.tar', 70.84)
 ('./output/train/Upd_Exp2_Image100/checkpoint-44.pth.tar', 70.7)
 ('./output/train/Upd_Exp2_Image100/checkpoint-38.pth.tar', 70.64)
 ('./output/train/Upd_Exp2_Image100/checkpoint-39.pth.tar', 69.82)

Train: 48 [   0/989 (  0%)]  Loss: 3.06 (3.06)  Time: 1.983s,   64.56/s  (1.983s,   64.56/s)  LR: 6.640e-04  Data: 1.201 (1.201)
Train: 48 [  50/989 (  5%)]  Loss: 3.23 (3.14)  Time: 0.670s,  190.93/s  (0.682s,  187.67/s)  LR: 6.640e-04  Data: 0.043 (0.061)
Train: 48 [ 100/989 ( 10%)]  Loss: 3.17 (3.15)  Time: 0.651s,  196.51/s  (0.667s,  191.98/s)  LR: 6.640e-04  Data: 0.039 (0.050)
Train: 48 [ 150/989 ( 15%)]  Loss: 3.15 (3.15)  Time: 0.615s,  208.04/s  (0.662s,  193.47/s)  LR: 6.640e-04  Data: 0.038 (0.046)
Train: 48 [ 200/989 ( 20%)]  Loss: 3.60 (3.24)  Time: 0.651s,  196.55/s  (0.660s,  193.99/s)  LR: 6.640e-04  Data: 0.046 (0.043)
Train: 48 [ 250/989 ( 25%)]  Loss: 2.42 (3.10)  Time: 0.642s,  199.41/s  (0.658s,  194.60/s)  LR: 6.640e-04  Data: 0.034 (0.042)
Train: 48 [ 300/989 ( 30%)]  Loss: 3.22 (3.12)  Time: 0.612s,  209.24/s  (0.656s,  195.02/s)  LR: 6.640e-04  Data: 0.041 (0.041)
Train: 48 [ 350/989 ( 35%)]  Loss: 3.63 (3.19)  Time: 0.642s,  199.41/s  (0.655s,  195.37/s)  LR: 6.640e-04  Data: 0.037 (0.040)
Train: 48 [ 400/989 ( 40%)]  Loss: 2.95 (3.16)  Time: 0.633s,  202.14/s  (0.655s,  195.52/s)  LR: 6.640e-04  Data: 0.045 (0.040)
Train: 48 [ 450/989 ( 46%)]  Loss: 3.15 (3.16)  Time: 0.650s,  196.86/s  (0.654s,  195.58/s)  LR: 6.640e-04  Data: 0.041 (0.040)
Train: 48 [ 500/989 ( 51%)]  Loss: 3.50 (3.19)  Time: 0.604s,  211.93/s  (0.655s,  195.32/s)  LR: 6.640e-04  Data: 0.041 (0.040)
Train: 48 [ 550/989 ( 56%)]  Loss: 2.82 (3.16)  Time: 0.654s,  195.79/s  (0.655s,  195.42/s)  LR: 6.640e-04  Data: 0.030 (0.040)
Train: 48 [ 600/989 ( 61%)]  Loss: 2.64 (3.12)  Time: 0.614s,  208.57/s  (0.655s,  195.41/s)  LR: 6.640e-04  Data: 0.031 (0.040)
Train: 48 [ 650/989 ( 66%)]  Loss: 3.21 (3.12)  Time: 0.696s,  183.82/s  (0.655s,  195.28/s)  LR: 6.640e-04  Data: 0.050 (0.040)
Train: 48 [ 700/989 ( 71%)]  Loss: 3.22 (3.13)  Time: 0.666s,  192.14/s  (0.656s,  195.12/s)  LR: 6.640e-04  Data: 0.045 (0.040)
Train: 48 [ 750/989 ( 76%)]  Loss: 2.89 (3.12)  Time: 0.661s,  193.59/s  (0.656s,  194.98/s)  LR: 6.640e-04  Data: 0.041 (0.040)
Train: 48 [ 800/989 ( 81%)]  Loss: 3.77 (3.16)  Time: 0.655s,  195.38/s  (0.657s,  194.83/s)  LR: 6.640e-04  Data: 0.029 (0.040)
Train: 48 [ 850/989 ( 86%)]  Loss: 3.54 (3.18)  Time: 0.668s,  191.59/s  (0.658s,  194.57/s)  LR: 6.640e-04  Data: 0.027 (0.040)
Train: 48 [ 900/989 ( 91%)]  Loss: 2.97 (3.17)  Time: 0.648s,  197.63/s  (0.659s,  194.24/s)  LR: 6.640e-04  Data: 0.028 (0.040)
Train: 48 [ 950/989 ( 96%)]  Loss: 3.14 (3.16)  Time: 0.665s,  192.43/s  (0.660s,  193.95/s)  LR: 6.640e-04  Data: 0.037 (0.040)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.210 (1.210)  Loss:   1.564 ( 1.564)  Acc@1:  60.156 ( 60.156)  Acc@5:  88.281 ( 88.281)
Test: [  39/39]  Time: 0.049 (0.311)  Loss:   0.578 ( 1.111)  Acc@1:  87.500 ( 73.080)  Acc@5: 100.000 ( 92.360)
Current checkpoints:
 ('./output/train/Upd_Exp2_Image100/checkpoint-48.pth.tar', 73.08)
 ('./output/train/Upd_Exp2_Image100/checkpoint-47.pth.tar', 72.72)
 ('./output/train/Upd_Exp2_Image100/checkpoint-42.pth.tar', 72.28)
 ('./output/train/Upd_Exp2_Image100/checkpoint-43.pth.tar', 72.22)
 ('./output/train/Upd_Exp2_Image100/checkpoint-45.pth.tar', 72.04)
 ('./output/train/Upd_Exp2_Image100/checkpoint-46.pth.tar', 71.56)
 ('./output/train/Upd_Exp2_Image100/checkpoint-41.pth.tar', 71.02)
 ('./output/train/Upd_Exp2_Image100/checkpoint-40.pth.tar', 70.84)
 ('./output/train/Upd_Exp2_Image100/checkpoint-44.pth.tar', 70.7)
 ('./output/train/Upd_Exp2_Image100/checkpoint-38.pth.tar', 70.64)

Train: 49 [   0/989 (  0%)]  Loss: 3.02 (3.02)  Time: 1.936s,   66.12/s  (1.936s,   66.12/s)  LR: 6.622e-04  Data: 1.197 (1.197)
Train: 49 [  50/989 (  5%)]  Loss: 2.75 (2.88)  Time: 0.641s,  199.66/s  (0.676s,  189.24/s)  LR: 6.622e-04  Data: 0.022 (0.062)
Train: 49 [ 100/989 ( 10%)]  Loss: 2.76 (2.84)  Time: 0.638s,  200.50/s  (0.662s,  193.44/s)  LR: 6.622e-04  Data: 0.043 (0.050)
Train: 49 [ 150/989 ( 15%)]  Loss: 2.59 (2.78)  Time: 0.626s,  204.41/s  (0.658s,  194.60/s)  LR: 6.622e-04  Data: 0.014 (0.045)
Train: 49 [ 200/989 ( 20%)]  Loss: 2.85 (2.79)  Time: 0.608s,  210.67/s  (0.655s,  195.31/s)  LR: 6.622e-04  Data: 0.039 (0.044)
Train: 49 [ 250/989 ( 25%)]  Loss: 3.36 (2.89)  Time: 0.648s,  197.63/s  (0.654s,  195.74/s)  LR: 6.622e-04  Data: 0.050 (0.043)
Train: 49 [ 300/989 ( 30%)]  Loss: 2.94 (2.90)  Time: 0.645s,  198.34/s  (0.653s,  196.03/s)  LR: 6.622e-04  Data: 0.048 (0.042)
Train: 49 [ 350/989 ( 35%)]  Loss: 2.90 (2.90)  Time: 0.668s,  191.60/s  (0.652s,  196.18/s)  LR: 6.622e-04  Data: 0.034 (0.041)
Train: 49 [ 400/989 ( 40%)]  Loss: 3.02 (2.91)  Time: 0.576s,  222.25/s  (0.652s,  196.37/s)  LR: 6.622e-04  Data: 0.031 (0.041)
Train: 49 [ 450/989 ( 46%)]  Loss: 2.81 (2.90)  Time: 0.665s,  192.42/s  (0.652s,  196.37/s)  LR: 6.622e-04  Data: 0.035 (0.041)
Train: 49 [ 500/989 ( 51%)]  Loss: 2.58 (2.87)  Time: 0.646s,  198.17/s  (0.652s,  196.40/s)  LR: 6.622e-04  Data: 0.033 (0.040)
Train: 49 [ 550/989 ( 56%)]  Loss: 2.98 (2.88)  Time: 0.667s,  191.97/s  (0.652s,  196.28/s)  LR: 6.622e-04  Data: 0.051 (0.040)
Train: 49 [ 600/989 ( 61%)]  Loss: 3.39 (2.92)  Time: 0.630s,  203.31/s  (0.652s,  196.31/s)  LR: 6.622e-04  Data: 0.041 (0.040)
Train: 49 [ 650/989 ( 66%)]  Loss: 3.11 (2.93)  Time: 0.662s,  193.32/s  (0.652s,  196.19/s)  LR: 6.622e-04  Data: 0.051 (0.040)
Train: 49 [ 700/989 ( 71%)]  Loss: 3.17 (2.95)  Time: 0.657s,  194.73/s  (0.653s,  195.99/s)  LR: 6.622e-04  Data: 0.044 (0.040)
Train: 49 [ 750/989 ( 76%)]  Loss: 3.40 (2.98)  Time: 0.641s,  199.69/s  (0.654s,  195.84/s)  LR: 6.622e-04  Data: 0.028 (0.040)
Train: 49 [ 800/989 ( 81%)]  Loss: 3.56 (3.01)  Time: 0.680s,  188.13/s  (0.654s,  195.61/s)  LR: 6.622e-04  Data: 0.036 (0.040)
Train: 49 [ 850/989 ( 86%)]  Loss: 3.02 (3.01)  Time: 0.732s,  174.88/s  (0.655s,  195.31/s)  LR: 6.622e-04  Data: 0.045 (0.040)
Train: 49 [ 900/989 ( 91%)]  Loss: 3.41 (3.03)  Time: 0.677s,  189.04/s  (0.657s,  194.93/s)  LR: 6.622e-04  Data: 0.030 (0.040)
Train: 49 [ 950/989 ( 96%)]  Loss: 2.93 (3.03)  Time: 0.698s,  183.25/s  (0.658s,  194.62/s)  LR: 6.622e-04  Data: 0.036 (0.040)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.206 (1.206)  Loss:   1.123 ( 1.123)  Acc@1:  75.781 ( 75.781)  Acc@5:  91.406 ( 91.406)
Test: [  39/39]  Time: 0.048 (0.316)  Loss:   0.383 ( 1.132)  Acc@1:  87.500 ( 73.280)  Acc@5: 100.000 ( 91.820)
Current checkpoints:
 ('./output/train/Upd_Exp2_Image100/checkpoint-49.pth.tar', 73.28)
 ('./output/train/Upd_Exp2_Image100/checkpoint-48.pth.tar', 73.08)
 ('./output/train/Upd_Exp2_Image100/checkpoint-47.pth.tar', 72.72)
 ('./output/train/Upd_Exp2_Image100/checkpoint-42.pth.tar', 72.28)
 ('./output/train/Upd_Exp2_Image100/checkpoint-43.pth.tar', 72.22)
 ('./output/train/Upd_Exp2_Image100/checkpoint-45.pth.tar', 72.04)
 ('./output/train/Upd_Exp2_Image100/checkpoint-46.pth.tar', 71.56)
 ('./output/train/Upd_Exp2_Image100/checkpoint-41.pth.tar', 71.02)
 ('./output/train/Upd_Exp2_Image100/checkpoint-40.pth.tar', 70.84)
 ('./output/train/Upd_Exp2_Image100/checkpoint-44.pth.tar', 70.7)

Train: 50 [   0/989 (  0%)]  Loss: 2.05 (2.05)  Time: 1.786s,   71.67/s  (1.786s,   71.67/s)  LR: 6.604e-04  Data: 1.058 (1.058)
Train: 50 [  50/989 (  5%)]  Loss: 3.03 (2.54)  Time: 0.646s,  198.17/s  (0.674s,  189.90/s)  LR: 6.604e-04  Data: 0.047 (0.057)
Train: 50 [ 100/989 ( 10%)]  Loss: 3.47 (2.85)  Time: 0.653s,  196.03/s  (0.659s,  194.22/s)  LR: 6.604e-04  Data: 0.039 (0.048)
Train: 50 [ 150/989 ( 15%)]  Loss: 2.34 (2.72)  Time: 0.656s,  195.24/s  (0.655s,  195.45/s)  LR: 6.604e-04  Data: 0.024 (0.045)
Train: 50 [ 200/989 ( 20%)]  Loss: 3.17 (2.81)  Time: 0.659s,  194.14/s  (0.654s,  195.85/s)  LR: 6.604e-04  Data: 0.024 (0.044)
Train: 50 [ 250/989 ( 25%)]  Loss: 3.05 (2.85)  Time: 0.650s,  196.96/s  (0.652s,  196.17/s)  LR: 6.604e-04  Data: 0.030 (0.042)
Train: 50 [ 300/989 ( 30%)]  Loss: 2.71 (2.83)  Time: 0.667s,  191.88/s  (0.652s,  196.39/s)  LR: 6.604e-04  Data: 0.060 (0.042)
Train: 50 [ 350/989 ( 35%)]  Loss: 2.86 (2.84)  Time: 0.649s,  197.26/s  (0.652s,  196.43/s)  LR: 6.604e-04  Data: 0.043 (0.041)
Train: 50 [ 400/989 ( 40%)]  Loss: 3.00 (2.85)  Time: 0.602s,  212.69/s  (0.652s,  196.39/s)  LR: 6.604e-04  Data: 0.042 (0.041)
Train: 50 [ 450/989 ( 46%)]  Loss: 3.54 (2.92)  Time: 0.749s,  170.89/s  (0.652s,  196.25/s)  LR: 6.604e-04  Data: 0.046 (0.041)
Train: 50 [ 500/989 ( 51%)]  Loss: 2.97 (2.93)  Time: 0.654s,  195.69/s  (0.652s,  196.17/s)  LR: 6.604e-04  Data: 0.040 (0.041)
Train: 50 [ 550/989 ( 56%)]  Loss: 2.53 (2.89)  Time: 0.646s,  198.22/s  (0.653s,  195.99/s)  LR: 6.604e-04  Data: 0.043 (0.040)
Train: 50 [ 600/989 ( 61%)]  Loss: 2.76 (2.88)  Time: 0.698s,  183.47/s  (0.653s,  195.98/s)  LR: 6.604e-04  Data: 0.024 (0.040)
Train: 50 [ 650/989 ( 66%)]  Loss: 3.38 (2.92)  Time: 0.643s,  199.02/s  (0.654s,  195.84/s)  LR: 6.604e-04  Data: 0.027 (0.040)
Train: 50 [ 700/989 ( 71%)]  Loss: 2.99 (2.92)  Time: 0.657s,  194.78/s  (0.654s,  195.71/s)  LR: 6.604e-04  Data: 0.041 (0.040)
Train: 50 [ 750/989 ( 76%)]  Loss: 3.38 (2.95)  Time: 0.671s,  190.73/s  (0.654s,  195.57/s)  LR: 6.604e-04  Data: 0.048 (0.040)
Train: 50 [ 800/989 ( 81%)]  Loss: 2.47 (2.92)  Time: 0.661s,  193.56/s  (0.656s,  195.26/s)  LR: 6.604e-04  Data: 0.046 (0.040)
Train: 50 [ 850/989 ( 86%)]  Loss: 2.27 (2.89)  Time: 0.639s,  200.30/s  (0.656s,  195.01/s)  LR: 6.604e-04  Data: 0.033 (0.040)
Train: 50 [ 900/989 ( 91%)]  Loss: 3.20 (2.90)  Time: 0.646s,  198.03/s  (0.658s,  194.55/s)  LR: 6.604e-04  Data: 0.040 (0.040)
Train: 50 [ 950/989 ( 96%)]  Loss: 3.37 (2.93)  Time: 0.660s,  193.95/s  (0.660s,  194.08/s)  LR: 6.604e-04  Data: 0.038 (0.040)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.214 (1.214)  Loss:   1.259 ( 1.259)  Acc@1:  70.312 ( 70.312)  Acc@5:  90.625 ( 90.625)
Test: [  39/39]  Time: 0.044 (0.312)  Loss:   0.265 ( 1.099)  Acc@1: 100.000 ( 73.720)  Acc@5: 100.000 ( 92.360)
Current checkpoints:
 ('./output/train/Upd_Exp2_Image100/checkpoint-50.pth.tar', 73.72)
 ('./output/train/Upd_Exp2_Image100/checkpoint-49.pth.tar', 73.28)
 ('./output/train/Upd_Exp2_Image100/checkpoint-48.pth.tar', 73.08)
 ('./output/train/Upd_Exp2_Image100/checkpoint-47.pth.tar', 72.72)
 ('./output/train/Upd_Exp2_Image100/checkpoint-42.pth.tar', 72.28)
 ('./output/train/Upd_Exp2_Image100/checkpoint-43.pth.tar', 72.22)
 ('./output/train/Upd_Exp2_Image100/checkpoint-45.pth.tar', 72.04)
 ('./output/train/Upd_Exp2_Image100/checkpoint-46.pth.tar', 71.56)
 ('./output/train/Upd_Exp2_Image100/checkpoint-41.pth.tar', 71.02)
 ('./output/train/Upd_Exp2_Image100/checkpoint-40.pth.tar', 70.84)

Train: 51 [   0/989 (  0%)]  Loss: 2.26 (2.26)  Time: 1.772s,   72.25/s  (1.772s,   72.25/s)  LR: 6.586e-04  Data: 1.022 (1.022)
Train: 51 [  50/989 (  5%)]  Loss: 3.58 (2.92)  Time: 0.686s,  186.65/s  (0.674s,  190.04/s)  LR: 6.586e-04  Data: 0.037 (0.059)
Train: 51 [ 100/989 ( 10%)]  Loss: 3.24 (3.03)  Time: 0.602s,  212.65/s  (0.659s,  194.15/s)  LR: 6.586e-04  Data: 0.023 (0.048)
Train: 51 [ 150/989 ( 15%)]  Loss: 3.02 (3.03)  Time: 0.677s,  189.04/s  (0.655s,  195.27/s)  LR: 6.586e-04  Data: 0.039 (0.045)
Train: 51 [ 200/989 ( 20%)]  Loss: 2.99 (3.02)  Time: 0.643s,  199.12/s  (0.653s,  195.99/s)  LR: 6.586e-04  Data: 0.036 (0.043)
Train: 51 [ 250/989 ( 25%)]  Loss: 2.39 (2.91)  Time: 0.644s,  198.65/s  (0.652s,  196.23/s)  LR: 6.586e-04  Data: 0.044 (0.043)
Train: 51 [ 300/989 ( 30%)]  Loss: 3.38 (2.98)  Time: 0.633s,  202.14/s  (0.651s,  196.51/s)  LR: 6.586e-04  Data: 0.046 (0.042)
Train: 51 [ 350/989 ( 35%)]  Loss: 3.44 (3.04)  Time: 0.605s,  211.67/s  (0.652s,  196.21/s)  LR: 6.586e-04  Data: 0.034 (0.042)
Train: 51 [ 400/989 ( 40%)]  Loss: 2.98 (3.03)  Time: 0.632s,  202.41/s  (0.652s,  196.25/s)  LR: 6.586e-04  Data: 0.034 (0.041)
Train: 51 [ 450/989 ( 46%)]  Loss: 2.99 (3.03)  Time: 0.670s,  191.18/s  (0.653s,  196.14/s)  LR: 6.586e-04  Data: 0.029 (0.041)
Train: 51 [ 500/989 ( 51%)]  Loss: 2.45 (2.97)  Time: 0.671s,  190.68/s  (0.653s,  196.17/s)  LR: 6.586e-04  Data: 0.057 (0.041)
Train: 51 [ 550/989 ( 56%)]  Loss: 2.76 (2.96)  Time: 0.642s,  199.44/s  (0.653s,  196.10/s)  LR: 6.586e-04  Data: 0.043 (0.041)
Train: 51 [ 600/989 ( 61%)]  Loss: 2.67 (2.94)  Time: 0.650s,  196.89/s  (0.653s,  196.07/s)  LR: 6.586e-04  Data: 0.039 (0.040)
Train: 51 [ 650/989 ( 66%)]  Loss: 3.06 (2.94)  Time: 0.666s,  192.19/s  (0.654s,  195.85/s)  LR: 6.586e-04  Data: 0.043 (0.040)
Train: 51 [ 700/989 ( 71%)]  Loss: 2.67 (2.93)  Time: 0.660s,  193.81/s  (0.654s,  195.73/s)  LR: 6.586e-04  Data: 0.042 (0.040)
Train: 51 [ 750/989 ( 76%)]  Loss: 2.97 (2.93)  Time: 0.654s,  195.66/s  (0.655s,  195.47/s)  LR: 6.586e-04  Data: 0.047 (0.040)
Train: 51 [ 800/989 ( 81%)]  Loss: 2.55 (2.91)  Time: 0.667s,  191.77/s  (0.656s,  195.24/s)  LR: 6.586e-04  Data: 0.032 (0.040)
Train: 51 [ 850/989 ( 86%)]  Loss: 3.18 (2.92)  Time: 0.660s,  193.97/s  (0.657s,  194.89/s)  LR: 6.586e-04  Data: 0.033 (0.040)
Train: 51 [ 900/989 ( 91%)]  Loss: 3.43 (2.95)  Time: 0.773s,  165.67/s  (0.658s,  194.49/s)  LR: 6.586e-04  Data: 0.054 (0.040)
Train: 51 [ 950/989 ( 96%)]  Loss: 2.66 (2.93)  Time: 0.673s,  190.29/s  (0.660s,  194.07/s)  LR: 6.586e-04  Data: 0.039 (0.040)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.223 (1.223)  Loss:   1.256 ( 1.256)  Acc@1:  71.875 ( 71.875)  Acc@5:  89.062 ( 89.062)
Test: [  39/39]  Time: 0.045 (0.312)  Loss:   0.324 ( 1.108)  Acc@1:  87.500 ( 73.720)  Acc@5: 100.000 ( 92.520)
Current checkpoints:
 ('./output/train/Upd_Exp2_Image100/checkpoint-50.pth.tar', 73.72)
 ('./output/train/Upd_Exp2_Image100/checkpoint-51.pth.tar', 73.72)
 ('./output/train/Upd_Exp2_Image100/checkpoint-49.pth.tar', 73.28)
 ('./output/train/Upd_Exp2_Image100/checkpoint-48.pth.tar', 73.08)
 ('./output/train/Upd_Exp2_Image100/checkpoint-47.pth.tar', 72.72)
 ('./output/train/Upd_Exp2_Image100/checkpoint-42.pth.tar', 72.28)
 ('./output/train/Upd_Exp2_Image100/checkpoint-43.pth.tar', 72.22)
 ('./output/train/Upd_Exp2_Image100/checkpoint-45.pth.tar', 72.04)
 ('./output/train/Upd_Exp2_Image100/checkpoint-46.pth.tar', 71.56)
 ('./output/train/Upd_Exp2_Image100/checkpoint-41.pth.tar', 71.02)

Train: 52 [   0/989 (  0%)]  Loss: 3.70 (3.70)  Time: 2.209s,   57.95/s  (2.209s,   57.95/s)  LR: 6.567e-04  Data: 1.052 (1.052)
Train: 52 [  50/989 (  5%)]  Loss: 2.90 (3.30)  Time: 0.679s,  188.49/s  (0.683s,  187.50/s)  LR: 6.567e-04  Data: 0.038 (0.057)
Train: 52 [ 100/989 ( 10%)]  Loss: 2.87 (3.16)  Time: 0.650s,  196.89/s  (0.667s,  191.96/s)  LR: 6.567e-04  Data: 0.028 (0.047)
Train: 52 [ 150/989 ( 15%)]  Loss: 2.42 (2.97)  Time: 0.644s,  198.79/s  (0.662s,  193.38/s)  LR: 6.567e-04  Data: 0.036 (0.043)
Train: 52 [ 200/989 ( 20%)]  Loss: 2.94 (2.97)  Time: 0.663s,  192.94/s  (0.660s,  193.93/s)  LR: 6.567e-04  Data: 0.040 (0.042)
Train: 52 [ 250/989 ( 25%)]  Loss: 2.78 (2.94)  Time: 0.636s,  201.41/s  (0.659s,  194.21/s)  LR: 6.567e-04  Data: 0.037 (0.041)
Train: 52 [ 300/989 ( 30%)]  Loss: 3.24 (2.98)  Time: 0.641s,  199.77/s  (0.657s,  194.73/s)  LR: 6.567e-04  Data: 0.016 (0.040)
Train: 52 [ 350/989 ( 35%)]  Loss: 2.76 (2.95)  Time: 0.653s,  196.04/s  (0.658s,  194.45/s)  LR: 6.567e-04  Data: 0.043 (0.041)
Train: 52 [ 400/989 ( 40%)]  Loss: 2.68 (2.92)  Time: 0.645s,  198.36/s  (0.658s,  194.53/s)  LR: 6.567e-04  Data: 0.047 (0.040)
Train: 52 [ 450/989 ( 46%)]  Loss: 2.55 (2.88)  Time: 0.631s,  202.94/s  (0.658s,  194.46/s)  LR: 6.567e-04  Data: 0.035 (0.040)
Train: 52 [ 500/989 ( 51%)]  Loss: 3.38 (2.93)  Time: 0.648s,  197.64/s  (0.658s,  194.43/s)  LR: 6.567e-04  Data: 0.045 (0.040)
Train: 52 [ 550/989 ( 56%)]  Loss: 3.20 (2.95)  Time: 0.673s,  190.16/s  (0.658s,  194.52/s)  LR: 6.567e-04  Data: 0.036 (0.040)
Train: 52 [ 600/989 ( 61%)]  Loss: 2.84 (2.94)  Time: 0.662s,  193.32/s  (0.658s,  194.53/s)  LR: 6.567e-04  Data: 0.042 (0.040)
Train: 52 [ 650/989 ( 66%)]  Loss: 2.91 (2.94)  Time: 0.660s,  193.82/s  (0.658s,  194.47/s)  LR: 6.567e-04  Data: 0.043 (0.040)
Train: 52 [ 700/989 ( 71%)]  Loss: 2.85 (2.93)  Time: 0.549s,  233.16/s  (0.659s,  194.24/s)  LR: 6.567e-04  Data: 0.040 (0.040)
Train: 52 [ 750/989 ( 76%)]  Loss: 3.36 (2.96)  Time: 0.698s,  183.30/s  (0.659s,  194.11/s)  LR: 6.567e-04  Data: 0.061 (0.040)
Train: 52 [ 800/989 ( 81%)]  Loss: 2.47 (2.93)  Time: 0.666s,  192.22/s  (0.660s,  193.92/s)  LR: 6.567e-04  Data: 0.043 (0.040)
Train: 52 [ 850/989 ( 86%)]  Loss: 2.62 (2.92)  Time: 0.707s,  181.16/s  (0.661s,  193.66/s)  LR: 6.567e-04  Data: 0.027 (0.040)
Train: 52 [ 900/989 ( 91%)]  Loss: 2.74 (2.91)  Time: 0.663s,  192.93/s  (0.662s,  193.26/s)  LR: 6.567e-04  Data: 0.058 (0.040)
Train: 52 [ 950/989 ( 96%)]  Loss: 2.28 (2.87)  Time: 0.673s,  190.17/s  (0.664s,  192.92/s)  LR: 6.567e-04  Data: 0.048 (0.040)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.215 (1.215)  Loss:   1.336 ( 1.336)  Acc@1:  69.531 ( 69.531)  Acc@5:  89.844 ( 89.844)
Test: [  39/39]  Time: 0.035 (0.312)  Loss:   0.476 ( 1.137)  Acc@1:  87.500 ( 73.940)  Acc@5: 100.000 ( 92.280)
Current checkpoints:
 ('./output/train/Upd_Exp2_Image100/checkpoint-52.pth.tar', 73.94)
 ('./output/train/Upd_Exp2_Image100/checkpoint-50.pth.tar', 73.72)
 ('./output/train/Upd_Exp2_Image100/checkpoint-51.pth.tar', 73.72)
 ('./output/train/Upd_Exp2_Image100/checkpoint-49.pth.tar', 73.28)
 ('./output/train/Upd_Exp2_Image100/checkpoint-48.pth.tar', 73.08)
 ('./output/train/Upd_Exp2_Image100/checkpoint-47.pth.tar', 72.72)
 ('./output/train/Upd_Exp2_Image100/checkpoint-42.pth.tar', 72.28)
 ('./output/train/Upd_Exp2_Image100/checkpoint-43.pth.tar', 72.22)
 ('./output/train/Upd_Exp2_Image100/checkpoint-45.pth.tar', 72.04)
 ('./output/train/Upd_Exp2_Image100/checkpoint-46.pth.tar', 71.56)

Train: 53 [   0/989 (  0%)]  Loss: 3.63 (3.63)  Time: 1.887s,   67.85/s  (1.887s,   67.85/s)  LR: 6.548e-04  Data: 1.093 (1.093)
Train: 53 [  50/989 (  5%)]  Loss: 2.58 (3.11)  Time: 0.639s,  200.35/s  (0.683s,  187.47/s)  LR: 6.548e-04  Data: 0.034 (0.056)
Train: 53 [ 100/989 ( 10%)]  Loss: 2.82 (3.01)  Time: 0.678s,  188.83/s  (0.665s,  192.50/s)  LR: 6.548e-04  Data: 0.062 (0.048)
Train: 53 [ 150/989 ( 15%)]  Loss: 3.01 (3.01)  Time: 0.646s,  198.08/s  (0.659s,  194.25/s)  LR: 6.548e-04  Data: 0.044 (0.044)
Train: 53 [ 200/989 ( 20%)]  Loss: 2.48 (2.91)  Time: 0.637s,  201.05/s  (0.657s,  194.81/s)  LR: 6.548e-04  Data: 0.043 (0.042)
Train: 53 [ 250/989 ( 25%)]  Loss: 3.37 (2.98)  Time: 0.642s,  199.30/s  (0.655s,  195.31/s)  LR: 6.548e-04  Data: 0.037 (0.041)
Train: 53 [ 300/989 ( 30%)]  Loss: 3.22 (3.02)  Time: 0.638s,  200.58/s  (0.655s,  195.54/s)  LR: 6.548e-04  Data: 0.044 (0.041)
Train: 53 [ 350/989 ( 35%)]  Loss: 2.62 (2.97)  Time: 0.662s,  193.49/s  (0.654s,  195.66/s)  LR: 6.548e-04  Data: 0.043 (0.041)
Train: 53 [ 400/989 ( 40%)]  Loss: 3.39 (3.02)  Time: 0.660s,  193.83/s  (0.654s,  195.74/s)  LR: 6.548e-04  Data: 0.034 (0.040)
Train: 53 [ 450/989 ( 46%)]  Loss: 3.18 (3.03)  Time: 0.657s,  194.92/s  (0.654s,  195.77/s)  LR: 6.548e-04  Data: 0.032 (0.040)
Train: 53 [ 500/989 ( 51%)]  Loss: 2.87 (3.02)  Time: 0.602s,  212.59/s  (0.654s,  195.73/s)  LR: 6.548e-04  Data: 0.028 (0.040)
Train: 53 [ 550/989 ( 56%)]  Loss: 2.68 (2.99)  Time: 0.649s,  197.16/s  (0.654s,  195.71/s)  LR: 6.548e-04  Data: 0.026 (0.040)
Train: 53 [ 600/989 ( 61%)]  Loss: 2.26 (2.93)  Time: 0.645s,  198.49/s  (0.654s,  195.62/s)  LR: 6.548e-04  Data: 0.045 (0.040)
Train: 53 [ 650/989 ( 66%)]  Loss: 3.31 (2.96)  Time: 0.729s,  175.66/s  (0.655s,  195.54/s)  LR: 6.548e-04  Data: 0.029 (0.040)
Train: 53 [ 700/989 ( 71%)]  Loss: 2.45 (2.93)  Time: 0.641s,  199.68/s  (0.655s,  195.53/s)  LR: 6.548e-04  Data: 0.025 (0.039)
Train: 53 [ 750/989 ( 76%)]  Loss: 3.10 (2.94)  Time: 0.677s,  189.18/s  (0.655s,  195.44/s)  LR: 6.548e-04  Data: 0.044 (0.039)
Train: 53 [ 800/989 ( 81%)]  Loss: 2.53 (2.91)  Time: 0.660s,  193.99/s  (0.656s,  195.26/s)  LR: 6.548e-04  Data: 0.049 (0.039)
Train: 53 [ 850/989 ( 86%)]  Loss: 2.38 (2.88)  Time: 0.712s,  179.87/s  (0.656s,  195.02/s)  LR: 6.548e-04  Data: 0.069 (0.040)
Train: 53 [ 900/989 ( 91%)]  Loss: 2.54 (2.87)  Time: 0.651s,  196.53/s  (0.657s,  194.72/s)  LR: 6.548e-04  Data: 0.032 (0.040)
Train: 53 [ 950/989 ( 96%)]  Loss: 2.91 (2.87)  Time: 0.640s,  200.10/s  (0.658s,  194.42/s)  LR: 6.548e-04  Data: 0.036 (0.040)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.194 (1.194)  Loss:   1.396 ( 1.396)  Acc@1:  69.531 ( 69.531)  Acc@5:  89.844 ( 89.844)
Test: [  39/39]  Time: 0.055 (0.313)  Loss:   0.664 ( 1.089)  Acc@1:  75.000 ( 75.400)  Acc@5: 100.000 ( 92.960)
Current checkpoints:
 ('./output/train/Upd_Exp2_Image100/checkpoint-53.pth.tar', 75.4)
 ('./output/train/Upd_Exp2_Image100/checkpoint-52.pth.tar', 73.94)
 ('./output/train/Upd_Exp2_Image100/checkpoint-50.pth.tar', 73.72)
 ('./output/train/Upd_Exp2_Image100/checkpoint-51.pth.tar', 73.72)
 ('./output/train/Upd_Exp2_Image100/checkpoint-49.pth.tar', 73.28)
 ('./output/train/Upd_Exp2_Image100/checkpoint-48.pth.tar', 73.08)
 ('./output/train/Upd_Exp2_Image100/checkpoint-47.pth.tar', 72.72)
 ('./output/train/Upd_Exp2_Image100/checkpoint-42.pth.tar', 72.28)
 ('./output/train/Upd_Exp2_Image100/checkpoint-43.pth.tar', 72.22)
 ('./output/train/Upd_Exp2_Image100/checkpoint-45.pth.tar', 72.04)

Train: 54 [   0/989 (  0%)]  Loss: 3.25 (3.25)  Time: 1.974s,   64.84/s  (1.974s,   64.84/s)  LR: 6.528e-04  Data: 1.260 (1.260)
Train: 54 [  50/989 (  5%)]  Loss: 2.47 (2.86)  Time: 0.640s,  200.00/s  (0.678s,  188.67/s)  LR: 6.528e-04  Data: 0.041 (0.061)
Train: 54 [ 100/989 ( 10%)]  Loss: 3.47 (3.07)  Time: 0.664s,  192.72/s  (0.664s,  192.67/s)  LR: 6.528e-04  Data: 0.023 (0.050)
Train: 54 [ 150/989 ( 15%)]  Loss: 2.77 (2.99)  Time: 0.643s,  199.09/s  (0.661s,  193.79/s)  LR: 6.528e-04  Data: 0.040 (0.046)
Train: 54 [ 200/989 ( 20%)]  Loss: 3.27 (3.05)  Time: 0.657s,  194.93/s  (0.658s,  194.63/s)  LR: 6.528e-04  Data: 0.047 (0.044)
Train: 54 [ 250/989 ( 25%)]  Loss: 3.24 (3.08)  Time: 0.655s,  195.40/s  (0.656s,  195.18/s)  LR: 6.528e-04  Data: 0.057 (0.043)
Train: 54 [ 300/989 ( 30%)]  Loss: 2.96 (3.06)  Time: 0.633s,  202.07/s  (0.655s,  195.44/s)  LR: 6.528e-04  Data: 0.045 (0.042)
Train: 54 [ 350/989 ( 35%)]  Loss: 3.19 (3.08)  Time: 0.635s,  201.51/s  (0.654s,  195.63/s)  LR: 6.528e-04  Data: 0.045 (0.042)
Train: 54 [ 400/989 ( 40%)]  Loss: 2.66 (3.03)  Time: 0.638s,  200.76/s  (0.653s,  195.88/s)  LR: 6.528e-04  Data: 0.031 (0.042)
Train: 54 [ 450/989 ( 46%)]  Loss: 2.66 (3.00)  Time: 0.687s,  186.22/s  (0.653s,  196.09/s)  LR: 6.528e-04  Data: 0.030 (0.041)
Train: 54 [ 500/989 ( 51%)]  Loss: 3.01 (3.00)  Time: 0.656s,  195.07/s  (0.653s,  196.16/s)  LR: 6.528e-04  Data: 0.031 (0.041)
Train: 54 [ 550/989 ( 56%)]  Loss: 3.50 (3.04)  Time: 0.652s,  196.42/s  (0.653s,  196.12/s)  LR: 6.528e-04  Data: 0.038 (0.040)
Train: 54 [ 600/989 ( 61%)]  Loss: 3.62 (3.08)  Time: 0.641s,  199.53/s  (0.653s,  196.05/s)  LR: 6.528e-04  Data: 0.033 (0.040)
Train: 54 [ 650/989 ( 66%)]  Loss: 3.05 (3.08)  Time: 0.672s,  190.48/s  (0.653s,  196.04/s)  LR: 6.528e-04  Data: 0.060 (0.040)
Train: 54 [ 700/989 ( 71%)]  Loss: 3.30 (3.10)  Time: 0.591s,  216.71/s  (0.653s,  195.90/s)  LR: 6.528e-04  Data: 0.034 (0.040)
Train: 54 [ 750/989 ( 76%)]  Loss: 2.93 (3.08)  Time: 0.648s,  197.40/s  (0.654s,  195.83/s)  LR: 6.528e-04  Data: 0.053 (0.040)
Train: 54 [ 800/989 ( 81%)]  Loss: 2.60 (3.06)  Time: 0.646s,  198.00/s  (0.654s,  195.71/s)  LR: 6.528e-04  Data: 0.030 (0.040)
Train: 54 [ 850/989 ( 86%)]  Loss: 2.83 (3.04)  Time: 0.693s,  184.58/s  (0.654s,  195.58/s)  LR: 6.528e-04  Data: 0.038 (0.040)
Train: 54 [ 900/989 ( 91%)]  Loss: 2.89 (3.04)  Time: 0.622s,  205.90/s  (0.656s,  195.25/s)  LR: 6.528e-04  Data: 0.032 (0.040)
Train: 54 [ 950/989 ( 96%)]  Loss: 3.23 (3.05)  Time: 0.656s,  195.19/s  (0.657s,  194.93/s)  LR: 6.528e-04  Data: 0.031 (0.040)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.239 (1.239)  Loss:   1.277 ( 1.277)  Acc@1:  75.000 ( 75.000)  Acc@5:  92.188 ( 92.188)
Test: [  39/39]  Time: 0.048 (0.311)  Loss:   0.861 ( 1.173)  Acc@1:  75.000 ( 75.380)  Acc@5: 100.000 ( 92.800)
Current checkpoints:
 ('./output/train/Upd_Exp2_Image100/checkpoint-53.pth.tar', 75.4)
 ('./output/train/Upd_Exp2_Image100/checkpoint-54.pth.tar', 75.38)
 ('./output/train/Upd_Exp2_Image100/checkpoint-52.pth.tar', 73.94)
 ('./output/train/Upd_Exp2_Image100/checkpoint-50.pth.tar', 73.72)
 ('./output/train/Upd_Exp2_Image100/checkpoint-51.pth.tar', 73.72)
 ('./output/train/Upd_Exp2_Image100/checkpoint-49.pth.tar', 73.28)
 ('./output/train/Upd_Exp2_Image100/checkpoint-48.pth.tar', 73.08)
 ('./output/train/Upd_Exp2_Image100/checkpoint-47.pth.tar', 72.72)
 ('./output/train/Upd_Exp2_Image100/checkpoint-42.pth.tar', 72.28)
 ('./output/train/Upd_Exp2_Image100/checkpoint-43.pth.tar', 72.22)

Train: 55 [   0/989 (  0%)]  Loss: 3.05 (3.05)  Time: 1.698s,   75.37/s  (1.698s,   75.37/s)  LR: 6.509e-04  Data: 0.986 (0.986)
Train: 55 [  50/989 (  5%)]  Loss: 3.24 (3.15)  Time: 0.652s,  196.29/s  (0.683s,  187.35/s)  LR: 6.509e-04  Data: 0.031 (0.057)
Train: 55 [ 100/989 ( 10%)]  Loss: 2.67 (2.99)  Time: 0.639s,  200.42/s  (0.667s,  191.94/s)  LR: 6.509e-04  Data: 0.037 (0.049)
Train: 55 [ 150/989 ( 15%)]  Loss: 3.40 (3.09)  Time: 0.648s,  197.63/s  (0.662s,  193.27/s)  LR: 6.509e-04  Data: 0.036 (0.045)
Train: 55 [ 200/989 ( 20%)]  Loss: 2.73 (3.02)  Time: 0.648s,  197.40/s  (0.661s,  193.74/s)  LR: 6.509e-04  Data: 0.043 (0.044)
Train: 55 [ 250/989 ( 25%)]  Loss: 3.35 (3.08)  Time: 0.657s,  194.68/s  (0.659s,  194.14/s)  LR: 6.509e-04  Data: 0.037 (0.043)
Train: 55 [ 300/989 ( 30%)]  Loss: 2.16 (2.94)  Time: 0.650s,  196.78/s  (0.658s,  194.41/s)  LR: 6.509e-04  Data: 0.040 (0.042)
Train: 55 [ 350/989 ( 35%)]  Loss: 2.94 (2.94)  Time: 0.664s,  192.73/s  (0.658s,  194.59/s)  LR: 6.509e-04  Data: 0.049 (0.041)
Train: 55 [ 400/989 ( 40%)]  Loss: 2.20 (2.86)  Time: 0.657s,  194.83/s  (0.658s,  194.64/s)  LR: 6.509e-04  Data: 0.039 (0.041)
Train: 55 [ 450/989 ( 46%)]  Loss: 2.58 (2.83)  Time: 0.599s,  213.68/s  (0.657s,  194.78/s)  LR: 6.509e-04  Data: 0.029 (0.041)
Train: 55 [ 500/989 ( 51%)]  Loss: 3.28 (2.87)  Time: 0.668s,  191.73/s  (0.657s,  194.77/s)  LR: 6.509e-04  Data: 0.048 (0.041)
Train: 55 [ 550/989 ( 56%)]  Loss: 2.25 (2.82)  Time: 0.649s,  197.18/s  (0.657s,  194.82/s)  LR: 6.509e-04  Data: 0.031 (0.041)
Train: 55 [ 600/989 ( 61%)]  Loss: 2.90 (2.83)  Time: 0.662s,  193.49/s  (0.656s,  194.97/s)  LR: 6.509e-04  Data: 0.039 (0.040)
Train: 55 [ 650/989 ( 66%)]  Loss: 2.71 (2.82)  Time: 0.667s,  192.04/s  (0.656s,  195.06/s)  LR: 6.509e-04  Data: 0.032 (0.040)
Train: 55 [ 700/989 ( 71%)]  Loss: 3.65 (2.87)  Time: 0.599s,  213.86/s  (0.656s,  195.05/s)  LR: 6.509e-04  Data: 0.038 (0.040)
Train: 55 [ 750/989 ( 76%)]  Loss: 3.34 (2.90)  Time: 0.635s,  201.61/s  (0.657s,  194.90/s)  LR: 6.509e-04  Data: 0.023 (0.040)
Train: 55 [ 800/989 ( 81%)]  Loss: 3.08 (2.91)  Time: 0.665s,  192.38/s  (0.657s,  194.83/s)  LR: 6.509e-04  Data: 0.047 (0.040)
Train: 55 [ 850/989 ( 86%)]  Loss: 2.63 (2.90)  Time: 0.715s,  179.06/s  (0.658s,  194.49/s)  LR: 6.509e-04  Data: 0.043 (0.040)
Train: 55 [ 900/989 ( 91%)]  Loss: 2.69 (2.89)  Time: 0.679s,  188.41/s  (0.659s,  194.33/s)  LR: 6.509e-04  Data: 0.052 (0.040)
Train: 55 [ 950/989 ( 96%)]  Loss: 2.84 (2.88)  Time: 0.698s,  183.27/s  (0.659s,  194.13/s)  LR: 6.509e-04  Data: 0.047 (0.040)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.210 (1.210)  Loss:   1.284 ( 1.284)  Acc@1:  75.781 ( 75.781)  Acc@5:  89.844 ( 89.844)
Test: [  39/39]  Time: 0.048 (0.310)  Loss:   0.644 ( 1.093)  Acc@1:  87.500 ( 75.200)  Acc@5: 100.000 ( 92.900)
Current checkpoints:
 ('./output/train/Upd_Exp2_Image100/checkpoint-53.pth.tar', 75.4)
 ('./output/train/Upd_Exp2_Image100/checkpoint-54.pth.tar', 75.38)
 ('./output/train/Upd_Exp2_Image100/checkpoint-55.pth.tar', 75.2)
 ('./output/train/Upd_Exp2_Image100/checkpoint-52.pth.tar', 73.94)
 ('./output/train/Upd_Exp2_Image100/checkpoint-50.pth.tar', 73.72)
 ('./output/train/Upd_Exp2_Image100/checkpoint-51.pth.tar', 73.72)
 ('./output/train/Upd_Exp2_Image100/checkpoint-49.pth.tar', 73.28)
 ('./output/train/Upd_Exp2_Image100/checkpoint-48.pth.tar', 73.08)
 ('./output/train/Upd_Exp2_Image100/checkpoint-47.pth.tar', 72.72)
 ('./output/train/Upd_Exp2_Image100/checkpoint-42.pth.tar', 72.28)

Train: 56 [   0/989 (  0%)]  Loss: 2.86 (2.86)  Time: 1.798s,   71.17/s  (1.798s,   71.17/s)  LR: 6.489e-04  Data: 1.052 (1.052)
Train: 56 [  50/989 (  5%)]  Loss: 3.29 (3.08)  Time: 0.641s,  199.66/s  (0.677s,  189.15/s)  LR: 6.489e-04  Data: 0.035 (0.058)
Train: 56 [ 100/989 ( 10%)]  Loss: 2.34 (2.83)  Time: 0.633s,  202.17/s  (0.660s,  193.93/s)  LR: 6.489e-04  Data: 0.037 (0.048)
Train: 56 [ 150/989 ( 15%)]  Loss: 3.64 (3.03)  Time: 0.644s,  198.77/s  (0.655s,  195.43/s)  LR: 6.489e-04  Data: 0.047 (0.044)
Train: 56 [ 200/989 ( 20%)]  Loss: 2.41 (2.91)  Time: 0.645s,  198.37/s  (0.653s,  195.98/s)  LR: 6.489e-04  Data: 0.023 (0.043)
Train: 56 [ 250/989 ( 25%)]  Loss: 2.89 (2.91)  Time: 0.647s,  197.87/s  (0.652s,  196.47/s)  LR: 6.489e-04  Data: 0.027 (0.042)
Train: 56 [ 300/989 ( 30%)]  Loss: 3.25 (2.95)  Time: 0.654s,  195.65/s  (0.651s,  196.56/s)  LR: 6.489e-04  Data: 0.039 (0.042)
Train: 56 [ 350/989 ( 35%)]  Loss: 2.74 (2.93)  Time: 0.618s,  207.26/s  (0.650s,  196.78/s)  LR: 6.489e-04  Data: 0.031 (0.041)
Train: 56 [ 400/989 ( 40%)]  Loss: 2.68 (2.90)  Time: 0.674s,  189.89/s  (0.651s,  196.61/s)  LR: 6.489e-04  Data: 0.035 (0.041)
Train: 56 [ 450/989 ( 46%)]  Loss: 2.92 (2.90)  Time: 0.615s,  208.16/s  (0.651s,  196.63/s)  LR: 6.489e-04  Data: 0.044 (0.041)
Train: 56 [ 500/989 ( 51%)]  Loss: 2.66 (2.88)  Time: 0.645s,  198.31/s  (0.651s,  196.64/s)  LR: 6.489e-04  Data: 0.042 (0.041)
Train: 56 [ 550/989 ( 56%)]  Loss: 2.51 (2.85)  Time: 0.646s,  198.28/s  (0.651s,  196.61/s)  LR: 6.489e-04  Data: 0.030 (0.040)
Train: 56 [ 600/989 ( 61%)]  Loss: 2.74 (2.84)  Time: 0.647s,  197.73/s  (0.651s,  196.56/s)  LR: 6.489e-04  Data: 0.053 (0.040)
Train: 56 [ 650/989 ( 66%)]  Loss: 3.00 (2.85)  Time: 0.635s,  201.64/s  (0.651s,  196.49/s)  LR: 6.489e-04  Data: 0.031 (0.040)
Train: 56 [ 700/989 ( 71%)]  Loss: 2.65 (2.84)  Time: 0.646s,  198.10/s  (0.652s,  196.37/s)  LR: 6.489e-04  Data: 0.040 (0.040)
Train: 56 [ 750/989 ( 76%)]  Loss: 2.70 (2.83)  Time: 0.663s,  192.99/s  (0.652s,  196.19/s)  LR: 6.489e-04  Data: 0.035 (0.040)
Train: 56 [ 800/989 ( 81%)]  Loss: 2.88 (2.83)  Time: 0.636s,  201.16/s  (0.653s,  196.02/s)  LR: 6.489e-04  Data: 0.030 (0.040)
Train: 56 [ 850/989 ( 86%)]  Loss: 2.86 (2.83)  Time: 0.649s,  197.13/s  (0.654s,  195.76/s)  LR: 6.489e-04  Data: 0.040 (0.040)
Train: 56 [ 900/989 ( 91%)]  Loss: 3.34 (2.86)  Time: 0.828s,  154.52/s  (0.655s,  195.41/s)  LR: 6.489e-04  Data: 0.059 (0.040)
Train: 56 [ 950/989 ( 96%)]  Loss: 2.95 (2.87)  Time: 0.643s,  198.95/s  (0.656s,  195.08/s)  LR: 6.489e-04  Data: 0.021 (0.040)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.209 (1.209)  Loss:   1.295 ( 1.295)  Acc@1:  70.312 ( 70.312)  Acc@5:  89.062 ( 89.062)
Test: [  39/39]  Time: 0.048 (0.312)  Loss:   1.045 ( 1.090)  Acc@1:  62.500 ( 74.980)  Acc@5:  87.500 ( 93.180)
Current checkpoints:
 ('./output/train/Upd_Exp2_Image100/checkpoint-53.pth.tar', 75.4)
 ('./output/train/Upd_Exp2_Image100/checkpoint-54.pth.tar', 75.38)
 ('./output/train/Upd_Exp2_Image100/checkpoint-55.pth.tar', 75.2)
 ('./output/train/Upd_Exp2_Image100/checkpoint-56.pth.tar', 74.98)
 ('./output/train/Upd_Exp2_Image100/checkpoint-52.pth.tar', 73.94)
 ('./output/train/Upd_Exp2_Image100/checkpoint-50.pth.tar', 73.72)
 ('./output/train/Upd_Exp2_Image100/checkpoint-51.pth.tar', 73.72)
 ('./output/train/Upd_Exp2_Image100/checkpoint-49.pth.tar', 73.28)
 ('./output/train/Upd_Exp2_Image100/checkpoint-48.pth.tar', 73.08)
 ('./output/train/Upd_Exp2_Image100/checkpoint-47.pth.tar', 72.72)

Train: 57 [   0/989 (  0%)]  Loss: 3.07 (3.07)  Time: 1.850s,   69.17/s  (1.850s,   69.17/s)  LR: 6.468e-04  Data: 1.107 (1.107)
Train: 57 [  50/989 (  5%)]  Loss: 2.94 (3.00)  Time: 0.648s,  197.56/s  (0.683s,  187.35/s)  LR: 6.468e-04  Data: 0.031 (0.058)
Train: 57 [ 100/989 ( 10%)]  Loss: 3.18 (3.06)  Time: 0.638s,  200.67/s  (0.667s,  191.92/s)  LR: 6.468e-04  Data: 0.040 (0.048)
Train: 57 [ 150/989 ( 15%)]  Loss: 2.30 (2.87)  Time: 0.642s,  199.32/s  (0.660s,  193.87/s)  LR: 6.468e-04  Data: 0.023 (0.043)
Train: 57 [ 200/989 ( 20%)]  Loss: 2.93 (2.88)  Time: 0.679s,  188.51/s  (0.656s,  195.04/s)  LR: 6.468e-04  Data: 0.017 (0.041)
Train: 57 [ 250/989 ( 25%)]  Loss: 3.42 (2.97)  Time: 0.667s,  191.92/s  (0.655s,  195.55/s)  LR: 6.468e-04  Data: 0.033 (0.041)
Train: 57 [ 300/989 ( 30%)]  Loss: 3.35 (3.02)  Time: 0.630s,  203.04/s  (0.653s,  196.03/s)  LR: 6.468e-04  Data: 0.049 (0.040)
Train: 57 [ 350/989 ( 35%)]  Loss: 3.14 (3.04)  Time: 0.655s,  195.47/s  (0.653s,  196.15/s)  LR: 6.468e-04  Data: 0.043 (0.040)
Train: 57 [ 400/989 ( 40%)]  Loss: 2.59 (2.99)  Time: 0.647s,  197.91/s  (0.652s,  196.30/s)  LR: 6.468e-04  Data: 0.042 (0.040)
Train: 57 [ 450/989 ( 46%)]  Loss: 3.44 (3.03)  Time: 0.653s,  196.03/s  (0.652s,  196.41/s)  LR: 6.468e-04  Data: 0.034 (0.040)
Train: 57 [ 500/989 ( 51%)]  Loss: 3.21 (3.05)  Time: 0.664s,  192.70/s  (0.651s,  196.52/s)  LR: 6.468e-04  Data: 0.042 (0.040)
Train: 57 [ 550/989 ( 56%)]  Loss: 3.11 (3.06)  Time: 0.697s,  183.67/s  (0.651s,  196.48/s)  LR: 6.468e-04  Data: 0.023 (0.040)
Train: 57 [ 600/989 ( 61%)]  Loss: 2.48 (3.01)  Time: 0.649s,  197.16/s  (0.651s,  196.54/s)  LR: 6.468e-04  Data: 0.048 (0.039)
Train: 57 [ 650/989 ( 66%)]  Loss: 2.28 (2.96)  Time: 0.649s,  197.14/s  (0.652s,  196.39/s)  LR: 6.468e-04  Data: 0.041 (0.039)
Train: 57 [ 700/989 ( 71%)]  Loss: 3.09 (2.97)  Time: 0.656s,  195.18/s  (0.652s,  196.29/s)  LR: 6.468e-04  Data: 0.044 (0.039)
Train: 57 [ 750/989 ( 76%)]  Loss: 3.11 (2.98)  Time: 0.676s,  189.26/s  (0.653s,  196.12/s)  LR: 6.468e-04  Data: 0.041 (0.039)
Train: 57 [ 800/989 ( 81%)]  Loss: 2.58 (2.95)  Time: 0.682s,  187.69/s  (0.653s,  195.96/s)  LR: 6.468e-04  Data: 0.051 (0.039)
Train: 57 [ 850/989 ( 86%)]  Loss: 3.15 (2.96)  Time: 0.668s,  191.48/s  (0.654s,  195.81/s)  LR: 6.468e-04  Data: 0.036 (0.039)
Train: 57 [ 900/989 ( 91%)]  Loss: 2.99 (2.96)  Time: 0.656s,  195.03/s  (0.655s,  195.48/s)  LR: 6.468e-04  Data: 0.037 (0.039)
Train: 57 [ 950/989 ( 96%)]  Loss: 3.04 (2.97)  Time: 0.648s,  197.48/s  (0.656s,  195.09/s)  LR: 6.468e-04  Data: 0.049 (0.039)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.190 (1.190)  Loss:   1.168 ( 1.168)  Acc@1:  73.438 ( 73.438)  Acc@5:  90.625 ( 90.625)
Test: [  39/39]  Time: 0.052 (0.311)  Loss:   0.392 ( 1.072)  Acc@1:  87.500 ( 74.960)  Acc@5: 100.000 ( 93.140)
Current checkpoints:
 ('./output/train/Upd_Exp2_Image100/checkpoint-53.pth.tar', 75.4)
 ('./output/train/Upd_Exp2_Image100/checkpoint-54.pth.tar', 75.38)
 ('./output/train/Upd_Exp2_Image100/checkpoint-55.pth.tar', 75.2)
 ('./output/train/Upd_Exp2_Image100/checkpoint-56.pth.tar', 74.98)
 ('./output/train/Upd_Exp2_Image100/checkpoint-57.pth.tar', 74.96)
 ('./output/train/Upd_Exp2_Image100/checkpoint-52.pth.tar', 73.94)
 ('./output/train/Upd_Exp2_Image100/checkpoint-50.pth.tar', 73.72)
 ('./output/train/Upd_Exp2_Image100/checkpoint-51.pth.tar', 73.72)
 ('./output/train/Upd_Exp2_Image100/checkpoint-49.pth.tar', 73.28)
 ('./output/train/Upd_Exp2_Image100/checkpoint-48.pth.tar', 73.08)

Train: 58 [   0/989 (  0%)]  Loss: 3.49 (3.49)  Time: 1.951s,   65.61/s  (1.951s,   65.61/s)  LR: 6.448e-04  Data: 1.192 (1.192)
Train: 58 [  50/989 (  5%)]  Loss: 2.82 (3.15)  Time: 0.664s,  192.83/s  (0.679s,  188.65/s)  LR: 6.448e-04  Data: 0.034 (0.059)
Train: 58 [ 100/989 ( 10%)]  Loss: 3.54 (3.28)  Time: 0.665s,  192.56/s  (0.662s,  193.38/s)  LR: 6.448e-04  Data: 0.029 (0.048)
Train: 58 [ 150/989 ( 15%)]  Loss: 2.31 (3.04)  Time: 0.642s,  199.45/s  (0.659s,  194.16/s)  LR: 6.448e-04  Data: 0.050 (0.045)
Train: 58 [ 200/989 ( 20%)]  Loss: 3.27 (3.09)  Time: 0.683s,  187.46/s  (0.658s,  194.61/s)  LR: 6.448e-04  Data: 0.045 (0.044)
Train: 58 [ 250/989 ( 25%)]  Loss: 2.92 (3.06)  Time: 0.680s,  188.28/s  (0.656s,  195.01/s)  LR: 6.448e-04  Data: 0.050 (0.043)
Train: 58 [ 300/989 ( 30%)]  Loss: 3.24 (3.08)  Time: 0.692s,  184.94/s  (0.656s,  195.12/s)  LR: 6.448e-04  Data: 0.060 (0.043)
Train: 58 [ 350/989 ( 35%)]  Loss: 2.91 (3.06)  Time: 0.636s,  201.33/s  (0.655s,  195.33/s)  LR: 6.448e-04  Data: 0.043 (0.042)
Train: 58 [ 400/989 ( 40%)]  Loss: 2.88 (3.04)  Time: 0.640s,  199.99/s  (0.655s,  195.48/s)  LR: 6.448e-04  Data: 0.044 (0.042)
Train: 58 [ 450/989 ( 46%)]  Loss: 2.92 (3.03)  Time: 0.669s,  191.26/s  (0.654s,  195.59/s)  LR: 6.448e-04  Data: 0.043 (0.041)
Train: 58 [ 500/989 ( 51%)]  Loss: 2.39 (2.97)  Time: 0.634s,  201.85/s  (0.655s,  195.35/s)  LR: 6.448e-04  Data: 0.034 (0.041)
Train: 58 [ 550/989 ( 56%)]  Loss: 2.43 (2.93)  Time: 0.636s,  201.28/s  (0.655s,  195.36/s)  LR: 6.448e-04  Data: 0.037 (0.041)
Train: 58 [ 600/989 ( 61%)]  Loss: 3.37 (2.96)  Time: 0.653s,  196.08/s  (0.656s,  195.21/s)  LR: 6.448e-04  Data: 0.057 (0.041)
Train: 58 [ 650/989 ( 66%)]  Loss: 2.56 (2.93)  Time: 0.691s,  185.30/s  (0.656s,  195.08/s)  LR: 6.448e-04  Data: 0.057 (0.041)
Train: 58 [ 700/989 ( 71%)]  Loss: 3.31 (2.96)  Time: 0.650s,  196.92/s  (0.657s,  194.93/s)  LR: 6.448e-04  Data: 0.037 (0.041)
Train: 58 [ 750/989 ( 76%)]  Loss: 3.23 (2.98)  Time: 0.669s,  191.19/s  (0.657s,  194.80/s)  LR: 6.448e-04  Data: 0.047 (0.041)
Train: 58 [ 800/989 ( 81%)]  Loss: 3.22 (2.99)  Time: 0.680s,  188.23/s  (0.657s,  194.68/s)  LR: 6.448e-04  Data: 0.038 (0.041)
Train: 58 [ 850/989 ( 86%)]  Loss: 2.81 (2.98)  Time: 0.589s,  217.42/s  (0.658s,  194.52/s)  LR: 6.448e-04  Data: 0.034 (0.041)
Train: 58 [ 900/989 ( 91%)]  Loss: 2.65 (2.96)  Time: 0.658s,  194.42/s  (0.659s,  194.26/s)  LR: 6.448e-04  Data: 0.030 (0.040)
Train: 58 [ 950/989 ( 96%)]  Loss: 2.39 (2.93)  Time: 0.686s,  186.59/s  (0.660s,  194.06/s)  LR: 6.448e-04  Data: 0.031 (0.040)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.194 (1.194)  Loss:   1.504 ( 1.504)  Acc@1:  68.750 ( 68.750)  Acc@5:  87.500 ( 87.500)
Test: [  39/39]  Time: 0.050 (0.308)  Loss:   0.590 ( 1.117)  Acc@1:  87.500 ( 74.560)  Acc@5: 100.000 ( 92.680)
Current checkpoints:
 ('./output/train/Upd_Exp2_Image100/checkpoint-53.pth.tar', 75.4)
 ('./output/train/Upd_Exp2_Image100/checkpoint-54.pth.tar', 75.38)
 ('./output/train/Upd_Exp2_Image100/checkpoint-55.pth.tar', 75.2)
 ('./output/train/Upd_Exp2_Image100/checkpoint-56.pth.tar', 74.98)
 ('./output/train/Upd_Exp2_Image100/checkpoint-57.pth.tar', 74.96)
 ('./output/train/Upd_Exp2_Image100/checkpoint-58.pth.tar', 74.56)
 ('./output/train/Upd_Exp2_Image100/checkpoint-52.pth.tar', 73.94)
 ('./output/train/Upd_Exp2_Image100/checkpoint-50.pth.tar', 73.72)
 ('./output/train/Upd_Exp2_Image100/checkpoint-51.pth.tar', 73.72)
 ('./output/train/Upd_Exp2_Image100/checkpoint-49.pth.tar', 73.28)

Train: 59 [   0/989 (  0%)]  Loss: 3.21 (3.21)  Time: 1.905s,   67.18/s  (1.905s,   67.18/s)  LR: 6.427e-04  Data: 1.198 (1.198)
Train: 59 [  50/989 (  5%)]  Loss: 3.33 (3.27)  Time: 0.635s,  201.64/s  (0.677s,  189.12/s)  LR: 6.427e-04  Data: 0.045 (0.062)
Train: 59 [ 100/989 ( 10%)]  Loss: 2.92 (3.15)  Time: 0.641s,  199.66/s  (0.663s,  193.00/s)  LR: 6.427e-04  Data: 0.043 (0.050)
Train: 59 [ 150/989 ( 15%)]  Loss: 2.84 (3.07)  Time: 0.629s,  203.40/s  (0.657s,  194.87/s)  LR: 6.427e-04  Data: 0.016 (0.045)
Train: 59 [ 200/989 ( 20%)]  Loss: 2.59 (2.98)  Time: 0.658s,  194.59/s  (0.654s,  195.76/s)  LR: 6.427e-04  Data: 0.040 (0.044)
Train: 59 [ 250/989 ( 25%)]  Loss: 3.34 (3.04)  Time: 0.651s,  196.64/s  (0.653s,  196.09/s)  LR: 6.427e-04  Data: 0.044 (0.042)
Train: 59 [ 300/989 ( 30%)]  Loss: 2.24 (2.92)  Time: 0.648s,  197.51/s  (0.651s,  196.50/s)  LR: 6.427e-04  Data: 0.026 (0.041)
Train: 59 [ 350/989 ( 35%)]  Loss: 2.32 (2.85)  Time: 0.660s,  193.92/s  (0.651s,  196.62/s)  LR: 6.427e-04  Data: 0.032 (0.041)
Train: 59 [ 400/989 ( 40%)]  Loss: 3.27 (2.89)  Time: 0.633s,  202.36/s  (0.651s,  196.51/s)  LR: 6.427e-04  Data: 0.025 (0.040)
Train: 59 [ 450/989 ( 46%)]  Loss: 2.09 (2.81)  Time: 0.657s,  194.77/s  (0.651s,  196.56/s)  LR: 6.427e-04  Data: 0.038 (0.040)
Train: 59 [ 500/989 ( 51%)]  Loss: 3.58 (2.88)  Time: 0.655s,  195.29/s  (0.651s,  196.55/s)  LR: 6.427e-04  Data: 0.022 (0.040)
Train: 59 [ 550/989 ( 56%)]  Loss: 3.25 (2.91)  Time: 0.656s,  195.01/s  (0.652s,  196.45/s)  LR: 6.427e-04  Data: 0.055 (0.040)
Train: 59 [ 600/989 ( 61%)]  Loss: 2.30 (2.87)  Time: 0.698s,  183.48/s  (0.652s,  196.30/s)  LR: 6.427e-04  Data: 0.041 (0.040)
Train: 59 [ 650/989 ( 66%)]  Loss: 2.23 (2.82)  Time: 0.701s,  182.57/s  (0.653s,  196.08/s)  LR: 6.427e-04  Data: 0.039 (0.040)
Train: 59 [ 700/989 ( 71%)]  Loss: 2.96 (2.83)  Time: 0.687s,  186.26/s  (0.654s,  195.85/s)  LR: 6.427e-04  Data: 0.051 (0.040)
Train: 59 [ 750/989 ( 76%)]  Loss: 3.36 (2.86)  Time: 0.689s,  185.76/s  (0.654s,  195.66/s)  LR: 6.427e-04  Data: 0.045 (0.040)
Train: 59 [ 800/989 ( 81%)]  Loss: 2.71 (2.86)  Time: 0.644s,  198.70/s  (0.655s,  195.45/s)  LR: 6.427e-04  Data: 0.041 (0.040)
Train: 59 [ 850/989 ( 86%)]  Loss: 2.79 (2.85)  Time: 0.663s,  193.01/s  (0.655s,  195.33/s)  LR: 6.427e-04  Data: 0.032 (0.040)
Train: 59 [ 900/989 ( 91%)]  Loss: 3.39 (2.88)  Time: 0.667s,  191.87/s  (0.656s,  195.01/s)  LR: 6.427e-04  Data: 0.044 (0.040)
Train: 59 [ 950/989 ( 96%)]  Loss: 2.07 (2.84)  Time: 0.719s,  177.97/s  (0.657s,  194.76/s)  LR: 6.427e-04  Data: 0.058 (0.040)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.230 (1.230)  Loss:   1.594 ( 1.594)  Acc@1:  63.281 ( 63.281)  Acc@5:  85.938 ( 85.938)
Test: [  39/39]  Time: 0.051 (0.311)  Loss:   0.600 ( 1.049)  Acc@1:  87.500 ( 74.960)  Acc@5: 100.000 ( 92.700)
Current checkpoints:
 ('./output/train/Upd_Exp2_Image100/checkpoint-53.pth.tar', 75.4)
 ('./output/train/Upd_Exp2_Image100/checkpoint-54.pth.tar', 75.38)
 ('./output/train/Upd_Exp2_Image100/checkpoint-55.pth.tar', 75.2)
 ('./output/train/Upd_Exp2_Image100/checkpoint-56.pth.tar', 74.98)
 ('./output/train/Upd_Exp2_Image100/checkpoint-57.pth.tar', 74.96)
 ('./output/train/Upd_Exp2_Image100/checkpoint-59.pth.tar', 74.96)
 ('./output/train/Upd_Exp2_Image100/checkpoint-58.pth.tar', 74.56)
 ('./output/train/Upd_Exp2_Image100/checkpoint-52.pth.tar', 73.94)
 ('./output/train/Upd_Exp2_Image100/checkpoint-50.pth.tar', 73.72)
 ('./output/train/Upd_Exp2_Image100/checkpoint-51.pth.tar', 73.72)

Train: 60 [   0/989 (  0%)]  Loss: 3.19 (3.19)  Time: 2.017s,   63.45/s  (2.017s,   63.45/s)  LR: 6.405e-04  Data: 1.295 (1.295)
Train: 60 [  50/989 (  5%)]  Loss: 2.79 (2.99)  Time: 0.647s,  197.70/s  (0.681s,  187.96/s)  LR: 6.405e-04  Data: 0.037 (0.066)
Train: 60 [ 100/989 ( 10%)]  Loss: 3.23 (3.07)  Time: 0.662s,  193.49/s  (0.665s,  192.46/s)  LR: 6.405e-04  Data: 0.025 (0.052)
Train: 60 [ 150/989 ( 15%)]  Loss: 3.20 (3.10)  Time: 0.634s,  201.91/s  (0.659s,  194.16/s)  LR: 6.405e-04  Data: 0.041 (0.048)
Train: 60 [ 200/989 ( 20%)]  Loss: 2.76 (3.03)  Time: 0.620s,  206.52/s  (0.656s,  195.19/s)  LR: 6.405e-04  Data: 0.016 (0.046)
Train: 60 [ 250/989 ( 25%)]  Loss: 2.80 (2.99)  Time: 0.643s,  199.15/s  (0.654s,  195.64/s)  LR: 6.405e-04  Data: 0.037 (0.045)
Train: 60 [ 300/989 ( 30%)]  Loss: 2.94 (2.99)  Time: 0.689s,  185.70/s  (0.653s,  195.89/s)  LR: 6.405e-04  Data: 0.038 (0.044)
Train: 60 [ 350/989 ( 35%)]  Loss: 3.73 (3.08)  Time: 0.679s,  188.39/s  (0.653s,  196.09/s)  LR: 6.405e-04  Data: 0.045 (0.043)
Train: 60 [ 400/989 ( 40%)]  Loss: 2.71 (3.04)  Time: 0.638s,  200.78/s  (0.652s,  196.28/s)  LR: 6.405e-04  Data: 0.037 (0.042)
Train: 60 [ 450/989 ( 46%)]  Loss: 3.06 (3.04)  Time: 0.654s,  195.73/s  (0.652s,  196.46/s)  LR: 6.405e-04  Data: 0.038 (0.042)
Train: 60 [ 500/989 ( 51%)]  Loss: 3.37 (3.07)  Time: 0.655s,  195.37/s  (0.651s,  196.53/s)  LR: 6.405e-04  Data: 0.044 (0.042)
Train: 60 [ 550/989 ( 56%)]  Loss: 2.42 (3.02)  Time: 0.640s,  200.08/s  (0.651s,  196.50/s)  LR: 6.405e-04  Data: 0.045 (0.041)
Train: 60 [ 600/989 ( 61%)]  Loss: 2.41 (2.97)  Time: 0.632s,  202.55/s  (0.651s,  196.56/s)  LR: 6.405e-04  Data: 0.038 (0.041)
Train: 60 [ 650/989 ( 66%)]  Loss: 2.95 (2.97)  Time: 0.680s,  188.26/s  (0.652s,  196.34/s)  LR: 6.405e-04  Data: 0.043 (0.041)
Train: 60 [ 700/989 ( 71%)]  Loss: 3.28 (2.99)  Time: 0.657s,  194.80/s  (0.652s,  196.23/s)  LR: 6.405e-04  Data: 0.034 (0.041)
Train: 60 [ 750/989 ( 76%)]  Loss: 2.99 (2.99)  Time: 0.660s,  193.95/s  (0.653s,  195.98/s)  LR: 6.405e-04  Data: 0.041 (0.041)
Train: 60 [ 800/989 ( 81%)]  Loss: 2.55 (2.96)  Time: 0.673s,  190.25/s  (0.654s,  195.81/s)  LR: 6.405e-04  Data: 0.050 (0.041)
Train: 60 [ 850/989 ( 86%)]  Loss: 2.68 (2.95)  Time: 0.698s,  183.49/s  (0.654s,  195.57/s)  LR: 6.405e-04  Data: 0.039 (0.041)
Train: 60 [ 900/989 ( 91%)]  Loss: 2.55 (2.93)  Time: 0.682s,  187.82/s  (0.655s,  195.31/s)  LR: 6.405e-04  Data: 0.035 (0.041)
Train: 60 [ 950/989 ( 96%)]  Loss: 3.57 (2.96)  Time: 0.688s,  186.07/s  (0.657s,  194.95/s)  LR: 6.405e-04  Data: 0.050 (0.041)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.210 (1.210)  Loss:   1.263 ( 1.263)  Acc@1:  72.656 ( 72.656)  Acc@5:  89.844 ( 89.844)
Test: [  39/39]  Time: 0.041 (0.312)  Loss:   0.645 ( 1.062)  Acc@1:  75.000 ( 75.360)  Acc@5: 100.000 ( 93.060)
Current checkpoints:
 ('./output/train/Upd_Exp2_Image100/checkpoint-53.pth.tar', 75.4)
 ('./output/train/Upd_Exp2_Image100/checkpoint-54.pth.tar', 75.38)
 ('./output/train/Upd_Exp2_Image100/checkpoint-60.pth.tar', 75.36)
 ('./output/train/Upd_Exp2_Image100/checkpoint-55.pth.tar', 75.2)
 ('./output/train/Upd_Exp2_Image100/checkpoint-56.pth.tar', 74.98)
 ('./output/train/Upd_Exp2_Image100/checkpoint-57.pth.tar', 74.96)
 ('./output/train/Upd_Exp2_Image100/checkpoint-59.pth.tar', 74.96)
 ('./output/train/Upd_Exp2_Image100/checkpoint-58.pth.tar', 74.56)
 ('./output/train/Upd_Exp2_Image100/checkpoint-52.pth.tar', 73.94)
 ('./output/train/Upd_Exp2_Image100/checkpoint-50.pth.tar', 73.72)

Train: 61 [   0/989 (  0%)]  Loss: 2.95 (2.95)  Time: 1.924s,   66.52/s  (1.924s,   66.52/s)  LR: 6.384e-04  Data: 1.138 (1.138)
Train: 61 [  50/989 (  5%)]  Loss: 2.69 (2.82)  Time: 0.665s,  192.57/s  (0.678s,  188.67/s)  LR: 6.384e-04  Data: 0.053 (0.058)
Train: 61 [ 100/989 ( 10%)]  Loss: 2.59 (2.74)  Time: 0.645s,  198.32/s  (0.663s,  193.08/s)  LR: 6.384e-04  Data: 0.018 (0.047)
Train: 61 [ 150/989 ( 15%)]  Loss: 3.35 (2.89)  Time: 0.659s,  194.22/s  (0.658s,  194.63/s)  LR: 6.384e-04  Data: 0.041 (0.044)
Train: 61 [ 200/989 ( 20%)]  Loss: 2.19 (2.75)  Time: 0.660s,  194.05/s  (0.655s,  195.30/s)  LR: 6.384e-04  Data: 0.029 (0.042)
Train: 61 [ 250/989 ( 25%)]  Loss: 3.49 (2.88)  Time: 0.648s,  197.65/s  (0.653s,  195.88/s)  LR: 6.384e-04  Data: 0.039 (0.042)
Train: 61 [ 300/989 ( 30%)]  Loss: 2.56 (2.83)  Time: 0.656s,  195.10/s  (0.653s,  196.08/s)  LR: 6.384e-04  Data: 0.035 (0.041)
Train: 61 [ 350/989 ( 35%)]  Loss: 3.27 (2.89)  Time: 0.620s,  206.31/s  (0.652s,  196.31/s)  LR: 6.384e-04  Data: 0.024 (0.041)
Train: 61 [ 400/989 ( 40%)]  Loss: 2.12 (2.80)  Time: 0.702s,  182.31/s  (0.652s,  196.33/s)  LR: 6.384e-04  Data: 0.051 (0.040)
Train: 61 [ 450/989 ( 46%)]  Loss: 2.86 (2.81)  Time: 0.631s,  202.92/s  (0.651s,  196.50/s)  LR: 6.384e-04  Data: 0.042 (0.040)
Train: 61 [ 500/989 ( 51%)]  Loss: 2.73 (2.80)  Time: 0.637s,  201.09/s  (0.651s,  196.58/s)  LR: 6.384e-04  Data: 0.041 (0.040)
Train: 61 [ 550/989 ( 56%)]  Loss: 3.44 (2.85)  Time: 0.635s,  201.56/s  (0.651s,  196.55/s)  LR: 6.384e-04  Data: 0.045 (0.040)
Train: 61 [ 600/989 ( 61%)]  Loss: 3.18 (2.88)  Time: 0.640s,  199.92/s  (0.651s,  196.53/s)  LR: 6.384e-04  Data: 0.031 (0.040)
Train: 61 [ 650/989 ( 66%)]  Loss: 2.76 (2.87)  Time: 0.635s,  201.65/s  (0.652s,  196.36/s)  LR: 6.384e-04  Data: 0.033 (0.040)
Train: 61 [ 700/989 ( 71%)]  Loss: 2.17 (2.82)  Time: 0.647s,  197.75/s  (0.652s,  196.37/s)  LR: 6.384e-04  Data: 0.037 (0.040)
Train: 61 [ 750/989 ( 76%)]  Loss: 2.36 (2.79)  Time: 0.638s,  200.58/s  (0.652s,  196.34/s)  LR: 6.384e-04  Data: 0.034 (0.040)
Train: 61 [ 800/989 ( 81%)]  Loss: 3.38 (2.83)  Time: 0.683s,  187.40/s  (0.653s,  196.10/s)  LR: 6.384e-04  Data: 0.021 (0.039)
Train: 61 [ 850/989 ( 86%)]  Loss: 2.93 (2.83)  Time: 0.653s,  196.16/s  (0.654s,  195.81/s)  LR: 6.384e-04  Data: 0.036 (0.040)
Train: 61 [ 900/989 ( 91%)]  Loss: 2.80 (2.83)  Time: 0.668s,  191.62/s  (0.655s,  195.56/s)  LR: 6.384e-04  Data: 0.036 (0.040)
Train: 61 [ 950/989 ( 96%)]  Loss: 2.97 (2.84)  Time: 0.707s,  181.01/s  (0.656s,  195.23/s)  LR: 6.384e-04  Data: 0.040 (0.039)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.218 (1.218)  Loss:   1.311 ( 1.311)  Acc@1:  75.000 ( 75.000)  Acc@5:  89.844 ( 89.844)
Test: [  39/39]  Time: 0.058 (0.313)  Loss:   0.870 ( 1.131)  Acc@1:  87.500 ( 76.520)  Acc@5:  87.500 ( 93.260)
Current checkpoints:
 ('./output/train/Upd_Exp2_Image100/checkpoint-61.pth.tar', 76.52)
 ('./output/train/Upd_Exp2_Image100/checkpoint-53.pth.tar', 75.4)
 ('./output/train/Upd_Exp2_Image100/checkpoint-54.pth.tar', 75.38)
 ('./output/train/Upd_Exp2_Image100/checkpoint-60.pth.tar', 75.36)
 ('./output/train/Upd_Exp2_Image100/checkpoint-55.pth.tar', 75.2)
 ('./output/train/Upd_Exp2_Image100/checkpoint-56.pth.tar', 74.98)
 ('./output/train/Upd_Exp2_Image100/checkpoint-57.pth.tar', 74.96)
 ('./output/train/Upd_Exp2_Image100/checkpoint-59.pth.tar', 74.96)
 ('./output/train/Upd_Exp2_Image100/checkpoint-58.pth.tar', 74.56)
 ('./output/train/Upd_Exp2_Image100/checkpoint-52.pth.tar', 73.94)

Train: 62 [   0/989 (  0%)]  Loss: 2.62 (2.62)  Time: 1.719s,   74.47/s  (1.719s,   74.47/s)  LR: 6.362e-04  Data: 0.995 (0.995)
Train: 62 [  50/989 (  5%)]  Loss: 2.20 (2.41)  Time: 0.649s,  197.21/s  (0.677s,  189.10/s)  LR: 6.362e-04  Data: 0.061 (0.057)
Train: 62 [ 100/989 ( 10%)]  Loss: 3.28 (2.70)  Time: 0.652s,  196.27/s  (0.661s,  193.56/s)  LR: 6.362e-04  Data: 0.045 (0.046)
Train: 62 [ 150/989 ( 15%)]  Loss: 2.08 (2.54)  Time: 0.631s,  202.94/s  (0.657s,  194.87/s)  LR: 6.362e-04  Data: 0.039 (0.043)
Train: 62 [ 200/989 ( 20%)]  Loss: 2.56 (2.55)  Time: 0.659s,  194.10/s  (0.655s,  195.46/s)  LR: 6.362e-04  Data: 0.050 (0.042)
Train: 62 [ 250/989 ( 25%)]  Loss: 3.21 (2.66)  Time: 0.619s,  206.87/s  (0.653s,  195.94/s)  LR: 6.362e-04  Data: 0.029 (0.041)
Train: 62 [ 300/989 ( 30%)]  Loss: 3.51 (2.78)  Time: 0.646s,  198.19/s  (0.652s,  196.35/s)  LR: 6.362e-04  Data: 0.046 (0.041)
Train: 62 [ 350/989 ( 35%)]  Loss: 2.39 (2.73)  Time: 0.632s,  202.45/s  (0.651s,  196.49/s)  LR: 6.362e-04  Data: 0.031 (0.040)
Train: 62 [ 400/989 ( 40%)]  Loss: 3.44 (2.81)  Time: 0.634s,  201.82/s  (0.651s,  196.61/s)  LR: 6.362e-04  Data: 0.031 (0.040)
Train: 62 [ 450/989 ( 46%)]  Loss: 3.48 (2.88)  Time: 0.646s,  198.24/s  (0.651s,  196.67/s)  LR: 6.362e-04  Data: 0.043 (0.040)
Train: 62 [ 500/989 ( 51%)]  Loss: 2.60 (2.85)  Time: 0.652s,  196.45/s  (0.651s,  196.73/s)  LR: 6.362e-04  Data: 0.025 (0.040)
Train: 62 [ 550/989 ( 56%)]  Loss: 2.60 (2.83)  Time: 0.645s,  198.36/s  (0.651s,  196.75/s)  LR: 6.362e-04  Data: 0.038 (0.040)
Train: 62 [ 600/989 ( 61%)]  Loss: 2.97 (2.84)  Time: 0.628s,  203.90/s  (0.651s,  196.76/s)  LR: 6.362e-04  Data: 0.031 (0.040)
Train: 62 [ 650/989 ( 66%)]  Loss: 3.49 (2.89)  Time: 0.659s,  194.10/s  (0.651s,  196.69/s)  LR: 6.362e-04  Data: 0.036 (0.039)
Train: 62 [ 700/989 ( 71%)]  Loss: 3.63 (2.94)  Time: 0.660s,  193.87/s  (0.651s,  196.62/s)  LR: 6.362e-04  Data: 0.038 (0.039)
Train: 62 [ 750/989 ( 76%)]  Loss: 3.04 (2.94)  Time: 0.657s,  194.85/s  (0.651s,  196.51/s)  LR: 6.362e-04  Data: 0.028 (0.039)
Train: 62 [ 800/989 ( 81%)]  Loss: 2.76 (2.93)  Time: 0.567s,  225.56/s  (0.652s,  196.36/s)  LR: 6.362e-04  Data: 0.043 (0.039)
Train: 62 [ 850/989 ( 86%)]  Loss: 2.82 (2.93)  Time: 0.634s,  201.80/s  (0.653s,  196.02/s)  LR: 6.362e-04  Data: 0.027 (0.039)
Train: 62 [ 900/989 ( 91%)]  Loss: 3.10 (2.94)  Time: 0.657s,  194.92/s  (0.654s,  195.71/s)  LR: 6.362e-04  Data: 0.029 (0.039)
Train: 62 [ 950/989 ( 96%)]  Loss: 3.27 (2.95)  Time: 0.691s,  185.15/s  (0.655s,  195.38/s)  LR: 6.362e-04  Data: 0.053 (0.039)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.239 (1.239)  Loss:   1.129 ( 1.129)  Acc@1:  75.781 ( 75.781)  Acc@5:  91.406 ( 91.406)
Test: [  39/39]  Time: 0.033 (0.312)  Loss:   0.585 ( 1.049)  Acc@1:  87.500 ( 76.660)  Acc@5: 100.000 ( 93.400)
Current checkpoints:
 ('./output/train/Upd_Exp2_Image100/checkpoint-62.pth.tar', 76.66)
 ('./output/train/Upd_Exp2_Image100/checkpoint-61.pth.tar', 76.52)
 ('./output/train/Upd_Exp2_Image100/checkpoint-53.pth.tar', 75.4)
 ('./output/train/Upd_Exp2_Image100/checkpoint-54.pth.tar', 75.38)
 ('./output/train/Upd_Exp2_Image100/checkpoint-60.pth.tar', 75.36)
 ('./output/train/Upd_Exp2_Image100/checkpoint-55.pth.tar', 75.2)
 ('./output/train/Upd_Exp2_Image100/checkpoint-56.pth.tar', 74.98)
 ('./output/train/Upd_Exp2_Image100/checkpoint-57.pth.tar', 74.96)
 ('./output/train/Upd_Exp2_Image100/checkpoint-59.pth.tar', 74.96)
 ('./output/train/Upd_Exp2_Image100/checkpoint-58.pth.tar', 74.56)

Train: 63 [   0/989 (  0%)]  Loss: 2.57 (2.57)  Time: 2.012s,   63.61/s  (2.012s,   63.61/s)  LR: 6.340e-04  Data: 1.199 (1.199)
Train: 63 [  50/989 (  5%)]  Loss: 3.01 (2.79)  Time: 0.651s,  196.69/s  (0.683s,  187.51/s)  LR: 6.340e-04  Data: 0.040 (0.063)
Train: 63 [ 100/989 ( 10%)]  Loss: 2.41 (2.66)  Time: 0.639s,  200.16/s  (0.665s,  192.43/s)  LR: 6.340e-04  Data: 0.034 (0.051)
Train: 63 [ 150/989 ( 15%)]  Loss: 2.70 (2.67)  Time: 0.649s,  197.27/s  (0.659s,  194.14/s)  LR: 6.340e-04  Data: 0.021 (0.046)
Train: 63 [ 200/989 ( 20%)]  Loss: 2.90 (2.72)  Time: 0.670s,  191.12/s  (0.656s,  195.12/s)  LR: 6.340e-04  Data: 0.047 (0.044)
Train: 63 [ 250/989 ( 25%)]  Loss: 2.42 (2.67)  Time: 0.602s,  212.48/s  (0.654s,  195.74/s)  LR: 6.340e-04  Data: 0.039 (0.043)
Train: 63 [ 300/989 ( 30%)]  Loss: 1.96 (2.57)  Time: 0.635s,  201.66/s  (0.653s,  196.07/s)  LR: 6.340e-04  Data: 0.041 (0.042)
Train: 63 [ 350/989 ( 35%)]  Loss: 2.15 (2.52)  Time: 0.633s,  202.20/s  (0.653s,  196.10/s)  LR: 6.340e-04  Data: 0.040 (0.042)
Train: 63 [ 400/989 ( 40%)]  Loss: 2.99 (2.57)  Time: 0.664s,  192.87/s  (0.652s,  196.33/s)  LR: 6.340e-04  Data: 0.046 (0.041)
Train: 63 [ 450/989 ( 46%)]  Loss: 2.53 (2.57)  Time: 0.634s,  201.99/s  (0.652s,  196.43/s)  LR: 6.340e-04  Data: 0.038 (0.041)
Train: 63 [ 500/989 ( 51%)]  Loss: 2.93 (2.60)  Time: 0.651s,  196.55/s  (0.651s,  196.52/s)  LR: 6.340e-04  Data: 0.031 (0.041)
Train: 63 [ 550/989 ( 56%)]  Loss: 3.41 (2.67)  Time: 0.655s,  195.30/s  (0.651s,  196.61/s)  LR: 6.340e-04  Data: 0.038 (0.040)
Train: 63 [ 600/989 ( 61%)]  Loss: 2.34 (2.64)  Time: 0.654s,  195.67/s  (0.651s,  196.57/s)  LR: 6.340e-04  Data: 0.031 (0.040)
Train: 63 [ 650/989 ( 66%)]  Loss: 2.13 (2.61)  Time: 0.674s,  190.01/s  (0.652s,  196.45/s)  LR: 6.340e-04  Data: 0.043 (0.040)
Train: 63 [ 700/989 ( 71%)]  Loss: 3.56 (2.67)  Time: 0.649s,  197.23/s  (0.652s,  196.45/s)  LR: 6.340e-04  Data: 0.033 (0.040)
Train: 63 [ 750/989 ( 76%)]  Loss: 2.01 (2.63)  Time: 0.687s,  186.40/s  (0.652s,  196.36/s)  LR: 6.340e-04  Data: 0.052 (0.040)
Train: 63 [ 800/989 ( 81%)]  Loss: 3.23 (2.66)  Time: 0.652s,  196.30/s  (0.652s,  196.19/s)  LR: 6.340e-04  Data: 0.037 (0.040)
Train: 63 [ 850/989 ( 86%)]  Loss: 2.64 (2.66)  Time: 0.653s,  195.96/s  (0.653s,  195.91/s)  LR: 6.340e-04  Data: 0.042 (0.040)
Train: 63 [ 900/989 ( 91%)]  Loss: 2.39 (2.65)  Time: 0.656s,  195.06/s  (0.655s,  195.51/s)  LR: 6.340e-04  Data: 0.042 (0.040)
Train: 63 [ 950/989 ( 96%)]  Loss: 2.82 (2.66)  Time: 0.638s,  200.63/s  (0.656s,  195.09/s)  LR: 6.340e-04  Data: 0.026 (0.040)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.292 (1.292)  Loss:   1.000 ( 1.000)  Acc@1:  78.906 ( 78.906)  Acc@5:  92.188 ( 92.188)
Test: [  39/39]  Time: 0.055 (0.315)  Loss:   0.503 ( 0.997)  Acc@1:  87.500 ( 76.760)  Acc@5:  87.500 ( 93.600)
Current checkpoints:
 ('./output/train/Upd_Exp2_Image100/checkpoint-63.pth.tar', 76.76)
 ('./output/train/Upd_Exp2_Image100/checkpoint-62.pth.tar', 76.66)
 ('./output/train/Upd_Exp2_Image100/checkpoint-61.pth.tar', 76.52)
 ('./output/train/Upd_Exp2_Image100/checkpoint-53.pth.tar', 75.4)
 ('./output/train/Upd_Exp2_Image100/checkpoint-54.pth.tar', 75.38)
 ('./output/train/Upd_Exp2_Image100/checkpoint-60.pth.tar', 75.36)
 ('./output/train/Upd_Exp2_Image100/checkpoint-55.pth.tar', 75.2)
 ('./output/train/Upd_Exp2_Image100/checkpoint-56.pth.tar', 74.98)
 ('./output/train/Upd_Exp2_Image100/checkpoint-57.pth.tar', 74.96)
 ('./output/train/Upd_Exp2_Image100/checkpoint-59.pth.tar', 74.96)

Train: 64 [   0/989 (  0%)]  Loss: 2.07 (2.07)  Time: 1.858s,   68.88/s  (1.858s,   68.88/s)  LR: 6.317e-04  Data: 1.110 (1.110)
Train: 64 [  50/989 (  5%)]  Loss: 2.83 (2.45)  Time: 0.638s,  200.70/s  (0.680s,  188.33/s)  LR: 6.317e-04  Data: 0.046 (0.057)
Train: 64 [ 100/989 ( 10%)]  Loss: 2.98 (2.63)  Time: 0.635s,  201.42/s  (0.663s,  193.00/s)  LR: 6.317e-04  Data: 0.041 (0.046)
Train: 64 [ 150/989 ( 15%)]  Loss: 2.51 (2.60)  Time: 0.623s,  205.60/s  (0.657s,  194.82/s)  LR: 6.317e-04  Data: 0.023 (0.044)
Train: 64 [ 200/989 ( 20%)]  Loss: 2.52 (2.58)  Time: 0.628s,  203.92/s  (0.654s,  195.59/s)  LR: 6.317e-04  Data: 0.042 (0.043)
Train: 64 [ 250/989 ( 25%)]  Loss: 2.68 (2.60)  Time: 0.645s,  198.37/s  (0.653s,  196.13/s)  LR: 6.317e-04  Data: 0.062 (0.042)
Train: 64 [ 300/989 ( 30%)]  Loss: 2.73 (2.62)  Time: 0.665s,  192.55/s  (0.652s,  196.41/s)  LR: 6.317e-04  Data: 0.053 (0.041)
slurmstepd: error: *** JOB 2006714 ON i67 CANCELLED AT 2024-03-03T16:53:27 DUE TO TIME LIMIT ***
