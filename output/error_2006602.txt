[2024-03-02 20:42:25,183] torch.distributed.run: [WARNING] 
[2024-03-02 20:42:25,183] torch.distributed.run: [WARNING] *****************************************
[2024-03-02 20:42:25,183] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-03-02 20:42:25,183] torch.distributed.run: [WARNING] *****************************************
Training in distributed mode with multiple processes, 1 device per process.Process 0, total 2, device cuda:0.
WARNING: No pretrained configuration specified for MONet_T model. Using a default. Please add a config to the model pretrained_cfg registry or pass explicitly.
Training in distributed mode with multiple processes, 1 device per process.Process 1, total 2, device cuda:1.
WARNING: No pretrained configuration specified for MONet_T model. Using a default. Please add a config to the model pretrained_cfg registry or pass explicitly.
Model MONet_T created, param count:10165736
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.875
	crop_mode: center
Learning rate (0.0007071067811865476) calculated from base learning rate (0.001) and effective global batch size (128) with sqrt scaling.
AMP not enabled. Training in float32.
Using native Torch DistributedDataParallel.
/home/sharipov/monet/venv/lib64/python3.9/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Scheduled epochs: 300. LR stepped per epoch.
/home/sharipov/monet/venv/lib64/python3.9/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Train: 0 [   0/989 (  0%)]  Loss: 4.61 (4.61)  Time: 7.290s,   17.56/s  (7.290s,   17.56/s)  LR: 1.000e-05  Data: 3.122 (3.122)
Train: 0 [  50/989 (  5%)]  Loss: 4.58 (4.60)  Time: 0.704s,  181.80/s  (0.793s,  161.50/s)  LR: 1.000e-05  Data: 0.023 (0.095)
Train: 0 [ 100/989 ( 10%)]  Loss: 4.53 (4.57)  Time: 0.661s,  193.72/s  (0.731s,  175.22/s)  LR: 1.000e-05  Data: 0.042 (0.067)
Train: 0 [ 150/989 ( 15%)]  Loss: 4.57 (4.57)  Time: 0.686s,  186.53/s  (0.709s,  180.61/s)  LR: 1.000e-05  Data: 0.065 (0.058)
Train: 0 [ 200/989 ( 20%)]  Loss: 4.56 (4.57)  Time: 0.641s,  199.66/s  (0.698s,  183.42/s)  LR: 1.000e-05  Data: 0.041 (0.054)
Train: 0 [ 250/989 ( 25%)]  Loss: 4.53 (4.56)  Time: 0.666s,  192.28/s  (0.692s,  185.01/s)  LR: 1.000e-05  Data: 0.041 (0.051)
Train: 0 [ 300/989 ( 30%)]  Loss: 4.53 (4.56)  Time: 0.671s,  190.89/s  (0.688s,  186.06/s)  LR: 1.000e-05  Data: 0.040 (0.049)
Train: 0 [ 350/989 ( 35%)]  Loss: 4.50 (4.55)  Time: 0.677s,  188.99/s  (0.685s,  186.83/s)  LR: 1.000e-05  Data: 0.032 (0.048)
Train: 0 [ 400/989 ( 40%)]  Loss: 4.55 (4.55)  Time: 0.671s,  190.74/s  (0.684s,  187.25/s)  LR: 1.000e-05  Data: 0.035 (0.046)
Train: 0 [ 450/989 ( 46%)]  Loss: 4.48 (4.54)  Time: 0.688s,  186.03/s  (0.683s,  187.42/s)  LR: 1.000e-05  Data: 0.057 (0.046)
Train: 0 [ 500/989 ( 51%)]  Loss: 4.57 (4.55)  Time: 0.672s,  190.54/s  (0.682s,  187.68/s)  LR: 1.000e-05  Data: 0.050 (0.045)
Train: 0 [ 550/989 ( 56%)]  Loss: 4.45 (4.54)  Time: 0.715s,  178.95/s  (0.681s,  187.90/s)  LR: 1.000e-05  Data: 0.031 (0.044)
Train: 0 [ 600/989 ( 61%)]  Loss: 4.54 (4.54)  Time: 0.645s,  198.43/s  (0.681s,  188.02/s)  LR: 1.000e-05  Data: 0.026 (0.044)
Train: 0 [ 650/989 ( 66%)]  Loss: 4.52 (4.54)  Time: 0.661s,  193.69/s  (0.680s,  188.19/s)  LR: 1.000e-05  Data: 0.033 (0.042)
Train: 0 [ 700/989 ( 71%)]  Loss: 4.49 (4.53)  Time: 0.663s,  193.12/s  (0.680s,  188.18/s)  LR: 1.000e-05  Data: 0.032 (0.042)
Train: 0 [ 750/989 ( 76%)]  Loss: 4.52 (4.53)  Time: 0.705s,  181.52/s  (0.680s,  188.22/s)  LR: 1.000e-05  Data: 0.041 (0.042)
Train: 0 [ 800/989 ( 81%)]  Loss: 4.52 (4.53)  Time: 0.709s,  180.44/s  (0.680s,  188.26/s)  LR: 1.000e-05  Data: 0.048 (0.041)
Train: 0 [ 850/989 ( 86%)]  Loss: 4.38 (4.52)  Time: 0.665s,  192.61/s  (0.680s,  188.13/s)  LR: 1.000e-05  Data: 0.033 (0.041)
Train: 0 [ 900/989 ( 91%)]  Loss: 4.53 (4.52)  Time: 0.679s,  188.42/s  (0.681s,  187.87/s)  LR: 1.000e-05  Data: 0.028 (0.040)
Train: 0 [ 950/989 ( 96%)]  Loss: 4.59 (4.53)  Time: 0.665s,  192.40/s  (0.682s,  187.68/s)  LR: 1.000e-05  Data: 0.027 (0.040)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 3.891 (3.891)  Loss:   4.084 ( 4.084)  Acc@1:   7.031 (  7.031)  Acc@5:  30.469 ( 30.469)
Test: [  39/39]  Time: 0.232 (0.391)  Loss:   4.430 ( 4.249)  Acc@1:   0.000 (  6.800)  Acc@5:  12.500 ( 22.900)
Current checkpoints:
 ('./output/train/Upd_Exp1_Image100/checkpoint-0.pth.tar', 6.8)

Train: 1 [   0/989 (  0%)]  Loss: 4.52 (4.52)  Time: 1.988s,   64.39/s  (1.988s,   64.39/s)  LR: 7.971e-05  Data: 0.979 (0.979)
Train: 1 [  50/989 (  5%)]  Loss: 4.55 (4.54)  Time: 0.660s,  194.05/s  (0.689s,  185.73/s)  LR: 7.971e-05  Data: 0.037 (0.052)
Train: 1 [ 100/989 ( 10%)]  Loss: 4.50 (4.53)  Time: 0.657s,  194.93/s  (0.676s,  189.37/s)  LR: 7.971e-05  Data: 0.037 (0.042)
Train: 1 [ 150/989 ( 15%)]  Loss: 4.56 (4.53)  Time: 0.637s,  200.95/s  (0.671s,  190.80/s)  LR: 7.971e-05  Data: 0.035 (0.039)
Train: 1 [ 200/989 ( 20%)]  Loss: 4.56 (4.54)  Time: 0.651s,  196.64/s  (0.670s,  191.06/s)  LR: 7.971e-05  Data: 0.023 (0.038)
Train: 1 [ 250/989 ( 25%)]  Loss: 4.49 (4.53)  Time: 0.652s,  196.23/s  (0.670s,  191.09/s)  LR: 7.971e-05  Data: 0.052 (0.037)
Train: 1 [ 300/989 ( 30%)]  Loss: 4.44 (4.52)  Time: 0.636s,  201.24/s  (0.669s,  191.39/s)  LR: 7.971e-05  Data: 0.038 (0.036)
Train: 1 [ 350/989 ( 35%)]  Loss: 4.49 (4.51)  Time: 0.649s,  197.29/s  (0.667s,  191.82/s)  LR: 7.971e-05  Data: 0.039 (0.036)
Train: 1 [ 400/989 ( 40%)]  Loss: 4.49 (4.51)  Time: 0.664s,  192.79/s  (0.666s,  192.12/s)  LR: 7.971e-05  Data: 0.035 (0.036)
Train: 1 [ 450/989 ( 46%)]  Loss: 4.45 (4.51)  Time: 0.653s,  195.93/s  (0.665s,  192.43/s)  LR: 7.971e-05  Data: 0.035 (0.035)
Train: 1 [ 500/989 ( 51%)]  Loss: 4.46 (4.50)  Time: 0.640s,  199.92/s  (0.665s,  192.52/s)  LR: 7.971e-05  Data: 0.029 (0.035)
Train: 1 [ 550/989 ( 56%)]  Loss: 4.56 (4.51)  Time: 0.641s,  199.62/s  (0.665s,  192.50/s)  LR: 7.971e-05  Data: 0.037 (0.035)
Train: 1 [ 600/989 ( 61%)]  Loss: 4.50 (4.51)  Time: 0.691s,  185.14/s  (0.665s,  192.37/s)  LR: 7.971e-05  Data: 0.039 (0.035)
Train: 1 [ 650/989 ( 66%)]  Loss: 4.37 (4.50)  Time: 0.650s,  197.07/s  (0.666s,  192.31/s)  LR: 7.971e-05  Data: 0.035 (0.035)
Train: 1 [ 700/989 ( 71%)]  Loss: 4.45 (4.49)  Time: 0.695s,  184.10/s  (0.666s,  192.14/s)  LR: 7.971e-05  Data: 0.045 (0.035)
Train: 1 [ 750/989 ( 76%)]  Loss: 4.33 (4.48)  Time: 0.653s,  196.01/s  (0.667s,  191.92/s)  LR: 7.971e-05  Data: 0.018 (0.034)
Train: 1 [ 800/989 ( 81%)]  Loss: 4.42 (4.48)  Time: 0.811s,  157.82/s  (0.668s,  191.64/s)  LR: 7.971e-05  Data: 0.035 (0.034)
Train: 1 [ 850/989 ( 86%)]  Loss: 4.37 (4.47)  Time: 0.704s,  181.84/s  (0.669s,  191.35/s)  LR: 7.971e-05  Data: 0.044 (0.034)
Train: 1 [ 900/989 ( 91%)]  Loss: 4.34 (4.47)  Time: 0.666s,  192.16/s  (0.670s,  191.04/s)  LR: 7.971e-05  Data: 0.016 (0.034)
Train: 1 [ 950/989 ( 96%)]  Loss: 4.39 (4.46)  Time: 0.689s,  185.77/s  (0.672s,  190.56/s)  LR: 7.971e-05  Data: 0.032 (0.034)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.300 (1.300)  Loss:   4.040 ( 4.040)  Acc@1:  12.500 ( 12.500)  Acc@5:  31.250 ( 31.250)
Test: [  39/39]  Time: 0.050 (0.324)  Loss:   4.040 ( 3.971)  Acc@1:   0.000 ( 10.820)  Acc@5:  25.000 ( 31.880)
Current checkpoints:
 ('./output/train/Upd_Exp1_Image100/checkpoint-1.pth.tar', 10.82)
 ('./output/train/Upd_Exp1_Image100/checkpoint-0.pth.tar', 6.8)

Train: 2 [   0/989 (  0%)]  Loss: 4.41 (4.41)  Time: 1.902s,   67.31/s  (1.902s,   67.31/s)  LR: 1.494e-04  Data: 1.022 (1.022)
Train: 2 [  50/989 (  5%)]  Loss: 4.42 (4.42)  Time: 0.657s,  194.82/s  (0.689s,  185.79/s)  LR: 1.494e-04  Data: 0.034 (0.056)
Train: 2 [ 100/989 ( 10%)]  Loss: 4.37 (4.40)  Time: 0.608s,  210.35/s  (0.673s,  190.26/s)  LR: 1.494e-04  Data: 0.032 (0.045)
Train: 2 [ 150/989 ( 15%)]  Loss: 4.39 (4.40)  Time: 0.658s,  194.44/s  (0.667s,  191.90/s)  LR: 1.494e-04  Data: 0.042 (0.041)
Train: 2 [ 200/989 ( 20%)]  Loss: 4.37 (4.39)  Time: 0.632s,  202.64/s  (0.665s,  192.55/s)  LR: 1.494e-04  Data: 0.029 (0.040)
Train: 2 [ 250/989 ( 25%)]  Loss: 4.48 (4.41)  Time: 0.656s,  195.11/s  (0.663s,  192.94/s)  LR: 1.494e-04  Data: 0.039 (0.039)
Train: 2 [ 300/989 ( 30%)]  Loss: 4.30 (4.39)  Time: 0.662s,  193.33/s  (0.663s,  192.99/s)  LR: 1.494e-04  Data: 0.043 (0.038)
Train: 2 [ 350/989 ( 35%)]  Loss: 4.28 (4.38)  Time: 0.628s,  203.70/s  (0.663s,  193.08/s)  LR: 1.494e-04  Data: 0.028 (0.037)
Train: 2 [ 400/989 ( 40%)]  Loss: 4.34 (4.37)  Time: 0.806s,  158.85/s  (0.664s,  192.87/s)  LR: 1.494e-04  Data: 0.040 (0.036)
Train: 2 [ 450/989 ( 46%)]  Loss: 4.44 (4.38)  Time: 0.637s,  200.79/s  (0.663s,  193.00/s)  LR: 1.494e-04  Data: 0.030 (0.036)
Train: 2 [ 500/989 ( 51%)]  Loss: 4.31 (4.38)  Time: 0.645s,  198.33/s  (0.663s,  193.02/s)  LR: 1.494e-04  Data: 0.032 (0.036)
Train: 2 [ 550/989 ( 56%)]  Loss: 4.42 (4.38)  Time: 0.681s,  187.84/s  (0.663s,  193.09/s)  LR: 1.494e-04  Data: 0.037 (0.035)
Train: 2 [ 600/989 ( 61%)]  Loss: 4.38 (4.38)  Time: 0.660s,  194.00/s  (0.663s,  193.03/s)  LR: 1.494e-04  Data: 0.032 (0.035)
Train: 2 [ 650/989 ( 66%)]  Loss: 4.33 (4.38)  Time: 0.665s,  192.57/s  (0.664s,  192.82/s)  LR: 1.494e-04  Data: 0.044 (0.035)
Train: 2 [ 700/989 ( 71%)]  Loss: 4.36 (4.37)  Time: 0.724s,  176.74/s  (0.665s,  192.62/s)  LR: 1.494e-04  Data: 0.058 (0.036)
Train: 2 [ 750/989 ( 76%)]  Loss: 4.40 (4.38)  Time: 0.669s,  191.25/s  (0.666s,  192.29/s)  LR: 1.494e-04  Data: 0.052 (0.036)
Train: 2 [ 800/989 ( 81%)]  Loss: 4.23 (4.37)  Time: 0.682s,  187.59/s  (0.667s,  192.00/s)  LR: 1.494e-04  Data: 0.034 (0.036)
Train: 2 [ 850/989 ( 86%)]  Loss: 4.35 (4.37)  Time: 0.613s,  208.76/s  (0.668s,  191.65/s)  LR: 1.494e-04  Data: 0.020 (0.036)
Train: 2 [ 900/989 ( 91%)]  Loss: 4.39 (4.37)  Time: 0.672s,  190.50/s  (0.669s,  191.30/s)  LR: 1.494e-04  Data: 0.031 (0.036)
Train: 2 [ 950/989 ( 96%)]  Loss: 4.35 (4.37)  Time: 0.730s,  175.35/s  (0.671s,  190.89/s)  LR: 1.494e-04  Data: 0.020 (0.036)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.296 (1.296)  Loss:   3.709 ( 3.709)  Acc@1:  17.969 ( 17.969)  Acc@5:  37.500 ( 37.500)
Test: [  39/39]  Time: 0.051 (0.325)  Loss:   3.120 ( 3.612)  Acc@1:   0.000 ( 16.100)  Acc@5:  87.500 ( 41.620)
Current checkpoints:
 ('./output/train/Upd_Exp1_Image100/checkpoint-2.pth.tar', 16.1)
 ('./output/train/Upd_Exp1_Image100/checkpoint-1.pth.tar', 10.82)
 ('./output/train/Upd_Exp1_Image100/checkpoint-0.pth.tar', 6.8)

Train: 3 [   0/989 (  0%)]  Loss: 4.34 (4.34)  Time: 1.807s,   70.82/s  (1.807s,   70.82/s)  LR: 2.191e-04  Data: 1.085 (1.085)
Train: 3 [  50/989 (  5%)]  Loss: 4.23 (4.28)  Time: 0.650s,  197.00/s  (0.678s,  188.80/s)  LR: 2.191e-04  Data: 0.052 (0.054)
Train: 3 [ 100/989 ( 10%)]  Loss: 4.25 (4.27)  Time: 0.636s,  201.29/s  (0.664s,  192.86/s)  LR: 2.191e-04  Data: 0.044 (0.045)
Train: 3 [ 150/989 ( 15%)]  Loss: 4.40 (4.30)  Time: 0.648s,  197.41/s  (0.659s,  194.18/s)  LR: 2.191e-04  Data: 0.046 (0.041)
Train: 3 [ 200/989 ( 20%)]  Loss: 4.25 (4.29)  Time: 0.671s,  190.64/s  (0.657s,  194.85/s)  LR: 2.191e-04  Data: 0.048 (0.040)
Train: 3 [ 250/989 ( 25%)]  Loss: 4.34 (4.30)  Time: 0.635s,  201.54/s  (0.656s,  195.07/s)  LR: 2.191e-04  Data: 0.043 (0.039)
Train: 3 [ 300/989 ( 30%)]  Loss: 4.27 (4.30)  Time: 0.675s,  189.67/s  (0.656s,  195.18/s)  LR: 2.191e-04  Data: 0.043 (0.039)
Train: 3 [ 350/989 ( 35%)]  Loss: 4.25 (4.29)  Time: 0.709s,  180.64/s  (0.656s,  195.18/s)  LR: 2.191e-04  Data: 0.039 (0.038)
Train: 3 [ 400/989 ( 40%)]  Loss: 4.28 (4.29)  Time: 0.650s,  196.78/s  (0.657s,  194.93/s)  LR: 2.191e-04  Data: 0.020 (0.038)
Train: 3 [ 450/989 ( 46%)]  Loss: 4.20 (4.28)  Time: 0.648s,  197.49/s  (0.657s,  194.95/s)  LR: 2.191e-04  Data: 0.043 (0.038)
Train: 3 [ 500/989 ( 51%)]  Loss: 4.27 (4.28)  Time: 0.718s,  178.37/s  (0.656s,  194.98/s)  LR: 2.191e-04  Data: 0.021 (0.038)
Train: 3 [ 550/989 ( 56%)]  Loss: 4.15 (4.27)  Time: 0.634s,  201.76/s  (0.657s,  194.92/s)  LR: 2.191e-04  Data: 0.038 (0.038)
Train: 3 [ 600/989 ( 61%)]  Loss: 4.34 (4.27)  Time: 0.651s,  196.51/s  (0.656s,  195.01/s)  LR: 2.191e-04  Data: 0.016 (0.037)
Train: 3 [ 650/989 ( 66%)]  Loss: 3.99 (4.25)  Time: 0.693s,  184.72/s  (0.657s,  194.96/s)  LR: 2.191e-04  Data: 0.030 (0.037)
Train: 3 [ 700/989 ( 71%)]  Loss: 4.22 (4.25)  Time: 0.690s,  185.45/s  (0.657s,  194.85/s)  LR: 2.191e-04  Data: 0.035 (0.037)
Train: 3 [ 750/989 ( 76%)]  Loss: 4.24 (4.25)  Time: 0.657s,  194.94/s  (0.657s,  194.68/s)  LR: 2.191e-04  Data: 0.035 (0.037)
Train: 3 [ 800/989 ( 81%)]  Loss: 3.99 (4.24)  Time: 0.705s,  181.58/s  (0.658s,  194.46/s)  LR: 2.191e-04  Data: 0.041 (0.037)
Train: 3 [ 850/989 ( 86%)]  Loss: 4.09 (4.23)  Time: 0.680s,  188.10/s  (0.660s,  194.05/s)  LR: 2.191e-04  Data: 0.038 (0.037)
Train: 3 [ 900/989 ( 91%)]  Loss: 4.29 (4.23)  Time: 0.636s,  201.22/s  (0.662s,  193.47/s)  LR: 2.191e-04  Data: 0.042 (0.036)
Train: 3 [ 950/989 ( 96%)]  Loss: 3.97 (4.22)  Time: 0.675s,  189.68/s  (0.663s,  192.96/s)  LR: 2.191e-04  Data: 0.039 (0.036)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.305 (1.305)  Loss:   3.416 ( 3.416)  Acc@1:  19.531 ( 19.531)  Acc@5:  48.438 ( 48.438)
Test: [  39/39]  Time: 0.049 (0.322)  Loss:   3.392 ( 3.355)  Acc@1:  25.000 ( 21.800)  Acc@5:  62.500 ( 49.280)
Current checkpoints:
 ('./output/train/Upd_Exp1_Image100/checkpoint-3.pth.tar', 21.8)
 ('./output/train/Upd_Exp1_Image100/checkpoint-2.pth.tar', 16.1)
 ('./output/train/Upd_Exp1_Image100/checkpoint-1.pth.tar', 10.82)
 ('./output/train/Upd_Exp1_Image100/checkpoint-0.pth.tar', 6.8)

Train: 4 [   0/989 (  0%)]  Loss: 4.32 (4.32)  Time: 1.964s,   65.16/s  (1.964s,   65.16/s)  LR: 2.888e-04  Data: 1.238 (1.238)
Train: 4 [  50/989 (  5%)]  Loss: 4.32 (4.32)  Time: 0.625s,  204.69/s  (0.684s,  187.24/s)  LR: 2.888e-04  Data: 0.028 (0.059)
Train: 4 [ 100/989 ( 10%)]  Loss: 4.20 (4.28)  Time: 0.630s,  203.23/s  (0.667s,  191.80/s)  LR: 2.888e-04  Data: 0.032 (0.047)
Train: 4 [ 150/989 ( 15%)]  Loss: 4.29 (4.28)  Time: 0.590s,  216.98/s  (0.662s,  193.45/s)  LR: 2.888e-04  Data: 0.019 (0.042)
Train: 4 [ 200/989 ( 20%)]  Loss: 3.98 (4.22)  Time: 0.667s,  191.87/s  (0.659s,  194.29/s)  LR: 2.888e-04  Data: 0.048 (0.040)
Train: 4 [ 250/989 ( 25%)]  Loss: 4.05 (4.19)  Time: 0.625s,  204.74/s  (0.657s,  194.72/s)  LR: 2.888e-04  Data: 0.024 (0.039)
Train: 4 [ 300/989 ( 30%)]  Loss: 4.33 (4.21)  Time: 0.679s,  188.47/s  (0.657s,  194.86/s)  LR: 2.888e-04  Data: 0.050 (0.039)
Train: 4 [ 350/989 ( 35%)]  Loss: 4.12 (4.20)  Time: 0.622s,  205.95/s  (0.656s,  195.20/s)  LR: 2.888e-04  Data: 0.030 (0.038)
Train: 4 [ 400/989 ( 40%)]  Loss: 3.99 (4.18)  Time: 0.635s,  201.48/s  (0.656s,  195.16/s)  LR: 2.888e-04  Data: 0.029 (0.038)
Train: 4 [ 450/989 ( 46%)]  Loss: 4.28 (4.19)  Time: 0.649s,  197.31/s  (0.656s,  195.22/s)  LR: 2.888e-04  Data: 0.027 (0.038)
Train: 4 [ 500/989 ( 51%)]  Loss: 4.24 (4.19)  Time: 0.629s,  203.49/s  (0.656s,  195.07/s)  LR: 2.888e-04  Data: 0.017 (0.037)
Train: 4 [ 550/989 ( 56%)]  Loss: 3.94 (4.17)  Time: 0.636s,  201.31/s  (0.657s,  194.88/s)  LR: 2.888e-04  Data: 0.026 (0.037)
Train: 4 [ 600/989 ( 61%)]  Loss: 4.05 (4.16)  Time: 0.639s,  200.29/s  (0.657s,  194.82/s)  LR: 2.888e-04  Data: 0.031 (0.037)
Train: 4 [ 650/989 ( 66%)]  Loss: 4.19 (4.17)  Time: 0.648s,  197.41/s  (0.657s,  194.73/s)  LR: 2.888e-04  Data: 0.037 (0.037)
Train: 4 [ 700/989 ( 71%)]  Loss: 4.05 (4.16)  Time: 0.663s,  193.16/s  (0.658s,  194.52/s)  LR: 2.888e-04  Data: 0.029 (0.037)
Train: 4 [ 750/989 ( 76%)]  Loss: 4.25 (4.16)  Time: 0.640s,  199.99/s  (0.658s,  194.47/s)  LR: 2.888e-04  Data: 0.063 (0.037)
Train: 4 [ 800/989 ( 81%)]  Loss: 3.79 (4.14)  Time: 0.776s,  164.98/s  (0.659s,  194.11/s)  LR: 2.888e-04  Data: 0.036 (0.037)
Train: 4 [ 850/989 ( 86%)]  Loss: 4.10 (4.14)  Time: 0.640s,  200.13/s  (0.661s,  193.78/s)  LR: 2.888e-04  Data: 0.023 (0.036)
Train: 4 [ 900/989 ( 91%)]  Loss: 3.88 (4.13)  Time: 0.659s,  194.13/s  (0.662s,  193.40/s)  LR: 2.888e-04  Data: 0.024 (0.036)
Train: 4 [ 950/989 ( 96%)]  Loss: 4.27 (4.13)  Time: 0.655s,  195.27/s  (0.663s,  193.01/s)  LR: 2.888e-04  Data: 0.032 (0.036)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.282 (1.282)  Loss:   3.421 ( 3.421)  Acc@1:  19.531 ( 19.531)  Acc@5:  41.406 ( 41.406)
Test: [  39/39]  Time: 0.049 (0.320)  Loss:   2.955 ( 3.198)  Acc@1:  25.000 ( 24.420)  Acc@5:  75.000 ( 52.880)
Current checkpoints:
 ('./output/train/Upd_Exp1_Image100/checkpoint-4.pth.tar', 24.42)
 ('./output/train/Upd_Exp1_Image100/checkpoint-3.pth.tar', 21.8)
 ('./output/train/Upd_Exp1_Image100/checkpoint-2.pth.tar', 16.1)
 ('./output/train/Upd_Exp1_Image100/checkpoint-1.pth.tar', 10.82)
 ('./output/train/Upd_Exp1_Image100/checkpoint-0.pth.tar', 6.8)

Train: 5 [   0/989 (  0%)]  Loss: 4.36 (4.36)  Time: 1.867s,   68.56/s  (1.867s,   68.56/s)  LR: 3.586e-04  Data: 1.096 (1.096)
Train: 5 [  50/989 (  5%)]  Loss: 4.17 (4.26)  Time: 0.656s,  195.25/s  (0.682s,  187.76/s)  LR: 3.586e-04  Data: 0.036 (0.057)
Train: 5 [ 100/989 ( 10%)]  Loss: 3.77 (4.10)  Time: 0.578s,  221.64/s  (0.665s,  192.35/s)  LR: 3.586e-04  Data: 0.017 (0.047)
Train: 5 [ 150/989 ( 15%)]  Loss: 4.12 (4.11)  Time: 0.621s,  206.25/s  (0.660s,  193.87/s)  LR: 3.586e-04  Data: 0.041 (0.043)
Train: 5 [ 200/989 ( 20%)]  Loss: 4.37 (4.16)  Time: 0.623s,  205.30/s  (0.659s,  194.22/s)  LR: 3.586e-04  Data: 0.040 (0.041)
Train: 5 [ 250/989 ( 25%)]  Loss: 4.24 (4.17)  Time: 0.658s,  194.48/s  (0.658s,  194.53/s)  LR: 3.586e-04  Data: 0.051 (0.040)
Train: 5 [ 300/989 ( 30%)]  Loss: 4.16 (4.17)  Time: 0.612s,  209.31/s  (0.657s,  194.95/s)  LR: 3.586e-04  Data: 0.027 (0.039)
Train: 5 [ 350/989 ( 35%)]  Loss: 3.89 (4.14)  Time: 0.584s,  219.36/s  (0.657s,  194.96/s)  LR: 3.586e-04  Data: 0.026 (0.038)
Train: 5 [ 400/989 ( 40%)]  Loss: 4.04 (4.13)  Time: 0.647s,  197.90/s  (0.657s,  194.89/s)  LR: 3.586e-04  Data: 0.033 (0.039)
Train: 5 [ 450/989 ( 46%)]  Loss: 3.81 (4.09)  Time: 0.649s,  197.12/s  (0.657s,  194.82/s)  LR: 3.586e-04  Data: 0.030 (0.038)
Train: 5 [ 500/989 ( 51%)]  Loss: 4.18 (4.10)  Time: 0.638s,  200.61/s  (0.657s,  194.82/s)  LR: 3.586e-04  Data: 0.042 (0.038)
Train: 5 [ 550/989 ( 56%)]  Loss: 3.99 (4.09)  Time: 0.571s,  223.98/s  (0.657s,  194.89/s)  LR: 3.586e-04  Data: 0.012 (0.038)
Train: 5 [ 600/989 ( 61%)]  Loss: 4.06 (4.09)  Time: 0.650s,  196.86/s  (0.657s,  194.73/s)  LR: 3.586e-04  Data: 0.029 (0.037)
Train: 5 [ 650/989 ( 66%)]  Loss: 4.27 (4.10)  Time: 0.644s,  198.66/s  (0.658s,  194.62/s)  LR: 3.586e-04  Data: 0.042 (0.037)
Train: 5 [ 700/989 ( 71%)]  Loss: 4.31 (4.12)  Time: 0.695s,  184.11/s  (0.658s,  194.38/s)  LR: 3.586e-04  Data: 0.032 (0.037)
Train: 5 [ 750/989 ( 76%)]  Loss: 4.22 (4.12)  Time: 0.651s,  196.52/s  (0.659s,  194.30/s)  LR: 3.586e-04  Data: 0.042 (0.037)
Train: 5 [ 800/989 ( 81%)]  Loss: 3.90 (4.11)  Time: 0.659s,  194.32/s  (0.659s,  194.13/s)  LR: 3.586e-04  Data: 0.044 (0.037)
Train: 5 [ 850/989 ( 86%)]  Loss: 4.07 (4.11)  Time: 0.700s,  182.89/s  (0.660s,  193.79/s)  LR: 3.586e-04  Data: 0.025 (0.037)
Train: 5 [ 900/989 ( 91%)]  Loss: 3.77 (4.09)  Time: 0.624s,  205.07/s  (0.662s,  193.37/s)  LR: 3.586e-04  Data: 0.019 (0.037)
Train: 5 [ 950/989 ( 96%)]  Loss: 3.72 (4.07)  Time: 0.656s,  195.13/s  (0.663s,  193.05/s)  LR: 3.586e-04  Data: 0.026 (0.037)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.260 (1.260)  Loss:   3.010 ( 3.010)  Acc@1:  25.000 ( 25.000)  Acc@5:  66.406 ( 66.406)
Test: [  39/39]  Time: 0.048 (0.322)  Loss:   2.372 ( 2.908)  Acc@1:  37.500 ( 28.320)  Acc@5: 100.000 ( 58.600)
Current checkpoints:
 ('./output/train/Upd_Exp1_Image100/checkpoint-5.pth.tar', 28.32)
 ('./output/train/Upd_Exp1_Image100/checkpoint-4.pth.tar', 24.42)
 ('./output/train/Upd_Exp1_Image100/checkpoint-3.pth.tar', 21.8)
 ('./output/train/Upd_Exp1_Image100/checkpoint-2.pth.tar', 16.1)
 ('./output/train/Upd_Exp1_Image100/checkpoint-1.pth.tar', 10.82)
 ('./output/train/Upd_Exp1_Image100/checkpoint-0.pth.tar', 6.8)

Train: 6 [   0/989 (  0%)]  Loss: 4.00 (4.00)  Time: 1.691s,   75.68/s  (1.691s,   75.68/s)  LR: 4.283e-04  Data: 0.973 (0.973)
Train: 6 [  50/989 (  5%)]  Loss: 3.99 (3.99)  Time: 0.639s,  200.42/s  (0.685s,  186.99/s)  LR: 4.283e-04  Data: 0.032 (0.056)
Train: 6 [ 100/989 ( 10%)]  Loss: 4.19 (4.06)  Time: 0.624s,  205.26/s  (0.669s,  191.42/s)  LR: 4.283e-04  Data: 0.023 (0.045)
Train: 6 [ 150/989 ( 15%)]  Loss: 4.03 (4.05)  Time: 0.659s,  194.09/s  (0.665s,  192.62/s)  LR: 4.283e-04  Data: 0.044 (0.042)
Train: 6 [ 200/989 ( 20%)]  Loss: 3.77 (4.00)  Time: 0.643s,  199.03/s  (0.663s,  192.99/s)  LR: 4.283e-04  Data: 0.030 (0.041)
Train: 6 [ 250/989 ( 25%)]  Loss: 3.68 (3.94)  Time: 0.648s,  197.65/s  (0.662s,  193.33/s)  LR: 4.283e-04  Data: 0.032 (0.040)
Train: 6 [ 300/989 ( 30%)]  Loss: 3.99 (3.95)  Time: 0.634s,  201.84/s  (0.660s,  193.94/s)  LR: 4.283e-04  Data: 0.037 (0.039)
Train: 6 [ 350/989 ( 35%)]  Loss: 3.68 (3.92)  Time: 0.667s,  191.89/s  (0.659s,  194.14/s)  LR: 4.283e-04  Data: 0.038 (0.039)
Train: 6 [ 400/989 ( 40%)]  Loss: 4.02 (3.93)  Time: 0.635s,  201.66/s  (0.659s,  194.30/s)  LR: 4.283e-04  Data: 0.040 (0.038)
Train: 6 [ 450/989 ( 46%)]  Loss: 4.24 (3.96)  Time: 0.634s,  201.78/s  (0.659s,  194.25/s)  LR: 4.283e-04  Data: 0.040 (0.038)
Train: 6 [ 500/989 ( 51%)]  Loss: 3.89 (3.95)  Time: 0.664s,  192.81/s  (0.659s,  194.29/s)  LR: 4.283e-04  Data: 0.031 (0.038)
Train: 6 [ 550/989 ( 56%)]  Loss: 3.87 (3.95)  Time: 0.674s,  189.80/s  (0.659s,  194.22/s)  LR: 4.283e-04  Data: 0.040 (0.038)
Train: 6 [ 600/989 ( 61%)]  Loss: 3.92 (3.94)  Time: 0.643s,  199.02/s  (0.659s,  194.13/s)  LR: 4.283e-04  Data: 0.034 (0.037)
Train: 6 [ 650/989 ( 66%)]  Loss: 4.18 (3.96)  Time: 0.655s,  195.38/s  (0.660s,  193.85/s)  LR: 4.283e-04  Data: 0.045 (0.038)
Train: 6 [ 700/989 ( 71%)]  Loss: 3.90 (3.96)  Time: 0.667s,  191.76/s  (0.661s,  193.73/s)  LR: 4.283e-04  Data: 0.035 (0.037)
Train: 6 [ 750/989 ( 76%)]  Loss: 4.17 (3.97)  Time: 0.803s,  159.48/s  (0.662s,  193.43/s)  LR: 4.283e-04  Data: 0.048 (0.037)
Train: 6 [ 800/989 ( 81%)]  Loss: 3.97 (3.97)  Time: 0.658s,  194.60/s  (0.662s,  193.27/s)  LR: 4.283e-04  Data: 0.038 (0.037)
Train: 6 [ 850/989 ( 86%)]  Loss: 3.95 (3.97)  Time: 0.687s,  186.30/s  (0.663s,  193.06/s)  LR: 4.283e-04  Data: 0.035 (0.037)
Train: 6 [ 900/989 ( 91%)]  Loss: 3.87 (3.96)  Time: 0.624s,  205.07/s  (0.664s,  192.84/s)  LR: 4.283e-04  Data: 0.033 (0.037)
Train: 6 [ 950/989 ( 96%)]  Loss: 3.89 (3.96)  Time: 0.713s,  179.44/s  (0.665s,  192.52/s)  LR: 4.283e-04  Data: 0.051 (0.037)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.281 (1.281)  Loss:   2.717 ( 2.717)  Acc@1:  32.812 ( 32.812)  Acc@5:  64.844 ( 64.844)
Test: [  39/39]  Time: 0.036 (0.322)  Loss:   3.267 ( 2.766)  Acc@1:   0.000 ( 32.220)  Acc@5:  62.500 ( 62.660)
Current checkpoints:
 ('./output/train/Upd_Exp1_Image100/checkpoint-6.pth.tar', 32.22)
 ('./output/train/Upd_Exp1_Image100/checkpoint-5.pth.tar', 28.32)
 ('./output/train/Upd_Exp1_Image100/checkpoint-4.pth.tar', 24.42)
 ('./output/train/Upd_Exp1_Image100/checkpoint-3.pth.tar', 21.8)
 ('./output/train/Upd_Exp1_Image100/checkpoint-2.pth.tar', 16.1)
 ('./output/train/Upd_Exp1_Image100/checkpoint-1.pth.tar', 10.82)
 ('./output/train/Upd_Exp1_Image100/checkpoint-0.pth.tar', 6.8)

Train: 7 [   0/989 (  0%)]  Loss: 4.23 (4.23)  Time: 2.048s,   62.51/s  (2.048s,   62.51/s)  LR: 4.980e-04  Data: 1.277 (1.277)
Train: 7 [  50/989 (  5%)]  Loss: 4.06 (4.14)  Time: 0.637s,  200.96/s  (0.691s,  185.33/s)  LR: 4.980e-04  Data: 0.038 (0.060)
Train: 7 [ 100/989 ( 10%)]  Loss: 3.97 (4.08)  Time: 0.620s,  206.29/s  (0.671s,  190.70/s)  LR: 4.980e-04  Data: 0.020 (0.047)
Train: 7 [ 150/989 ( 15%)]  Loss: 4.26 (4.13)  Time: 0.604s,  212.06/s  (0.666s,  192.11/s)  LR: 4.980e-04  Data: 0.041 (0.045)
Train: 7 [ 200/989 ( 20%)]  Loss: 4.28 (4.16)  Time: 0.633s,  202.34/s  (0.664s,  192.64/s)  LR: 4.980e-04  Data: 0.034 (0.043)
Train: 7 [ 250/989 ( 25%)]  Loss: 4.02 (4.14)  Time: 0.632s,  202.53/s  (0.662s,  193.21/s)  LR: 4.980e-04  Data: 0.026 (0.041)
Train: 7 [ 300/989 ( 30%)]  Loss: 4.43 (4.18)  Time: 0.644s,  198.62/s  (0.662s,  193.35/s)  LR: 4.980e-04  Data: 0.039 (0.040)
Train: 7 [ 350/989 ( 35%)]  Loss: 3.54 (4.10)  Time: 0.692s,  184.88/s  (0.661s,  193.67/s)  LR: 4.980e-04  Data: 0.043 (0.039)
Train: 7 [ 400/989 ( 40%)]  Loss: 4.09 (4.10)  Time: 0.624s,  205.21/s  (0.662s,  193.46/s)  LR: 4.980e-04  Data: 0.053 (0.039)
Train: 7 [ 450/989 ( 46%)]  Loss: 3.64 (4.05)  Time: 0.662s,  193.49/s  (0.661s,  193.69/s)  LR: 4.980e-04  Data: 0.036 (0.039)
Train: 7 [ 500/989 ( 51%)]  Loss: 3.83 (4.03)  Time: 0.679s,  188.65/s  (0.661s,  193.57/s)  LR: 4.980e-04  Data: 0.016 (0.039)
Train: 7 [ 550/989 ( 56%)]  Loss: 3.64 (4.00)  Time: 0.653s,  196.13/s  (0.661s,  193.61/s)  LR: 4.980e-04  Data: 0.041 (0.038)
Train: 7 [ 600/989 ( 61%)]  Loss: 4.12 (4.01)  Time: 0.647s,  197.94/s  (0.661s,  193.64/s)  LR: 4.980e-04  Data: 0.031 (0.038)
Train: 7 [ 650/989 ( 66%)]  Loss: 3.90 (4.00)  Time: 0.660s,  193.90/s  (0.661s,  193.69/s)  LR: 4.980e-04  Data: 0.016 (0.038)
Train: 7 [ 700/989 ( 71%)]  Loss: 4.10 (4.01)  Time: 0.648s,  197.50/s  (0.660s,  193.80/s)  LR: 4.980e-04  Data: 0.038 (0.038)
Train: 7 [ 750/989 ( 76%)]  Loss: 3.55 (3.98)  Time: 0.652s,  196.36/s  (0.661s,  193.73/s)  LR: 4.980e-04  Data: 0.038 (0.037)
Train: 7 [ 800/989 ( 81%)]  Loss: 3.90 (3.97)  Time: 0.623s,  205.60/s  (0.661s,  193.58/s)  LR: 4.980e-04  Data: 0.033 (0.037)
Train: 7 [ 850/989 ( 86%)]  Loss: 3.68 (3.96)  Time: 0.626s,  204.32/s  (0.662s,  193.22/s)  LR: 4.980e-04  Data: 0.031 (0.037)
Train: 7 [ 900/989 ( 91%)]  Loss: 3.67 (3.94)  Time: 0.651s,  196.70/s  (0.663s,  193.00/s)  LR: 4.980e-04  Data: 0.025 (0.037)
Train: 7 [ 950/989 ( 96%)]  Loss: 3.67 (3.93)  Time: 0.630s,  203.30/s  (0.664s,  192.74/s)  LR: 4.980e-04  Data: 0.035 (0.037)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.306 (1.306)  Loss:   2.798 ( 2.798)  Acc@1:  34.375 ( 34.375)  Acc@5:  64.844 ( 64.844)
Test: [  39/39]  Time: 0.049 (0.324)  Loss:   2.340 ( 2.715)  Acc@1:  12.500 ( 35.220)  Acc@5: 100.000 ( 65.640)
Current checkpoints:
 ('./output/train/Upd_Exp1_Image100/checkpoint-7.pth.tar', 35.22)
 ('./output/train/Upd_Exp1_Image100/checkpoint-6.pth.tar', 32.22)
 ('./output/train/Upd_Exp1_Image100/checkpoint-5.pth.tar', 28.32)
 ('./output/train/Upd_Exp1_Image100/checkpoint-4.pth.tar', 24.42)
 ('./output/train/Upd_Exp1_Image100/checkpoint-3.pth.tar', 21.8)
 ('./output/train/Upd_Exp1_Image100/checkpoint-2.pth.tar', 16.1)
 ('./output/train/Upd_Exp1_Image100/checkpoint-1.pth.tar', 10.82)
 ('./output/train/Upd_Exp1_Image100/checkpoint-0.pth.tar', 6.8)

Train: 8 [   0/989 (  0%)]  Loss: 4.08 (4.08)  Time: 1.697s,   75.44/s  (1.697s,   75.44/s)  LR: 5.677e-04  Data: 0.984 (0.984)
Train: 8 [  50/989 (  5%)]  Loss: 3.66 (3.87)  Time: 0.637s,  200.86/s  (0.682s,  187.72/s)  LR: 5.677e-04  Data: 0.032 (0.059)
Train: 8 [ 100/989 ( 10%)]  Loss: 4.03 (3.93)  Time: 0.644s,  198.82/s  (0.667s,  192.00/s)  LR: 5.677e-04  Data: 0.026 (0.046)
Train: 8 [ 150/989 ( 15%)]  Loss: 4.15 (3.98)  Time: 0.657s,  194.69/s  (0.663s,  193.02/s)  LR: 5.677e-04  Data: 0.036 (0.043)
Train: 8 [ 200/989 ( 20%)]  Loss: 3.66 (3.92)  Time: 0.647s,  197.90/s  (0.662s,  193.35/s)  LR: 5.677e-04  Data: 0.048 (0.042)
Train: 8 [ 250/989 ( 25%)]  Loss: 4.02 (3.94)  Time: 0.647s,  197.71/s  (0.661s,  193.73/s)  LR: 5.677e-04  Data: 0.032 (0.041)
Train: 8 [ 300/989 ( 30%)]  Loss: 4.13 (3.96)  Time: 0.643s,  198.99/s  (0.660s,  194.01/s)  LR: 5.677e-04  Data: 0.035 (0.040)
Train: 8 [ 350/989 ( 35%)]  Loss: 3.64 (3.92)  Time: 0.677s,  189.01/s  (0.659s,  194.15/s)  LR: 5.677e-04  Data: 0.059 (0.039)
Train: 8 [ 400/989 ( 40%)]  Loss: 4.17 (3.95)  Time: 0.639s,  200.30/s  (0.660s,  193.98/s)  LR: 5.677e-04  Data: 0.035 (0.039)
Train: 8 [ 450/989 ( 46%)]  Loss: 3.82 (3.94)  Time: 0.634s,  201.77/s  (0.659s,  194.09/s)  LR: 5.677e-04  Data: 0.032 (0.038)
Train: 8 [ 500/989 ( 51%)]  Loss: 4.17 (3.96)  Time: 0.708s,  180.90/s  (0.659s,  194.12/s)  LR: 5.677e-04  Data: 0.040 (0.038)
Train: 8 [ 550/989 ( 56%)]  Loss: 4.12 (3.97)  Time: 0.589s,  217.19/s  (0.659s,  194.21/s)  LR: 5.677e-04  Data: 0.034 (0.038)
Train: 8 [ 600/989 ( 61%)]  Loss: 3.39 (3.93)  Time: 0.638s,  200.72/s  (0.659s,  194.14/s)  LR: 5.677e-04  Data: 0.028 (0.037)
Train: 8 [ 650/989 ( 66%)]  Loss: 3.97 (3.93)  Time: 0.680s,  188.26/s  (0.659s,  194.21/s)  LR: 5.677e-04  Data: 0.052 (0.037)
Train: 8 [ 700/989 ( 71%)]  Loss: 3.82 (3.92)  Time: 0.659s,  194.26/s  (0.659s,  194.13/s)  LR: 5.677e-04  Data: 0.027 (0.038)
Train: 8 [ 750/989 ( 76%)]  Loss: 4.12 (3.93)  Time: 0.649s,  197.34/s  (0.660s,  193.80/s)  LR: 5.677e-04  Data: 0.026 (0.038)
Train: 8 [ 800/989 ( 81%)]  Loss: 3.85 (3.93)  Time: 0.653s,  196.12/s  (0.662s,  193.41/s)  LR: 5.677e-04  Data: 0.034 (0.038)
Train: 8 [ 850/989 ( 86%)]  Loss: 3.67 (3.92)  Time: 0.682s,  187.72/s  (0.663s,  193.09/s)  LR: 5.677e-04  Data: 0.028 (0.037)
Train: 8 [ 900/989 ( 91%)]  Loss: 3.89 (3.91)  Time: 0.650s,  196.80/s  (0.664s,  192.77/s)  LR: 5.677e-04  Data: 0.029 (0.037)
Train: 8 [ 950/989 ( 96%)]  Loss: 3.92 (3.91)  Time: 0.645s,  198.54/s  (0.665s,  192.46/s)  LR: 5.677e-04  Data: 0.022 (0.037)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.271 (1.271)  Loss:   2.695 ( 2.695)  Acc@1:  30.469 ( 30.469)  Acc@5:  70.312 ( 70.312)
Test: [  39/39]  Time: 0.047 (0.322)  Loss:   1.846 ( 2.557)  Acc@1:  37.500 ( 37.740)  Acc@5: 100.000 ( 68.620)
Current checkpoints:
 ('./output/train/Upd_Exp1_Image100/checkpoint-8.pth.tar', 37.74)
 ('./output/train/Upd_Exp1_Image100/checkpoint-7.pth.tar', 35.22)
 ('./output/train/Upd_Exp1_Image100/checkpoint-6.pth.tar', 32.22)
 ('./output/train/Upd_Exp1_Image100/checkpoint-5.pth.tar', 28.32)
 ('./output/train/Upd_Exp1_Image100/checkpoint-4.pth.tar', 24.42)
 ('./output/train/Upd_Exp1_Image100/checkpoint-3.pth.tar', 21.8)
 ('./output/train/Upd_Exp1_Image100/checkpoint-2.pth.tar', 16.1)
 ('./output/train/Upd_Exp1_Image100/checkpoint-1.pth.tar', 10.82)
 ('./output/train/Upd_Exp1_Image100/checkpoint-0.pth.tar', 6.8)

Train: 9 [   0/989 (  0%)]  Loss: 3.64 (3.64)  Time: 1.942s,   65.91/s  (1.942s,   65.91/s)  LR: 6.374e-04  Data: 1.153 (1.153)
Train: 9 [  50/989 (  5%)]  Loss: 3.30 (3.47)  Time: 0.640s,  199.88/s  (0.681s,  187.84/s)  LR: 6.374e-04  Data: 0.029 (0.057)
Train: 9 [ 100/989 ( 10%)]  Loss: 4.17 (3.70)  Time: 0.615s,  208.11/s  (0.665s,  192.59/s)  LR: 6.374e-04  Data: 0.020 (0.047)
Train: 9 [ 150/989 ( 15%)]  Loss: 4.24 (3.84)  Time: 0.656s,  195.08/s  (0.661s,  193.76/s)  LR: 6.374e-04  Data: 0.052 (0.043)
Train: 9 [ 200/989 ( 20%)]  Loss: 3.94 (3.86)  Time: 0.612s,  209.23/s  (0.657s,  194.78/s)  LR: 6.374e-04  Data: 0.017 (0.040)
Train: 9 [ 250/989 ( 25%)]  Loss: 3.77 (3.84)  Time: 0.635s,  201.48/s  (0.656s,  195.21/s)  LR: 6.374e-04  Data: 0.025 (0.040)
Train: 9 [ 300/989 ( 30%)]  Loss: 3.98 (3.86)  Time: 0.633s,  202.34/s  (0.656s,  195.27/s)  LR: 6.374e-04  Data: 0.034 (0.039)
Train: 9 [ 350/989 ( 35%)]  Loss: 4.07 (3.89)  Time: 0.643s,  199.07/s  (0.655s,  195.34/s)  LR: 6.374e-04  Data: 0.032 (0.038)
Train: 9 [ 400/989 ( 40%)]  Loss: 3.58 (3.86)  Time: 0.626s,  204.59/s  (0.655s,  195.45/s)  LR: 6.374e-04  Data: 0.028 (0.038)
Train: 9 [ 450/989 ( 46%)]  Loss: 4.13 (3.88)  Time: 0.648s,  197.46/s  (0.655s,  195.51/s)  LR: 6.374e-04  Data: 0.037 (0.037)
Train: 9 [ 500/989 ( 51%)]  Loss: 3.83 (3.88)  Time: 0.647s,  197.78/s  (0.655s,  195.54/s)  LR: 6.374e-04  Data: 0.038 (0.037)
Train: 9 [ 550/989 ( 56%)]  Loss: 4.22 (3.91)  Time: 0.561s,  228.32/s  (0.655s,  195.48/s)  LR: 6.374e-04  Data: 0.026 (0.037)
Train: 9 [ 600/989 ( 61%)]  Loss: 3.85 (3.90)  Time: 0.628s,  203.95/s  (0.655s,  195.33/s)  LR: 6.374e-04  Data: 0.023 (0.037)
Train: 9 [ 650/989 ( 66%)]  Loss: 3.76 (3.89)  Time: 0.648s,  197.47/s  (0.656s,  195.14/s)  LR: 6.374e-04  Data: 0.061 (0.036)
Train: 9 [ 700/989 ( 71%)]  Loss: 4.07 (3.90)  Time: 0.632s,  202.48/s  (0.656s,  194.99/s)  LR: 6.374e-04  Data: 0.017 (0.036)
Train: 9 [ 750/989 ( 76%)]  Loss: 3.58 (3.88)  Time: 0.553s,  231.45/s  (0.657s,  194.92/s)  LR: 6.374e-04  Data: 0.037 (0.036)
Train: 9 [ 800/989 ( 81%)]  Loss: 3.56 (3.86)  Time: 0.655s,  195.36/s  (0.658s,  194.59/s)  LR: 6.374e-04  Data: 0.034 (0.036)
Train: 9 [ 850/989 ( 86%)]  Loss: 3.58 (3.85)  Time: 0.614s,  208.52/s  (0.659s,  194.26/s)  LR: 6.374e-04  Data: 0.036 (0.036)
Train: 9 [ 900/989 ( 91%)]  Loss: 3.31 (3.82)  Time: 0.659s,  194.18/s  (0.660s,  193.94/s)  LR: 6.374e-04  Data: 0.020 (0.036)
Train: 9 [ 950/989 ( 96%)]  Loss: 3.66 (3.81)  Time: 0.686s,  186.57/s  (0.661s,  193.57/s)  LR: 6.374e-04  Data: 0.055 (0.036)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.264 (1.264)  Loss:   2.535 ( 2.535)  Acc@1:  35.938 ( 35.938)  Acc@5:  67.969 ( 67.969)
Test: [  39/39]  Time: 0.048 (0.322)  Loss:   1.321 ( 2.403)  Acc@1:  87.500 ( 40.720)  Acc@5: 100.000 ( 70.800)
Current checkpoints:
 ('./output/train/Upd_Exp1_Image100/checkpoint-9.pth.tar', 40.72)
 ('./output/train/Upd_Exp1_Image100/checkpoint-8.pth.tar', 37.74)
 ('./output/train/Upd_Exp1_Image100/checkpoint-7.pth.tar', 35.22)
 ('./output/train/Upd_Exp1_Image100/checkpoint-6.pth.tar', 32.22)
 ('./output/train/Upd_Exp1_Image100/checkpoint-5.pth.tar', 28.32)
 ('./output/train/Upd_Exp1_Image100/checkpoint-4.pth.tar', 24.42)
 ('./output/train/Upd_Exp1_Image100/checkpoint-3.pth.tar', 21.8)
 ('./output/train/Upd_Exp1_Image100/checkpoint-2.pth.tar', 16.1)
 ('./output/train/Upd_Exp1_Image100/checkpoint-1.pth.tar', 10.82)
 ('./output/train/Upd_Exp1_Image100/checkpoint-0.pth.tar', 6.8)

Train: 10 [   0/989 (  0%)]  Loss: 3.75 (3.75)  Time: 1.999s,   64.04/s  (1.999s,   64.04/s)  LR: 7.052e-04  Data: 1.232 (1.232)
Train: 10 [  50/989 (  5%)]  Loss: 4.10 (3.93)  Time: 0.611s,  209.61/s  (0.683s,  187.54/s)  LR: 7.052e-04  Data: 0.021 (0.056)
Train: 10 [ 100/989 ( 10%)]  Loss: 4.04 (3.96)  Time: 0.654s,  195.86/s  (0.665s,  192.36/s)  LR: 7.052e-04  Data: 0.025 (0.045)
Train: 10 [ 150/989 ( 15%)]  Loss: 3.69 (3.90)  Time: 0.691s,  185.24/s  (0.661s,  193.75/s)  LR: 7.052e-04  Data: 0.040 (0.042)
Train: 10 [ 200/989 ( 20%)]  Loss: 3.90 (3.90)  Time: 0.629s,  203.65/s  (0.659s,  194.26/s)  LR: 7.052e-04  Data: 0.024 (0.040)
Train: 10 [ 250/989 ( 25%)]  Loss: 4.07 (3.92)  Time: 0.626s,  204.62/s  (0.657s,  194.86/s)  LR: 7.052e-04  Data: 0.019 (0.038)
Train: 10 [ 300/989 ( 30%)]  Loss: 3.90 (3.92)  Time: 0.619s,  206.86/s  (0.656s,  195.13/s)  LR: 7.052e-04  Data: 0.037 (0.038)
Train: 10 [ 350/989 ( 35%)]  Loss: 4.06 (3.94)  Time: 0.648s,  197.65/s  (0.656s,  195.27/s)  LR: 7.052e-04  Data: 0.032 (0.037)
Train: 10 [ 400/989 ( 40%)]  Loss: 4.02 (3.95)  Time: 0.633s,  202.32/s  (0.655s,  195.36/s)  LR: 7.052e-04  Data: 0.029 (0.037)
Train: 10 [ 450/989 ( 46%)]  Loss: 3.71 (3.92)  Time: 0.645s,  198.43/s  (0.655s,  195.34/s)  LR: 7.052e-04  Data: 0.037 (0.037)
Train: 10 [ 500/989 ( 51%)]  Loss: 3.77 (3.91)  Time: 0.622s,  205.95/s  (0.655s,  195.36/s)  LR: 7.052e-04  Data: 0.029 (0.037)
Train: 10 [ 550/989 ( 56%)]  Loss: 4.01 (3.92)  Time: 0.573s,  223.46/s  (0.655s,  195.35/s)  LR: 7.052e-04  Data: 0.025 (0.037)
Train: 10 [ 600/989 ( 61%)]  Loss: 3.96 (3.92)  Time: 0.586s,  218.28/s  (0.655s,  195.33/s)  LR: 7.052e-04  Data: 0.019 (0.036)
Train: 10 [ 650/989 ( 66%)]  Loss: 4.15 (3.94)  Time: 0.646s,  198.22/s  (0.656s,  195.11/s)  LR: 7.052e-04  Data: 0.050 (0.037)
Train: 10 [ 700/989 ( 71%)]  Loss: 3.74 (3.92)  Time: 0.643s,  199.19/s  (0.657s,  194.90/s)  LR: 7.052e-04  Data: 0.026 (0.036)
Train: 10 [ 750/989 ( 76%)]  Loss: 3.98 (3.93)  Time: 0.652s,  196.35/s  (0.658s,  194.66/s)  LR: 7.052e-04  Data: 0.040 (0.036)
Train: 10 [ 800/989 ( 81%)]  Loss: 3.80 (3.92)  Time: 0.691s,  185.25/s  (0.659s,  194.35/s)  LR: 7.052e-04  Data: 0.046 (0.036)
Train: 10 [ 850/989 ( 86%)]  Loss: 3.75 (3.91)  Time: 0.658s,  194.67/s  (0.659s,  194.09/s)  LR: 7.052e-04  Data: 0.027 (0.036)
Train: 10 [ 900/989 ( 91%)]  Loss: 4.05 (3.92)  Time: 0.645s,  198.40/s  (0.661s,  193.77/s)  LR: 7.052e-04  Data: 0.021 (0.036)
Train: 10 [ 950/989 ( 96%)]  Loss: 3.76 (3.91)  Time: 0.692s,  184.99/s  (0.662s,  193.34/s)  LR: 7.052e-04  Data: 0.038 (0.036)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.264 (1.264)  Loss:   2.680 ( 2.680)  Acc@1:  34.375 ( 34.375)  Acc@5:  63.281 ( 63.281)
Test: [  39/39]  Time: 0.048 (0.322)  Loss:   1.743 ( 2.308)  Acc@1:  75.000 ( 43.620)  Acc@5: 100.000 ( 72.640)
Current checkpoints:
 ('./output/train/Upd_Exp1_Image100/checkpoint-10.pth.tar', 43.62)
 ('./output/train/Upd_Exp1_Image100/checkpoint-9.pth.tar', 40.72)
 ('./output/train/Upd_Exp1_Image100/checkpoint-8.pth.tar', 37.74)
 ('./output/train/Upd_Exp1_Image100/checkpoint-7.pth.tar', 35.22)
 ('./output/train/Upd_Exp1_Image100/checkpoint-6.pth.tar', 32.22)
 ('./output/train/Upd_Exp1_Image100/checkpoint-5.pth.tar', 28.32)
 ('./output/train/Upd_Exp1_Image100/checkpoint-4.pth.tar', 24.42)
 ('./output/train/Upd_Exp1_Image100/checkpoint-3.pth.tar', 21.8)
 ('./output/train/Upd_Exp1_Image100/checkpoint-2.pth.tar', 16.1)
 ('./output/train/Upd_Exp1_Image100/checkpoint-1.pth.tar', 10.82)

Train: 11 [   0/989 (  0%)]  Loss: 4.10 (4.10)  Time: 1.708s,   74.92/s  (1.708s,   74.92/s)  LR: 7.048e-04  Data: 0.991 (0.991)
Train: 11 [  50/989 (  5%)]  Loss: 3.76 (3.93)  Time: 0.658s,  194.51/s  (0.674s,  190.01/s)  LR: 7.048e-04  Data: 0.054 (0.054)
Train: 11 [ 100/989 ( 10%)]  Loss: 4.09 (3.99)  Time: 0.663s,  192.95/s  (0.660s,  193.94/s)  LR: 7.048e-04  Data: 0.056 (0.045)
Train: 11 [ 150/989 ( 15%)]  Loss: 4.06 (4.01)  Time: 0.665s,  192.51/s  (0.657s,  194.91/s)  LR: 7.048e-04  Data: 0.053 (0.041)
Train: 11 [ 200/989 ( 20%)]  Loss: 3.83 (3.97)  Time: 0.642s,  199.23/s  (0.654s,  195.57/s)  LR: 7.048e-04  Data: 0.035 (0.040)
Train: 11 [ 250/989 ( 25%)]  Loss: 3.64 (3.92)  Time: 0.729s,  175.70/s  (0.654s,  195.62/s)  LR: 7.048e-04  Data: 0.063 (0.039)
Train: 11 [ 300/989 ( 30%)]  Loss: 3.50 (3.86)  Time: 0.650s,  197.01/s  (0.654s,  195.82/s)  LR: 7.048e-04  Data: 0.016 (0.038)
Train: 11 [ 350/989 ( 35%)]  Loss: 3.96 (3.87)  Time: 0.667s,  191.91/s  (0.654s,  195.72/s)  LR: 7.048e-04  Data: 0.040 (0.038)
Train: 11 [ 400/989 ( 40%)]  Loss: 3.76 (3.86)  Time: 0.644s,  198.71/s  (0.654s,  195.69/s)  LR: 7.048e-04  Data: 0.039 (0.038)
Train: 11 [ 450/989 ( 46%)]  Loss: 3.90 (3.86)  Time: 0.770s,  166.16/s  (0.655s,  195.55/s)  LR: 7.048e-04  Data: 0.046 (0.038)
Train: 11 [ 500/989 ( 51%)]  Loss: 3.81 (3.86)  Time: 0.604s,  211.92/s  (0.655s,  195.49/s)  LR: 7.048e-04  Data: 0.020 (0.038)
Train: 11 [ 550/989 ( 56%)]  Loss: 3.34 (3.81)  Time: 0.635s,  201.68/s  (0.655s,  195.41/s)  LR: 7.048e-04  Data: 0.034 (0.037)
Train: 11 [ 600/989 ( 61%)]  Loss: 3.60 (3.80)  Time: 0.622s,  205.79/s  (0.655s,  195.37/s)  LR: 7.048e-04  Data: 0.027 (0.037)
Train: 11 [ 650/989 ( 66%)]  Loss: 3.80 (3.80)  Time: 0.642s,  199.38/s  (0.655s,  195.30/s)  LR: 7.048e-04  Data: 0.021 (0.037)
Train: 11 [ 700/989 ( 71%)]  Loss: 3.90 (3.80)  Time: 0.775s,  165.21/s  (0.657s,  194.97/s)  LR: 7.048e-04  Data: 0.034 (0.037)
Train: 11 [ 750/989 ( 76%)]  Loss: 3.92 (3.81)  Time: 0.680s,  188.11/s  (0.657s,  194.83/s)  LR: 7.048e-04  Data: 0.046 (0.037)
Train: 11 [ 800/989 ( 81%)]  Loss: 3.69 (3.81)  Time: 0.646s,  198.16/s  (0.658s,  194.56/s)  LR: 7.048e-04  Data: 0.039 (0.037)
Train: 11 [ 850/989 ( 86%)]  Loss: 3.66 (3.80)  Time: 0.625s,  204.72/s  (0.659s,  194.10/s)  LR: 7.048e-04  Data: 0.032 (0.037)
Train: 11 [ 900/989 ( 91%)]  Loss: 3.63 (3.79)  Time: 0.548s,  233.52/s  (0.661s,  193.76/s)  LR: 7.048e-04  Data: 0.019 (0.037)
Train: 11 [ 950/989 ( 96%)]  Loss: 4.02 (3.80)  Time: 0.657s,  194.73/s  (0.662s,  193.21/s)  LR: 7.048e-04  Data: 0.028 (0.037)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.272 (1.272)  Loss:   2.113 ( 2.113)  Acc@1:  50.000 ( 50.000)  Acc@5:  76.562 ( 76.562)
Test: [  39/39]  Time: 0.046 (0.323)  Loss:   2.099 ( 2.199)  Acc@1:  37.500 ( 44.440)  Acc@5:  87.500 ( 74.140)
Current checkpoints:
 ('./output/train/Upd_Exp1_Image100/checkpoint-11.pth.tar', 44.44)
 ('./output/train/Upd_Exp1_Image100/checkpoint-10.pth.tar', 43.62)
 ('./output/train/Upd_Exp1_Image100/checkpoint-9.pth.tar', 40.72)
 ('./output/train/Upd_Exp1_Image100/checkpoint-8.pth.tar', 37.74)
 ('./output/train/Upd_Exp1_Image100/checkpoint-7.pth.tar', 35.22)
 ('./output/train/Upd_Exp1_Image100/checkpoint-6.pth.tar', 32.22)
 ('./output/train/Upd_Exp1_Image100/checkpoint-5.pth.tar', 28.32)
 ('./output/train/Upd_Exp1_Image100/checkpoint-4.pth.tar', 24.42)
 ('./output/train/Upd_Exp1_Image100/checkpoint-3.pth.tar', 21.8)
 ('./output/train/Upd_Exp1_Image100/checkpoint-2.pth.tar', 16.1)

Train: 12 [   0/989 (  0%)]  Loss: 3.96 (3.96)  Time: 1.933s,   66.21/s  (1.933s,   66.21/s)  LR: 7.044e-04  Data: 1.165 (1.165)
Train: 12 [  50/989 (  5%)]  Loss: 3.88 (3.92)  Time: 0.658s,  194.53/s  (0.685s,  186.77/s)  LR: 7.044e-04  Data: 0.029 (0.057)
Train: 12 [ 100/989 ( 10%)]  Loss: 4.00 (3.94)  Time: 0.655s,  195.52/s  (0.666s,  192.06/s)  LR: 7.044e-04  Data: 0.029 (0.047)
Train: 12 [ 150/989 ( 15%)]  Loss: 4.18 (4.00)  Time: 0.657s,  194.90/s  (0.661s,  193.52/s)  LR: 7.044e-04  Data: 0.054 (0.044)
Train: 12 [ 200/989 ( 20%)]  Loss: 3.76 (3.95)  Time: 0.639s,  200.42/s  (0.658s,  194.43/s)  LR: 7.044e-04  Data: 0.041 (0.042)
Train: 12 [ 250/989 ( 25%)]  Loss: 4.36 (4.02)  Time: 0.670s,  190.92/s  (0.657s,  194.83/s)  LR: 7.044e-04  Data: 0.042 (0.041)
Train: 12 [ 300/989 ( 30%)]  Loss: 3.42 (3.94)  Time: 0.664s,  192.90/s  (0.656s,  195.16/s)  LR: 7.044e-04  Data: 0.034 (0.040)
Train: 12 [ 350/989 ( 35%)]  Loss: 3.61 (3.90)  Time: 0.670s,  190.91/s  (0.656s,  195.15/s)  LR: 7.044e-04  Data: 0.043 (0.039)
Train: 12 [ 400/989 ( 40%)]  Loss: 3.59 (3.86)  Time: 0.653s,  196.08/s  (0.655s,  195.35/s)  LR: 7.044e-04  Data: 0.040 (0.039)
Train: 12 [ 450/989 ( 46%)]  Loss: 3.62 (3.84)  Time: 0.658s,  194.44/s  (0.655s,  195.37/s)  LR: 7.044e-04  Data: 0.041 (0.039)
Train: 12 [ 500/989 ( 51%)]  Loss: 3.56 (3.81)  Time: 0.661s,  193.75/s  (0.655s,  195.32/s)  LR: 7.044e-04  Data: 0.040 (0.038)
Train: 12 [ 550/989 ( 56%)]  Loss: 3.49 (3.79)  Time: 0.734s,  174.36/s  (0.656s,  195.19/s)  LR: 7.044e-04  Data: 0.043 (0.038)
Train: 12 [ 600/989 ( 61%)]  Loss: 3.96 (3.80)  Time: 0.740s,  172.94/s  (0.657s,  194.97/s)  LR: 7.044e-04  Data: 0.060 (0.038)
Train: 12 [ 650/989 ( 66%)]  Loss: 3.80 (3.80)  Time: 0.686s,  186.54/s  (0.657s,  194.81/s)  LR: 7.044e-04  Data: 0.042 (0.038)
Train: 12 [ 700/989 ( 71%)]  Loss: 3.50 (3.78)  Time: 0.670s,  191.13/s  (0.658s,  194.59/s)  LR: 7.044e-04  Data: 0.045 (0.038)
Train: 12 [ 750/989 ( 76%)]  Loss: 4.01 (3.79)  Time: 0.647s,  197.82/s  (0.658s,  194.41/s)  LR: 7.044e-04  Data: 0.031 (0.037)
Train: 12 [ 800/989 ( 81%)]  Loss: 3.70 (3.79)  Time: 0.635s,  201.65/s  (0.659s,  194.10/s)  LR: 7.044e-04  Data: 0.019 (0.037)
Train: 12 [ 850/989 ( 86%)]  Loss: 3.51 (3.77)  Time: 0.654s,  195.82/s  (0.661s,  193.73/s)  LR: 7.044e-04  Data: 0.030 (0.037)
Train: 12 [ 900/989 ( 91%)]  Loss: 3.44 (3.76)  Time: 0.656s,  195.24/s  (0.662s,  193.37/s)  LR: 7.044e-04  Data: 0.034 (0.037)
Train: 12 [ 950/989 ( 96%)]  Loss: 3.68 (3.75)  Time: 0.646s,  198.04/s  (0.663s,  192.97/s)  LR: 7.044e-04  Data: 0.019 (0.037)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.318 (1.318)  Loss:   2.216 ( 2.216)  Acc@1:  40.625 ( 40.625)  Acc@5:  76.562 ( 76.562)
Test: [  39/39]  Time: 0.049 (0.325)  Loss:   2.407 ( 2.077)  Acc@1:  37.500 ( 47.700)  Acc@5:  75.000 ( 76.980)
Current checkpoints:
 ('./output/train/Upd_Exp1_Image100/checkpoint-12.pth.tar', 47.7)
 ('./output/train/Upd_Exp1_Image100/checkpoint-11.pth.tar', 44.44)
 ('./output/train/Upd_Exp1_Image100/checkpoint-10.pth.tar', 43.62)
 ('./output/train/Upd_Exp1_Image100/checkpoint-9.pth.tar', 40.72)
 ('./output/train/Upd_Exp1_Image100/checkpoint-8.pth.tar', 37.74)
 ('./output/train/Upd_Exp1_Image100/checkpoint-7.pth.tar', 35.22)
 ('./output/train/Upd_Exp1_Image100/checkpoint-6.pth.tar', 32.22)
 ('./output/train/Upd_Exp1_Image100/checkpoint-5.pth.tar', 28.32)
 ('./output/train/Upd_Exp1_Image100/checkpoint-4.pth.tar', 24.42)
 ('./output/train/Upd_Exp1_Image100/checkpoint-3.pth.tar', 21.8)

Train: 13 [   0/989 (  0%)]  Loss: 3.50 (3.50)  Time: 1.729s,   74.04/s  (1.729s,   74.04/s)  LR: 7.039e-04  Data: 0.995 (0.995)
Train: 13 [  50/989 (  5%)]  Loss: 3.61 (3.55)  Time: 0.633s,  202.12/s  (0.682s,  187.65/s)  LR: 7.039e-04  Data: 0.036 (0.056)
Train: 13 [ 100/989 ( 10%)]  Loss: 3.51 (3.54)  Time: 0.633s,  202.20/s  (0.668s,  191.68/s)  LR: 7.039e-04  Data: 0.022 (0.046)
Train: 13 [ 150/989 ( 15%)]  Loss: 3.25 (3.47)  Time: 0.645s,  198.42/s  (0.665s,  192.51/s)  LR: 7.039e-04  Data: 0.016 (0.045)
Train: 13 [ 200/989 ( 20%)]  Loss: 3.55 (3.48)  Time: 0.616s,  207.66/s  (0.662s,  193.35/s)  LR: 7.039e-04  Data: 0.026 (0.042)
Train: 13 [ 250/989 ( 25%)]  Loss: 3.17 (3.43)  Time: 0.644s,  198.75/s  (0.660s,  193.82/s)  LR: 7.039e-04  Data: 0.036 (0.041)
Train: 13 [ 300/989 ( 30%)]  Loss: 3.32 (3.42)  Time: 0.671s,  190.83/s  (0.659s,  194.35/s)  LR: 7.039e-04  Data: 0.017 (0.039)
Train: 13 [ 350/989 ( 35%)]  Loss: 3.65 (3.44)  Time: 0.697s,  183.74/s  (0.659s,  194.22/s)  LR: 7.039e-04  Data: 0.056 (0.039)
Train: 13 [ 400/989 ( 40%)]  Loss: 4.01 (3.51)  Time: 0.658s,  194.65/s  (0.658s,  194.39/s)  LR: 7.039e-04  Data: 0.036 (0.039)
Train: 13 [ 450/989 ( 46%)]  Loss: 3.36 (3.49)  Time: 0.664s,  192.80/s  (0.659s,  194.24/s)  LR: 7.039e-04  Data: 0.040 (0.039)
Train: 13 [ 500/989 ( 51%)]  Loss: 3.39 (3.48)  Time: 0.697s,  183.60/s  (0.659s,  194.19/s)  LR: 7.039e-04  Data: 0.058 (0.038)
Train: 13 [ 550/989 ( 56%)]  Loss: 3.44 (3.48)  Time: 0.676s,  189.44/s  (0.659s,  194.20/s)  LR: 7.039e-04  Data: 0.051 (0.038)
Train: 13 [ 600/989 ( 61%)]  Loss: 3.67 (3.49)  Time: 0.638s,  200.73/s  (0.659s,  194.26/s)  LR: 7.039e-04  Data: 0.025 (0.038)
Train: 13 [ 650/989 ( 66%)]  Loss: 3.89 (3.52)  Time: 0.633s,  202.07/s  (0.659s,  194.26/s)  LR: 7.039e-04  Data: 0.032 (0.038)
Train: 13 [ 700/989 ( 71%)]  Loss: 3.63 (3.53)  Time: 0.672s,  190.59/s  (0.659s,  194.32/s)  LR: 7.039e-04  Data: 0.042 (0.038)
Train: 13 [ 750/989 ( 76%)]  Loss: 3.20 (3.51)  Time: 0.658s,  194.45/s  (0.659s,  194.16/s)  LR: 7.039e-04  Data: 0.035 (0.037)
Train: 13 [ 800/989 ( 81%)]  Loss: 3.42 (3.50)  Time: 0.733s,  174.68/s  (0.660s,  193.99/s)  LR: 7.039e-04  Data: 0.060 (0.037)
Train: 13 [ 850/989 ( 86%)]  Loss: 3.46 (3.50)  Time: 0.674s,  189.88/s  (0.661s,  193.64/s)  LR: 7.039e-04  Data: 0.029 (0.037)
Train: 13 [ 900/989 ( 91%)]  Loss: 3.17 (3.48)  Time: 0.681s,  187.89/s  (0.662s,  193.24/s)  LR: 7.039e-04  Data: 0.041 (0.037)
Train: 13 [ 950/989 ( 96%)]  Loss: 4.02 (3.51)  Time: 0.692s,  185.05/s  (0.664s,  192.82/s)  LR: 7.039e-04  Data: 0.033 (0.037)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.239 (1.239)  Loss:   2.221 ( 2.221)  Acc@1:  44.531 ( 44.531)  Acc@5:  73.438 ( 73.438)
Test: [  39/39]  Time: 0.058 (0.322)  Loss:   1.407 ( 1.991)  Acc@1:  62.500 ( 51.120)  Acc@5: 100.000 ( 79.020)
Current checkpoints:
 ('./output/train/Upd_Exp1_Image100/checkpoint-13.pth.tar', 51.12)
 ('./output/train/Upd_Exp1_Image100/checkpoint-12.pth.tar', 47.7)
 ('./output/train/Upd_Exp1_Image100/checkpoint-11.pth.tar', 44.44)
 ('./output/train/Upd_Exp1_Image100/checkpoint-10.pth.tar', 43.62)
 ('./output/train/Upd_Exp1_Image100/checkpoint-9.pth.tar', 40.72)
 ('./output/train/Upd_Exp1_Image100/checkpoint-8.pth.tar', 37.74)
 ('./output/train/Upd_Exp1_Image100/checkpoint-7.pth.tar', 35.22)
 ('./output/train/Upd_Exp1_Image100/checkpoint-6.pth.tar', 32.22)
 ('./output/train/Upd_Exp1_Image100/checkpoint-5.pth.tar', 28.32)
 ('./output/train/Upd_Exp1_Image100/checkpoint-4.pth.tar', 24.42)

Train: 14 [   0/989 (  0%)]  Loss: 3.33 (3.33)  Time: 1.870s,   68.44/s  (1.870s,   68.44/s)  LR: 7.034e-04  Data: 1.109 (1.109)
Train: 14 [  50/989 (  5%)]  Loss: 3.37 (3.35)  Time: 0.634s,  201.95/s  (0.687s,  186.45/s)  LR: 7.034e-04  Data: 0.045 (0.061)
Train: 14 [ 100/989 ( 10%)]  Loss: 3.87 (3.52)  Time: 0.634s,  202.02/s  (0.667s,  191.83/s)  LR: 7.034e-04  Data: 0.033 (0.048)
Train: 14 [ 150/989 ( 15%)]  Loss: 3.86 (3.61)  Time: 0.656s,  195.18/s  (0.661s,  193.70/s)  LR: 7.034e-04  Data: 0.042 (0.043)
Train: 14 [ 200/989 ( 20%)]  Loss: 3.07 (3.50)  Time: 0.667s,  191.84/s  (0.659s,  194.18/s)  LR: 7.034e-04  Data: 0.035 (0.043)
Train: 14 [ 250/989 ( 25%)]  Loss: 3.74 (3.54)  Time: 0.625s,  204.73/s  (0.657s,  194.82/s)  LR: 7.034e-04  Data: 0.031 (0.041)
Train: 14 [ 300/989 ( 30%)]  Loss: 3.49 (3.53)  Time: 0.679s,  188.60/s  (0.656s,  195.08/s)  LR: 7.034e-04  Data: 0.051 (0.040)
Train: 14 [ 350/989 ( 35%)]  Loss: 3.56 (3.53)  Time: 0.646s,  198.26/s  (0.655s,  195.29/s)  LR: 7.034e-04  Data: 0.021 (0.039)
Train: 14 [ 400/989 ( 40%)]  Loss: 4.01 (3.59)  Time: 0.650s,  196.93/s  (0.655s,  195.28/s)  LR: 7.034e-04  Data: 0.044 (0.039)
Train: 14 [ 450/989 ( 46%)]  Loss: 3.06 (3.53)  Time: 0.694s,  184.37/s  (0.655s,  195.35/s)  LR: 7.034e-04  Data: 0.045 (0.038)
Train: 14 [ 500/989 ( 51%)]  Loss: 3.45 (3.53)  Time: 0.628s,  203.83/s  (0.655s,  195.35/s)  LR: 7.034e-04  Data: 0.043 (0.038)
Train: 14 [ 550/989 ( 56%)]  Loss: 3.61 (3.53)  Time: 0.658s,  194.59/s  (0.655s,  195.31/s)  LR: 7.034e-04  Data: 0.053 (0.038)
Train: 14 [ 600/989 ( 61%)]  Loss: 3.28 (3.51)  Time: 0.648s,  197.56/s  (0.655s,  195.28/s)  LR: 7.034e-04  Data: 0.031 (0.038)
Train: 14 [ 650/989 ( 66%)]  Loss: 3.23 (3.49)  Time: 0.645s,  198.59/s  (0.656s,  195.22/s)  LR: 7.034e-04  Data: 0.049 (0.038)
Train: 14 [ 700/989 ( 71%)]  Loss: 3.18 (3.47)  Time: 0.667s,  191.77/s  (0.656s,  195.14/s)  LR: 7.034e-04  Data: 0.045 (0.037)
Train: 14 [ 750/989 ( 76%)]  Loss: 3.32 (3.46)  Time: 0.761s,  168.20/s  (0.657s,  194.89/s)  LR: 7.034e-04  Data: 0.028 (0.037)
Train: 14 [ 800/989 ( 81%)]  Loss: 3.77 (3.48)  Time: 0.670s,  191.16/s  (0.657s,  194.69/s)  LR: 7.034e-04  Data: 0.044 (0.037)
Train: 14 [ 850/989 ( 86%)]  Loss: 3.71 (3.49)  Time: 0.617s,  207.51/s  (0.658s,  194.38/s)  LR: 7.034e-04  Data: 0.005 (0.037)
Train: 14 [ 900/989 ( 91%)]  Loss: 3.03 (3.47)  Time: 0.700s,  182.78/s  (0.660s,  194.08/s)  LR: 7.034e-04  Data: 0.044 (0.036)
Train: 14 [ 950/989 ( 96%)]  Loss: 4.07 (3.50)  Time: 0.661s,  193.52/s  (0.661s,  193.72/s)  LR: 7.034e-04  Data: 0.030 (0.036)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.269 (1.269)  Loss:   1.902 ( 1.902)  Acc@1:  57.812 ( 57.812)  Acc@5:  82.031 ( 82.031)
Test: [  39/39]  Time: 0.049 (0.322)  Loss:   1.191 ( 1.999)  Acc@1:  62.500 ( 51.000)  Acc@5: 100.000 ( 79.440)
Current checkpoints:
 ('./output/train/Upd_Exp1_Image100/checkpoint-13.pth.tar', 51.12)
 ('./output/train/Upd_Exp1_Image100/checkpoint-14.pth.tar', 51.0)
 ('./output/train/Upd_Exp1_Image100/checkpoint-12.pth.tar', 47.7)
 ('./output/train/Upd_Exp1_Image100/checkpoint-11.pth.tar', 44.44)
 ('./output/train/Upd_Exp1_Image100/checkpoint-10.pth.tar', 43.62)
 ('./output/train/Upd_Exp1_Image100/checkpoint-9.pth.tar', 40.72)
 ('./output/train/Upd_Exp1_Image100/checkpoint-8.pth.tar', 37.74)
 ('./output/train/Upd_Exp1_Image100/checkpoint-7.pth.tar', 35.22)
 ('./output/train/Upd_Exp1_Image100/checkpoint-6.pth.tar', 32.22)
 ('./output/train/Upd_Exp1_Image100/checkpoint-5.pth.tar', 28.32)

Train: 15 [   0/989 (  0%)]  Loss: 3.54 (3.54)  Time: 1.901s,   67.33/s  (1.901s,   67.33/s)  LR: 7.028e-04  Data: 1.179 (1.179)
Train: 15 [  50/989 (  5%)]  Loss: 3.67 (3.61)  Time: 0.655s,  195.33/s  (0.682s,  187.66/s)  LR: 7.028e-04  Data: 0.040 (0.058)
Train: 15 [ 100/989 ( 10%)]  Loss: 3.16 (3.46)  Time: 0.660s,  193.87/s  (0.665s,  192.39/s)  LR: 7.028e-04  Data: 0.028 (0.047)
Train: 15 [ 150/989 ( 15%)]  Loss: 3.79 (3.54)  Time: 0.626s,  204.35/s  (0.661s,  193.70/s)  LR: 7.028e-04  Data: 0.021 (0.042)
Train: 15 [ 200/989 ( 20%)]  Loss: 3.33 (3.50)  Time: 0.620s,  206.60/s  (0.658s,  194.48/s)  LR: 7.028e-04  Data: 0.020 (0.039)
Train: 15 [ 250/989 ( 25%)]  Loss: 3.60 (3.52)  Time: 0.645s,  198.45/s  (0.657s,  194.76/s)  LR: 7.028e-04  Data: 0.050 (0.038)
Train: 15 [ 300/989 ( 30%)]  Loss: 3.03 (3.45)  Time: 0.663s,  193.02/s  (0.656s,  195.01/s)  LR: 7.028e-04  Data: 0.042 (0.038)
Train: 15 [ 350/989 ( 35%)]  Loss: 3.18 (3.41)  Time: 0.664s,  192.65/s  (0.655s,  195.35/s)  LR: 7.028e-04  Data: 0.043 (0.037)
Train: 15 [ 400/989 ( 40%)]  Loss: 3.98 (3.48)  Time: 0.643s,  199.15/s  (0.655s,  195.28/s)  LR: 7.028e-04  Data: 0.037 (0.037)
Train: 15 [ 450/989 ( 46%)]  Loss: 3.28 (3.46)  Time: 0.713s,  179.49/s  (0.655s,  195.48/s)  LR: 7.028e-04  Data: 0.054 (0.037)
Train: 15 [ 500/989 ( 51%)]  Loss: 3.61 (3.47)  Time: 0.635s,  201.67/s  (0.655s,  195.48/s)  LR: 7.028e-04  Data: 0.034 (0.037)
Train: 15 [ 550/989 ( 56%)]  Loss: 3.45 (3.47)  Time: 0.638s,  200.75/s  (0.655s,  195.32/s)  LR: 7.028e-04  Data: 0.034 (0.037)
Train: 15 [ 600/989 ( 61%)]  Loss: 3.53 (3.47)  Time: 0.653s,  195.93/s  (0.656s,  195.20/s)  LR: 7.028e-04  Data: 0.044 (0.037)
Train: 15 [ 650/989 ( 66%)]  Loss: 3.56 (3.48)  Time: 0.653s,  196.15/s  (0.656s,  195.18/s)  LR: 7.028e-04  Data: 0.039 (0.037)
Train: 15 [ 700/989 ( 71%)]  Loss: 3.86 (3.50)  Time: 0.606s,  211.30/s  (0.656s,  195.10/s)  LR: 7.028e-04  Data: 0.017 (0.036)
Train: 15 [ 750/989 ( 76%)]  Loss: 3.81 (3.52)  Time: 0.633s,  202.31/s  (0.657s,  194.92/s)  LR: 7.028e-04  Data: 0.029 (0.036)
Train: 15 [ 800/989 ( 81%)]  Loss: 4.07 (3.56)  Time: 0.656s,  195.26/s  (0.658s,  194.62/s)  LR: 7.028e-04  Data: 0.027 (0.037)
Train: 15 [ 850/989 ( 86%)]  Loss: 2.99 (3.52)  Time: 0.654s,  195.74/s  (0.659s,  194.30/s)  LR: 7.028e-04  Data: 0.029 (0.036)
Train: 15 [ 900/989 ( 91%)]  Loss: 3.91 (3.54)  Time: 0.653s,  196.10/s  (0.660s,  193.96/s)  LR: 7.028e-04  Data: 0.028 (0.036)
Train: 15 [ 950/989 ( 96%)]  Loss: 3.88 (3.56)  Time: 0.687s,  186.27/s  (0.661s,  193.59/s)  LR: 7.028e-04  Data: 0.035 (0.036)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.251 (1.251)  Loss:   1.796 ( 1.796)  Acc@1:  58.594 ( 58.594)  Acc@5:  82.812 ( 82.812)
Test: [  39/39]  Time: 0.036 (0.321)  Loss:   1.663 ( 1.928)  Acc@1:  37.500 ( 52.660)  Acc@5:  87.500 ( 79.940)
Current checkpoints:
 ('./output/train/Upd_Exp1_Image100/checkpoint-15.pth.tar', 52.66)
 ('./output/train/Upd_Exp1_Image100/checkpoint-13.pth.tar', 51.12)
 ('./output/train/Upd_Exp1_Image100/checkpoint-14.pth.tar', 51.0)
 ('./output/train/Upd_Exp1_Image100/checkpoint-12.pth.tar', 47.7)
 ('./output/train/Upd_Exp1_Image100/checkpoint-11.pth.tar', 44.44)
 ('./output/train/Upd_Exp1_Image100/checkpoint-10.pth.tar', 43.62)
 ('./output/train/Upd_Exp1_Image100/checkpoint-9.pth.tar', 40.72)
 ('./output/train/Upd_Exp1_Image100/checkpoint-8.pth.tar', 37.74)
 ('./output/train/Upd_Exp1_Image100/checkpoint-7.pth.tar', 35.22)
 ('./output/train/Upd_Exp1_Image100/checkpoint-6.pth.tar', 32.22)

Train: 16 [   0/989 (  0%)]  Loss: 3.52 (3.52)  Time: 1.850s,   69.19/s  (1.850s,   69.19/s)  LR: 7.022e-04  Data: 1.115 (1.115)
Train: 16 [  50/989 (  5%)]  Loss: 3.70 (3.61)  Time: 0.643s,  199.04/s  (0.682s,  187.56/s)  LR: 7.022e-04  Data: 0.034 (0.056)
Train: 16 [ 100/989 ( 10%)]  Loss: 3.59 (3.60)  Time: 0.666s,  192.31/s  (0.665s,  192.62/s)  LR: 7.022e-04  Data: 0.042 (0.045)
Train: 16 [ 150/989 ( 15%)]  Loss: 3.62 (3.61)  Time: 0.597s,  214.32/s  (0.659s,  194.17/s)  LR: 7.022e-04  Data: 0.036 (0.043)
Train: 16 [ 200/989 ( 20%)]  Loss: 3.99 (3.68)  Time: 0.637s,  201.08/s  (0.658s,  194.55/s)  LR: 7.022e-04  Data: 0.031 (0.041)
Train: 16 [ 250/989 ( 25%)]  Loss: 3.12 (3.59)  Time: 0.626s,  204.47/s  (0.657s,  194.77/s)  LR: 7.022e-04  Data: 0.024 (0.041)
Train: 16 [ 300/989 ( 30%)]  Loss: 3.80 (3.62)  Time: 0.625s,  204.85/s  (0.657s,  194.96/s)  LR: 7.022e-04  Data: 0.022 (0.040)
Train: 16 [ 350/989 ( 35%)]  Loss: 3.99 (3.67)  Time: 0.698s,  183.34/s  (0.656s,  195.07/s)  LR: 7.022e-04  Data: 0.077 (0.039)
Train: 16 [ 400/989 ( 40%)]  Loss: 3.58 (3.66)  Time: 0.646s,  198.13/s  (0.656s,  195.24/s)  LR: 7.022e-04  Data: 0.043 (0.039)
Train: 16 [ 450/989 ( 46%)]  Loss: 3.59 (3.65)  Time: 0.636s,  201.16/s  (0.656s,  195.27/s)  LR: 7.022e-04  Data: 0.033 (0.038)
Train: 16 [ 500/989 ( 51%)]  Loss: 3.96 (3.68)  Time: 0.634s,  202.01/s  (0.655s,  195.32/s)  LR: 7.022e-04  Data: 0.039 (0.038)
Train: 16 [ 550/989 ( 56%)]  Loss: 3.31 (3.65)  Time: 0.640s,  200.01/s  (0.656s,  195.26/s)  LR: 7.022e-04  Data: 0.038 (0.038)
Train: 16 [ 600/989 ( 61%)]  Loss: 3.11 (3.61)  Time: 0.663s,  193.04/s  (0.656s,  195.24/s)  LR: 7.022e-04  Data: 0.030 (0.037)
Train: 16 [ 650/989 ( 66%)]  Loss: 3.65 (3.61)  Time: 0.655s,  195.36/s  (0.657s,  194.95/s)  LR: 7.022e-04  Data: 0.035 (0.037)
Train: 16 [ 700/989 ( 71%)]  Loss: 3.57 (3.61)  Time: 0.698s,  183.29/s  (0.657s,  194.81/s)  LR: 7.022e-04  Data: 0.036 (0.037)
Train: 16 [ 750/989 ( 76%)]  Loss: 3.57 (3.60)  Time: 0.689s,  185.86/s  (0.658s,  194.67/s)  LR: 7.022e-04  Data: 0.027 (0.037)
Train: 16 [ 800/989 ( 81%)]  Loss: 4.14 (3.64)  Time: 0.643s,  198.94/s  (0.658s,  194.40/s)  LR: 7.022e-04  Data: 0.030 (0.037)
Train: 16 [ 850/989 ( 86%)]  Loss: 3.97 (3.65)  Time: 0.694s,  184.31/s  (0.660s,  194.06/s)  LR: 7.022e-04  Data: 0.034 (0.037)
Train: 16 [ 900/989 ( 91%)]  Loss: 3.51 (3.65)  Time: 0.605s,  211.67/s  (0.661s,  193.70/s)  LR: 7.022e-04  Data: 0.012 (0.037)
Train: 16 [ 950/989 ( 96%)]  Loss: 3.73 (3.65)  Time: 0.527s,  242.80/s  (0.662s,  193.33/s)  LR: 7.022e-04  Data: 0.017 (0.036)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.300 (1.300)  Loss:   1.837 ( 1.837)  Acc@1:  58.594 ( 58.594)  Acc@5:  83.594 ( 83.594)
Test: [  39/39]  Time: 0.049 (0.321)  Loss:   0.814 ( 1.754)  Acc@1:  87.500 ( 55.900)  Acc@5: 100.000 ( 82.640)
Current checkpoints:
 ('./output/train/Upd_Exp1_Image100/checkpoint-16.pth.tar', 55.9)
 ('./output/train/Upd_Exp1_Image100/checkpoint-15.pth.tar', 52.66)
 ('./output/train/Upd_Exp1_Image100/checkpoint-13.pth.tar', 51.12)
 ('./output/train/Upd_Exp1_Image100/checkpoint-14.pth.tar', 51.0)
 ('./output/train/Upd_Exp1_Image100/checkpoint-12.pth.tar', 47.7)
 ('./output/train/Upd_Exp1_Image100/checkpoint-11.pth.tar', 44.44)
 ('./output/train/Upd_Exp1_Image100/checkpoint-10.pth.tar', 43.62)
 ('./output/train/Upd_Exp1_Image100/checkpoint-9.pth.tar', 40.72)
 ('./output/train/Upd_Exp1_Image100/checkpoint-8.pth.tar', 37.74)
 ('./output/train/Upd_Exp1_Image100/checkpoint-7.pth.tar', 35.22)

Train: 17 [   0/989 (  0%)]  Loss: 3.24 (3.24)  Time: 1.954s,   65.52/s  (1.954s,   65.52/s)  LR: 7.016e-04  Data: 1.165 (1.165)
Train: 17 [  50/989 (  5%)]  Loss: 3.32 (3.28)  Time: 0.621s,  206.21/s  (0.682s,  187.75/s)  LR: 7.016e-04  Data: 0.016 (0.057)
Train: 17 [ 100/989 ( 10%)]  Loss: 3.39 (3.31)  Time: 0.645s,  198.44/s  (0.665s,  192.39/s)  LR: 7.016e-04  Data: 0.016 (0.045)
Train: 17 [ 150/989 ( 15%)]  Loss: 2.88 (3.21)  Time: 0.649s,  197.24/s  (0.660s,  194.01/s)  LR: 7.016e-04  Data: 0.049 (0.044)
Train: 17 [ 200/989 ( 20%)]  Loss: 3.34 (3.23)  Time: 0.634s,  201.75/s  (0.657s,  194.77/s)  LR: 7.016e-04  Data: 0.037 (0.042)
Train: 17 [ 250/989 ( 25%)]  Loss: 3.47 (3.27)  Time: 0.625s,  204.76/s  (0.656s,  195.20/s)  LR: 7.016e-04  Data: 0.026 (0.041)
Train: 17 [ 300/989 ( 30%)]  Loss: 3.55 (3.31)  Time: 0.669s,  191.34/s  (0.656s,  195.17/s)  LR: 7.016e-04  Data: 0.040 (0.040)
Train: 17 [ 350/989 ( 35%)]  Loss: 3.30 (3.31)  Time: 0.638s,  200.63/s  (0.655s,  195.42/s)  LR: 7.016e-04  Data: 0.047 (0.040)
Train: 17 [ 400/989 ( 40%)]  Loss: 3.50 (3.33)  Time: 0.659s,  194.29/s  (0.655s,  195.41/s)  LR: 7.016e-04  Data: 0.029 (0.039)
Train: 17 [ 450/989 ( 46%)]  Loss: 3.37 (3.34)  Time: 0.631s,  202.89/s  (0.655s,  195.53/s)  LR: 7.016e-04  Data: 0.027 (0.039)
Train: 17 [ 500/989 ( 51%)]  Loss: 3.30 (3.33)  Time: 0.633s,  202.29/s  (0.654s,  195.62/s)  LR: 7.016e-04  Data: 0.027 (0.038)
Train: 17 [ 550/989 ( 56%)]  Loss: 3.69 (3.36)  Time: 0.626s,  204.36/s  (0.654s,  195.64/s)  LR: 7.016e-04  Data: 0.037 (0.038)
Train: 17 [ 600/989 ( 61%)]  Loss: 3.76 (3.39)  Time: 0.659s,  194.16/s  (0.654s,  195.64/s)  LR: 7.016e-04  Data: 0.042 (0.038)
Train: 17 [ 650/989 ( 66%)]  Loss: 3.54 (3.40)  Time: 0.676s,  189.25/s  (0.655s,  195.52/s)  LR: 7.016e-04  Data: 0.035 (0.038)
Train: 17 [ 700/989 ( 71%)]  Loss: 3.54 (3.41)  Time: 0.665s,  192.51/s  (0.655s,  195.33/s)  LR: 7.016e-04  Data: 0.048 (0.038)
Train: 17 [ 750/989 ( 76%)]  Loss: 4.00 (3.45)  Time: 0.655s,  195.30/s  (0.656s,  195.09/s)  LR: 7.016e-04  Data: 0.038 (0.037)
Train: 17 [ 800/989 ( 81%)]  Loss: 3.84 (3.47)  Time: 0.646s,  198.27/s  (0.658s,  194.67/s)  LR: 7.016e-04  Data: 0.034 (0.037)
Train: 17 [ 850/989 ( 86%)]  Loss: 3.32 (3.46)  Time: 0.682s,  187.66/s  (0.659s,  194.35/s)  LR: 7.016e-04  Data: 0.046 (0.037)
Train: 17 [ 900/989 ( 91%)]  Loss: 3.74 (3.48)  Time: 0.708s,  180.91/s  (0.660s,  193.90/s)  LR: 7.016e-04  Data: 0.034 (0.037)
Train: 17 [ 950/989 ( 96%)]  Loss: 3.56 (3.48)  Time: 0.660s,  193.97/s  (0.662s,  193.48/s)  LR: 7.016e-04  Data: 0.031 (0.037)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.250 (1.250)  Loss:   1.839 ( 1.839)  Acc@1:  57.812 ( 57.812)  Acc@5:  84.375 ( 84.375)
Test: [  39/39]  Time: 0.047 (0.320)  Loss:   1.526 ( 1.769)  Acc@1:  50.000 ( 56.200)  Acc@5: 100.000 ( 83.180)
Current checkpoints:
 ('./output/train/Upd_Exp1_Image100/checkpoint-17.pth.tar', 56.2)
 ('./output/train/Upd_Exp1_Image100/checkpoint-16.pth.tar', 55.9)
 ('./output/train/Upd_Exp1_Image100/checkpoint-15.pth.tar', 52.66)
 ('./output/train/Upd_Exp1_Image100/checkpoint-13.pth.tar', 51.12)
 ('./output/train/Upd_Exp1_Image100/checkpoint-14.pth.tar', 51.0)
 ('./output/train/Upd_Exp1_Image100/checkpoint-12.pth.tar', 47.7)
 ('./output/train/Upd_Exp1_Image100/checkpoint-11.pth.tar', 44.44)
 ('./output/train/Upd_Exp1_Image100/checkpoint-10.pth.tar', 43.62)
 ('./output/train/Upd_Exp1_Image100/checkpoint-9.pth.tar', 40.72)
 ('./output/train/Upd_Exp1_Image100/checkpoint-8.pth.tar', 37.74)

Train: 18 [   0/989 (  0%)]  Loss: 2.88 (2.88)  Time: 1.724s,   74.23/s  (1.724s,   74.23/s)  LR: 7.009e-04  Data: 1.000 (1.000)
Train: 18 [  50/989 (  5%)]  Loss: 3.61 (3.25)  Time: 0.634s,  202.04/s  (0.677s,  188.93/s)  LR: 7.009e-04  Data: 0.041 (0.057)
Train: 18 [ 100/989 ( 10%)]  Loss: 3.85 (3.45)  Time: 0.650s,  196.95/s  (0.663s,  193.20/s)  LR: 7.009e-04  Data: 0.036 (0.048)
Train: 18 [ 150/989 ( 15%)]  Loss: 3.14 (3.37)  Time: 0.637s,  200.80/s  (0.659s,  194.19/s)  LR: 7.009e-04  Data: 0.020 (0.043)
Train: 18 [ 200/989 ( 20%)]  Loss: 3.73 (3.44)  Time: 0.630s,  203.04/s  (0.657s,  194.82/s)  LR: 7.009e-04  Data: 0.028 (0.042)
Train: 18 [ 250/989 ( 25%)]  Loss: 3.43 (3.44)  Time: 0.649s,  197.31/s  (0.655s,  195.33/s)  LR: 7.009e-04  Data: 0.035 (0.040)
Train: 18 [ 300/989 ( 30%)]  Loss: 3.26 (3.42)  Time: 0.651s,  196.63/s  (0.655s,  195.56/s)  LR: 7.009e-04  Data: 0.028 (0.040)
Train: 18 [ 350/989 ( 35%)]  Loss: 3.30 (3.40)  Time: 0.660s,  193.94/s  (0.654s,  195.78/s)  LR: 7.009e-04  Data: 0.037 (0.039)
Train: 18 [ 400/989 ( 40%)]  Loss: 3.62 (3.43)  Time: 0.647s,  197.72/s  (0.653s,  195.92/s)  LR: 7.009e-04  Data: 0.036 (0.039)
Train: 18 [ 450/989 ( 46%)]  Loss: 4.00 (3.48)  Time: 0.547s,  234.13/s  (0.654s,  195.81/s)  LR: 7.009e-04  Data: 0.037 (0.039)
Train: 18 [ 500/989 ( 51%)]  Loss: 3.32 (3.47)  Time: 0.702s,  182.38/s  (0.654s,  195.65/s)  LR: 7.009e-04  Data: 0.032 (0.038)
Train: 18 [ 550/989 ( 56%)]  Loss: 2.94 (3.43)  Time: 0.636s,  201.25/s  (0.654s,  195.58/s)  LR: 7.009e-04  Data: 0.030 (0.038)
Train: 18 [ 600/989 ( 61%)]  Loss: 3.06 (3.40)  Time: 0.641s,  199.65/s  (0.655s,  195.46/s)  LR: 7.009e-04  Data: 0.039 (0.038)
Train: 18 [ 650/989 ( 66%)]  Loss: 3.65 (3.42)  Time: 0.681s,  187.91/s  (0.656s,  195.27/s)  LR: 7.009e-04  Data: 0.055 (0.038)
Train: 18 [ 700/989 ( 71%)]  Loss: 3.59 (3.43)  Time: 0.659s,  194.25/s  (0.656s,  195.05/s)  LR: 7.009e-04  Data: 0.028 (0.038)
Train: 18 [ 750/989 ( 76%)]  Loss: 3.85 (3.45)  Time: 0.627s,  204.00/s  (0.656s,  194.99/s)  LR: 7.009e-04  Data: 0.022 (0.038)
Train: 18 [ 800/989 ( 81%)]  Loss: 3.09 (3.43)  Time: 0.771s,  165.96/s  (0.658s,  194.65/s)  LR: 7.009e-04  Data: 0.042 (0.037)
Train: 18 [ 850/989 ( 86%)]  Loss: 3.09 (3.41)  Time: 0.693s,  184.60/s  (0.659s,  194.28/s)  LR: 7.009e-04  Data: 0.043 (0.037)
Train: 18 [ 900/989 ( 91%)]  Loss: 3.64 (3.42)  Time: 0.596s,  214.90/s  (0.660s,  193.94/s)  LR: 7.009e-04  Data: 0.016 (0.037)
Train: 18 [ 950/989 ( 96%)]  Loss: 3.57 (3.43)  Time: 0.651s,  196.54/s  (0.661s,  193.53/s)  LR: 7.009e-04  Data: 0.030 (0.037)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.233 (1.233)  Loss:   1.744 ( 1.744)  Acc@1:  58.594 ( 58.594)  Acc@5:  85.156 ( 85.156)
Test: [  39/39]  Time: 0.051 (0.318)  Loss:   0.935 ( 1.649)  Acc@1:  75.000 ( 58.640)  Acc@5: 100.000 ( 83.640)
Current checkpoints:
 ('./output/train/Upd_Exp1_Image100/checkpoint-18.pth.tar', 58.64)
 ('./output/train/Upd_Exp1_Image100/checkpoint-17.pth.tar', 56.2)
 ('./output/train/Upd_Exp1_Image100/checkpoint-16.pth.tar', 55.9)
 ('./output/train/Upd_Exp1_Image100/checkpoint-15.pth.tar', 52.66)
 ('./output/train/Upd_Exp1_Image100/checkpoint-13.pth.tar', 51.12)
 ('./output/train/Upd_Exp1_Image100/checkpoint-14.pth.tar', 51.0)
 ('./output/train/Upd_Exp1_Image100/checkpoint-12.pth.tar', 47.7)
 ('./output/train/Upd_Exp1_Image100/checkpoint-11.pth.tar', 44.44)
 ('./output/train/Upd_Exp1_Image100/checkpoint-10.pth.tar', 43.62)
 ('./output/train/Upd_Exp1_Image100/checkpoint-9.pth.tar', 40.72)

Train: 19 [   0/989 (  0%)]  Loss: 2.88 (2.88)  Time: 1.726s,   74.17/s  (1.726s,   74.17/s)  LR: 7.002e-04  Data: 1.030 (1.030)
Train: 19 [  50/989 (  5%)]  Loss: 3.83 (3.36)  Time: 0.635s,  201.42/s  (0.676s,  189.46/s)  LR: 7.002e-04  Data: 0.037 (0.059)
Train: 19 [ 100/989 ( 10%)]  Loss: 3.71 (3.48)  Time: 0.623s,  205.42/s  (0.662s,  193.26/s)  LR: 7.002e-04  Data: 0.024 (0.048)
Train: 19 [ 150/989 ( 15%)]  Loss: 3.47 (3.47)  Time: 0.633s,  202.12/s  (0.658s,  194.63/s)  LR: 7.002e-04  Data: 0.036 (0.044)
Train: 19 [ 200/989 ( 20%)]  Loss: 3.42 (3.46)  Time: 0.631s,  202.86/s  (0.657s,  194.92/s)  LR: 7.002e-04  Data: 0.032 (0.042)
Train: 19 [ 250/989 ( 25%)]  Loss: 3.00 (3.39)  Time: 0.641s,  199.72/s  (0.657s,  194.87/s)  LR: 7.002e-04  Data: 0.024 (0.041)
Train: 19 [ 300/989 ( 30%)]  Loss: 3.76 (3.44)  Time: 0.633s,  202.21/s  (0.656s,  195.13/s)  LR: 7.002e-04  Data: 0.015 (0.040)
Train: 19 [ 350/989 ( 35%)]  Loss: 3.96 (3.50)  Time: 0.634s,  201.86/s  (0.656s,  195.11/s)  LR: 7.002e-04  Data: 0.023 (0.040)
Train: 19 [ 400/989 ( 40%)]  Loss: 3.40 (3.49)  Time: 0.647s,  197.77/s  (0.656s,  195.12/s)  LR: 7.002e-04  Data: 0.038 (0.039)
Train: 19 [ 450/989 ( 46%)]  Loss: 3.46 (3.49)  Time: 0.664s,  192.69/s  (0.657s,  194.92/s)  LR: 7.002e-04  Data: 0.036 (0.039)
Train: 19 [ 500/989 ( 51%)]  Loss: 2.82 (3.43)  Time: 0.694s,  184.42/s  (0.658s,  194.63/s)  LR: 7.002e-04  Data: 0.042 (0.039)
Train: 19 [ 550/989 ( 56%)]  Loss: 3.25 (3.41)  Time: 0.581s,  220.30/s  (0.657s,  194.70/s)  LR: 7.002e-04  Data: 0.024 (0.039)
Train: 19 [ 600/989 ( 61%)]  Loss: 3.26 (3.40)  Time: 0.642s,  199.38/s  (0.657s,  194.75/s)  LR: 7.002e-04  Data: 0.028 (0.038)
Train: 19 [ 650/989 ( 66%)]  Loss: 3.46 (3.40)  Time: 0.656s,  195.25/s  (0.658s,  194.66/s)  LR: 7.002e-04  Data: 0.038 (0.038)
Train: 19 [ 700/989 ( 71%)]  Loss: 2.96 (3.37)  Time: 0.675s,  189.71/s  (0.658s,  194.59/s)  LR: 7.002e-04  Data: 0.043 (0.038)
Train: 19 [ 750/989 ( 76%)]  Loss: 3.29 (3.37)  Time: 0.651s,  196.68/s  (0.658s,  194.46/s)  LR: 7.002e-04  Data: 0.034 (0.038)
Train: 19 [ 800/989 ( 81%)]  Loss: 2.98 (3.35)  Time: 0.680s,  188.17/s  (0.659s,  194.21/s)  LR: 7.002e-04  Data: 0.037 (0.038)
Train: 19 [ 850/989 ( 86%)]  Loss: 3.41 (3.35)  Time: 0.613s,  208.92/s  (0.660s,  193.84/s)  LR: 7.002e-04  Data: 0.025 (0.038)
Train: 19 [ 900/989 ( 91%)]  Loss: 3.72 (3.37)  Time: 0.674s,  189.80/s  (0.661s,  193.56/s)  LR: 7.002e-04  Data: 0.029 (0.037)
Train: 19 [ 950/989 ( 96%)]  Loss: 3.10 (3.36)  Time: 0.684s,  187.16/s  (0.663s,  193.10/s)  LR: 7.002e-04  Data: 0.039 (0.037)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.246 (1.246)  Loss:   2.368 ( 2.368)  Acc@1:  37.500 ( 37.500)  Acc@5:  75.000 ( 75.000)
Test: [  39/39]  Time: 0.036 (0.323)  Loss:   0.910 ( 1.681)  Acc@1:  87.500 ( 58.640)  Acc@5: 100.000 ( 83.640)
Current checkpoints:
 ('./output/train/Upd_Exp1_Image100/checkpoint-18.pth.tar', 58.64)
 ('./output/train/Upd_Exp1_Image100/checkpoint-19.pth.tar', 58.64)
 ('./output/train/Upd_Exp1_Image100/checkpoint-17.pth.tar', 56.2)
 ('./output/train/Upd_Exp1_Image100/checkpoint-16.pth.tar', 55.9)
 ('./output/train/Upd_Exp1_Image100/checkpoint-15.pth.tar', 52.66)
 ('./output/train/Upd_Exp1_Image100/checkpoint-13.pth.tar', 51.12)
 ('./output/train/Upd_Exp1_Image100/checkpoint-14.pth.tar', 51.0)
 ('./output/train/Upd_Exp1_Image100/checkpoint-12.pth.tar', 47.7)
 ('./output/train/Upd_Exp1_Image100/checkpoint-11.pth.tar', 44.44)
 ('./output/train/Upd_Exp1_Image100/checkpoint-10.pth.tar', 43.62)

Train: 20 [   0/989 (  0%)]  Loss: 3.90 (3.90)  Time: 1.834s,   69.80/s  (1.834s,   69.80/s)  LR: 6.995e-04  Data: 1.043 (1.043)
Train: 20 [  50/989 (  5%)]  Loss: 3.15 (3.53)  Time: 0.633s,  202.32/s  (0.684s,  187.14/s)  LR: 6.995e-04  Data: 0.037 (0.054)
Train: 20 [ 100/989 ( 10%)]  Loss: 3.41 (3.49)  Time: 0.617s,  207.52/s  (0.671s,  190.80/s)  LR: 6.995e-04  Data: 0.029 (0.044)
Train: 20 [ 150/989 ( 15%)]  Loss: 2.95 (3.35)  Time: 0.635s,  201.60/s  (0.666s,  192.09/s)  LR: 6.995e-04  Data: 0.021 (0.042)
Train: 20 [ 200/989 ( 20%)]  Loss: 3.48 (3.38)  Time: 0.661s,  193.58/s  (0.664s,  192.88/s)  LR: 6.995e-04  Data: 0.039 (0.040)
Train: 20 [ 250/989 ( 25%)]  Loss: 3.07 (3.33)  Time: 0.639s,  200.30/s  (0.662s,  193.43/s)  LR: 6.995e-04  Data: 0.027 (0.039)
Train: 20 [ 300/989 ( 30%)]  Loss: 3.49 (3.35)  Time: 0.630s,  203.33/s  (0.661s,  193.77/s)  LR: 6.995e-04  Data: 0.024 (0.038)
Train: 20 [ 350/989 ( 35%)]  Loss: 3.28 (3.34)  Time: 0.649s,  197.26/s  (0.660s,  194.05/s)  LR: 6.995e-04  Data: 0.048 (0.038)
Train: 20 [ 400/989 ( 40%)]  Loss: 3.27 (3.33)  Time: 0.633s,  202.23/s  (0.659s,  194.13/s)  LR: 6.995e-04  Data: 0.031 (0.038)
Train: 20 [ 450/989 ( 46%)]  Loss: 3.00 (3.30)  Time: 0.652s,  196.20/s  (0.659s,  194.32/s)  LR: 6.995e-04  Data: 0.036 (0.038)
Train: 20 [ 500/989 ( 51%)]  Loss: 3.73 (3.34)  Time: 0.644s,  198.78/s  (0.659s,  194.35/s)  LR: 6.995e-04  Data: 0.031 (0.038)
Train: 20 [ 550/989 ( 56%)]  Loss: 3.69 (3.37)  Time: 0.636s,  201.11/s  (0.658s,  194.40/s)  LR: 6.995e-04  Data: 0.028 (0.037)
Train: 20 [ 600/989 ( 61%)]  Loss: 3.45 (3.38)  Time: 0.629s,  203.58/s  (0.658s,  194.45/s)  LR: 6.995e-04  Data: 0.037 (0.037)
Train: 20 [ 650/989 ( 66%)]  Loss: 3.31 (3.37)  Time: 0.635s,  201.63/s  (0.659s,  194.33/s)  LR: 6.995e-04  Data: 0.031 (0.037)
Train: 20 [ 700/989 ( 71%)]  Loss: 3.39 (3.37)  Time: 0.648s,  197.39/s  (0.659s,  194.15/s)  LR: 6.995e-04  Data: 0.033 (0.038)
Train: 20 [ 750/989 ( 76%)]  Loss: 3.68 (3.39)  Time: 0.687s,  186.35/s  (0.660s,  194.04/s)  LR: 6.995e-04  Data: 0.059 (0.037)
Train: 20 [ 800/989 ( 81%)]  Loss: 2.66 (3.35)  Time: 0.664s,  192.65/s  (0.660s,  193.91/s)  LR: 6.995e-04  Data: 0.040 (0.037)
Train: 20 [ 850/989 ( 86%)]  Loss: 3.26 (3.34)  Time: 0.667s,  191.88/s  (0.661s,  193.69/s)  LR: 6.995e-04  Data: 0.034 (0.037)
Train: 20 [ 900/989 ( 91%)]  Loss: 3.20 (3.34)  Time: 0.667s,  191.93/s  (0.662s,  193.39/s)  LR: 6.995e-04  Data: 0.036 (0.037)
Train: 20 [ 950/989 ( 96%)]  Loss: 3.02 (3.32)  Time: 0.709s,  180.56/s  (0.663s,  192.99/s)  LR: 6.995e-04  Data: 0.044 (0.037)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.242 (1.242)  Loss:   2.166 ( 2.166)  Acc@1:  45.312 ( 45.312)  Acc@5:  75.781 ( 75.781)
Test: [  39/39]  Time: 0.047 (0.321)  Loss:   1.157 ( 1.626)  Acc@1:  62.500 ( 60.160)  Acc@5: 100.000 ( 85.160)
Current checkpoints:
 ('./output/train/Upd_Exp1_Image100/checkpoint-20.pth.tar', 60.16)
 ('./output/train/Upd_Exp1_Image100/checkpoint-18.pth.tar', 58.64)
 ('./output/train/Upd_Exp1_Image100/checkpoint-19.pth.tar', 58.64)
 ('./output/train/Upd_Exp1_Image100/checkpoint-17.pth.tar', 56.2)
 ('./output/train/Upd_Exp1_Image100/checkpoint-16.pth.tar', 55.9)
 ('./output/train/Upd_Exp1_Image100/checkpoint-15.pth.tar', 52.66)
 ('./output/train/Upd_Exp1_Image100/checkpoint-13.pth.tar', 51.12)
 ('./output/train/Upd_Exp1_Image100/checkpoint-14.pth.tar', 51.0)
 ('./output/train/Upd_Exp1_Image100/checkpoint-12.pth.tar', 47.7)
 ('./output/train/Upd_Exp1_Image100/checkpoint-11.pth.tar', 44.44)

Train: 21 [   0/989 (  0%)]  Loss: 3.93 (3.93)  Time: 1.867s,   68.56/s  (1.867s,   68.56/s)  LR: 6.987e-04  Data: 1.094 (1.094)
Train: 21 [  50/989 (  5%)]  Loss: 3.32 (3.62)  Time: 0.652s,  196.30/s  (0.681s,  187.90/s)  LR: 6.987e-04  Data: 0.053 (0.056)
Train: 21 [ 100/989 ( 10%)]  Loss: 3.47 (3.57)  Time: 0.649s,  197.34/s  (0.665s,  192.47/s)  LR: 6.987e-04  Data: 0.035 (0.045)
Train: 21 [ 150/989 ( 15%)]  Loss: 3.54 (3.57)  Time: 0.632s,  202.56/s  (0.659s,  194.23/s)  LR: 6.987e-04  Data: 0.023 (0.042)
Train: 21 [ 200/989 ( 20%)]  Loss: 3.25 (3.50)  Time: 0.679s,  188.55/s  (0.657s,  194.85/s)  LR: 6.987e-04  Data: 0.032 (0.040)
Train: 21 [ 250/989 ( 25%)]  Loss: 3.76 (3.55)  Time: 0.622s,  205.80/s  (0.656s,  195.24/s)  LR: 6.987e-04  Data: 0.029 (0.040)
Train: 21 [ 300/989 ( 30%)]  Loss: 3.58 (3.55)  Time: 0.653s,  196.14/s  (0.656s,  195.15/s)  LR: 6.987e-04  Data: 0.045 (0.039)
Train: 21 [ 350/989 ( 35%)]  Loss: 3.31 (3.52)  Time: 0.657s,  194.82/s  (0.655s,  195.29/s)  LR: 6.987e-04  Data: 0.036 (0.039)
Train: 21 [ 400/989 ( 40%)]  Loss: 3.68 (3.54)  Time: 0.666s,  192.31/s  (0.655s,  195.41/s)  LR: 6.987e-04  Data: 0.054 (0.038)
Train: 21 [ 450/989 ( 46%)]  Loss: 3.43 (3.53)  Time: 0.650s,  196.93/s  (0.655s,  195.31/s)  LR: 6.987e-04  Data: 0.027 (0.038)
Train: 21 [ 500/989 ( 51%)]  Loss: 3.41 (3.52)  Time: 0.633s,  202.23/s  (0.655s,  195.32/s)  LR: 6.987e-04  Data: 0.033 (0.038)
Train: 21 [ 550/989 ( 56%)]  Loss: 3.28 (3.50)  Time: 0.633s,  202.22/s  (0.655s,  195.30/s)  LR: 6.987e-04  Data: 0.032 (0.038)
Train: 21 [ 600/989 ( 61%)]  Loss: 2.74 (3.44)  Time: 0.650s,  196.99/s  (0.656s,  195.19/s)  LR: 6.987e-04  Data: 0.040 (0.038)
Train: 21 [ 650/989 ( 66%)]  Loss: 3.70 (3.46)  Time: 0.687s,  186.27/s  (0.656s,  195.05/s)  LR: 6.987e-04  Data: 0.055 (0.037)
Train: 21 [ 700/989 ( 71%)]  Loss: 2.95 (3.42)  Time: 0.615s,  208.13/s  (0.656s,  195.07/s)  LR: 6.987e-04  Data: 0.041 (0.037)
Train: 21 [ 750/989 ( 76%)]  Loss: 3.43 (3.42)  Time: 0.641s,  199.63/s  (0.657s,  194.93/s)  LR: 6.987e-04  Data: 0.028 (0.037)
Train: 21 [ 800/989 ( 81%)]  Loss: 3.25 (3.41)  Time: 0.669s,  191.32/s  (0.657s,  194.69/s)  LR: 6.987e-04  Data: 0.044 (0.037)
Train: 21 [ 850/989 ( 86%)]  Loss: 2.84 (3.38)  Time: 0.634s,  201.75/s  (0.659s,  194.27/s)  LR: 6.987e-04  Data: 0.027 (0.037)
Train: 21 [ 900/989 ( 91%)]  Loss: 3.09 (3.37)  Time: 0.676s,  189.36/s  (0.660s,  193.88/s)  LR: 6.987e-04  Data: 0.038 (0.037)
Train: 21 [ 950/989 ( 96%)]  Loss: 3.20 (3.36)  Time: 0.700s,  182.85/s  (0.661s,  193.59/s)  LR: 6.987e-04  Data: 0.037 (0.037)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.247 (1.247)  Loss:   1.777 ( 1.777)  Acc@1:  57.812 ( 57.812)  Acc@5:  81.250 ( 81.250)
Test: [  39/39]  Time: 0.050 (0.324)  Loss:   1.575 ( 1.641)  Acc@1:  62.500 ( 59.900)  Acc@5: 100.000 ( 85.060)
Current checkpoints:
 ('./output/train/Upd_Exp1_Image100/checkpoint-20.pth.tar', 60.16)
 ('./output/train/Upd_Exp1_Image100/checkpoint-21.pth.tar', 59.9)
 ('./output/train/Upd_Exp1_Image100/checkpoint-18.pth.tar', 58.64)
 ('./output/train/Upd_Exp1_Image100/checkpoint-19.pth.tar', 58.64)
 ('./output/train/Upd_Exp1_Image100/checkpoint-17.pth.tar', 56.2)
 ('./output/train/Upd_Exp1_Image100/checkpoint-16.pth.tar', 55.9)
 ('./output/train/Upd_Exp1_Image100/checkpoint-15.pth.tar', 52.66)
 ('./output/train/Upd_Exp1_Image100/checkpoint-13.pth.tar', 51.12)
 ('./output/train/Upd_Exp1_Image100/checkpoint-14.pth.tar', 51.0)
 ('./output/train/Upd_Exp1_Image100/checkpoint-12.pth.tar', 47.7)

Train: 22 [   0/989 (  0%)]  Loss: 3.58 (3.58)  Time: 2.081s,   61.52/s  (2.081s,   61.52/s)  LR: 6.979e-04  Data: 1.287 (1.287)
Train: 22 [  50/989 (  5%)]  Loss: 2.65 (3.12)  Time: 0.639s,  200.37/s  (0.688s,  186.01/s)  LR: 6.979e-04  Data: 0.044 (0.061)
Train: 22 [ 100/989 ( 10%)]  Loss: 3.64 (3.29)  Time: 0.696s,  184.03/s  (0.671s,  190.64/s)  LR: 6.979e-04  Data: 0.053 (0.050)
Train: 22 [ 150/989 ( 15%)]  Loss: 3.08 (3.24)  Time: 0.645s,  198.45/s  (0.667s,  191.89/s)  LR: 6.979e-04  Data: 0.033 (0.045)
Train: 22 [ 200/989 ( 20%)]  Loss: 3.77 (3.34)  Time: 0.638s,  200.57/s  (0.665s,  192.46/s)  LR: 6.979e-04  Data: 0.027 (0.042)
Train: 22 [ 250/989 ( 25%)]  Loss: 3.45 (3.36)  Time: 0.710s,  180.22/s  (0.663s,  193.09/s)  LR: 6.979e-04  Data: 0.036 (0.041)
Train: 22 [ 300/989 ( 30%)]  Loss: 3.56 (3.39)  Time: 0.649s,  197.10/s  (0.661s,  193.51/s)  LR: 6.979e-04  Data: 0.047 (0.040)
Train: 22 [ 350/989 ( 35%)]  Loss: 3.57 (3.41)  Time: 0.624s,  205.00/s  (0.660s,  193.98/s)  LR: 6.979e-04  Data: 0.024 (0.039)
Train: 22 [ 400/989 ( 40%)]  Loss: 3.17 (3.39)  Time: 0.666s,  192.30/s  (0.659s,  194.19/s)  LR: 6.979e-04  Data: 0.047 (0.038)
Train: 22 [ 450/989 ( 46%)]  Loss: 3.16 (3.36)  Time: 0.630s,  203.11/s  (0.659s,  194.19/s)  LR: 6.979e-04  Data: 0.026 (0.038)
Train: 22 [ 500/989 ( 51%)]  Loss: 3.53 (3.38)  Time: 0.637s,  201.03/s  (0.659s,  194.34/s)  LR: 6.979e-04  Data: 0.033 (0.038)
Train: 22 [ 550/989 ( 56%)]  Loss: 3.92 (3.42)  Time: 0.603s,  212.21/s  (0.658s,  194.42/s)  LR: 6.979e-04  Data: 0.035 (0.037)
Train: 22 [ 600/989 ( 61%)]  Loss: 3.77 (3.45)  Time: 0.706s,  181.19/s  (0.659s,  194.30/s)  LR: 6.979e-04  Data: 0.041 (0.038)
Train: 22 [ 650/989 ( 66%)]  Loss: 3.51 (3.45)  Time: 0.640s,  200.15/s  (0.659s,  194.29/s)  LR: 6.979e-04  Data: 0.034 (0.037)
Train: 22 [ 700/989 ( 71%)]  Loss: 3.79 (3.48)  Time: 0.660s,  194.02/s  (0.659s,  194.20/s)  LR: 6.979e-04  Data: 0.026 (0.037)
Train: 22 [ 750/989 ( 76%)]  Loss: 3.27 (3.46)  Time: 0.662s,  193.27/s  (0.659s,  194.19/s)  LR: 6.979e-04  Data: 0.041 (0.037)
Train: 22 [ 800/989 ( 81%)]  Loss: 2.99 (3.44)  Time: 0.645s,  198.38/s  (0.659s,  194.09/s)  LR: 6.979e-04  Data: 0.029 (0.037)
Train: 22 [ 850/989 ( 86%)]  Loss: 3.53 (3.44)  Time: 0.622s,  205.89/s  (0.660s,  193.84/s)  LR: 6.979e-04  Data: 0.011 (0.037)
Train: 22 [ 900/989 ( 91%)]  Loss: 3.34 (3.44)  Time: 0.647s,  197.81/s  (0.661s,  193.56/s)  LR: 6.979e-04  Data: 0.033 (0.036)
Train: 22 [ 950/989 ( 96%)]  Loss: 3.54 (3.44)  Time: 0.711s,  180.12/s  (0.663s,  193.18/s)  LR: 6.979e-04  Data: 0.029 (0.036)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.249 (1.249)  Loss:   1.543 ( 1.543)  Acc@1:  69.531 ( 69.531)  Acc@5:  90.625 ( 90.625)
Test: [  39/39]  Time: 0.050 (0.321)  Loss:   1.925 ( 1.608)  Acc@1:  37.500 ( 61.120)  Acc@5:  87.500 ( 86.020)
Current checkpoints:
 ('./output/train/Upd_Exp1_Image100/checkpoint-22.pth.tar', 61.12)
 ('./output/train/Upd_Exp1_Image100/checkpoint-20.pth.tar', 60.16)
 ('./output/train/Upd_Exp1_Image100/checkpoint-21.pth.tar', 59.9)
 ('./output/train/Upd_Exp1_Image100/checkpoint-18.pth.tar', 58.64)
 ('./output/train/Upd_Exp1_Image100/checkpoint-19.pth.tar', 58.64)
 ('./output/train/Upd_Exp1_Image100/checkpoint-17.pth.tar', 56.2)
 ('./output/train/Upd_Exp1_Image100/checkpoint-16.pth.tar', 55.9)
 ('./output/train/Upd_Exp1_Image100/checkpoint-15.pth.tar', 52.66)
 ('./output/train/Upd_Exp1_Image100/checkpoint-13.pth.tar', 51.12)
 ('./output/train/Upd_Exp1_Image100/checkpoint-14.pth.tar', 51.0)

Train: 23 [   0/989 (  0%)]  Loss: 3.58 (3.58)  Time: 1.609s,   79.55/s  (1.609s,   79.55/s)  LR: 6.970e-04  Data: 0.895 (0.895)
Train: 23 [  50/989 (  5%)]  Loss: 3.45 (3.51)  Time: 0.631s,  202.78/s  (0.678s,  188.69/s)  LR: 6.970e-04  Data: 0.030 (0.053)
Train: 23 [ 100/989 ( 10%)]  Loss: 3.14 (3.39)  Time: 0.625s,  204.88/s  (0.664s,  192.63/s)  LR: 6.970e-04  Data: 0.026 (0.046)
Train: 23 [ 150/989 ( 15%)]  Loss: 3.75 (3.48)  Time: 0.629s,  203.62/s  (0.659s,  194.31/s)  LR: 6.970e-04  Data: 0.028 (0.042)
Train: 23 [ 200/989 ( 20%)]  Loss: 3.17 (3.42)  Time: 0.645s,  198.53/s  (0.657s,  194.95/s)  LR: 6.970e-04  Data: 0.042 (0.040)
Train: 23 [ 250/989 ( 25%)]  Loss: 3.72 (3.47)  Time: 0.591s,  216.53/s  (0.656s,  195.19/s)  LR: 6.970e-04  Data: 0.020 (0.040)
Train: 23 [ 300/989 ( 30%)]  Loss: 2.56 (3.34)  Time: 0.681s,  187.93/s  (0.655s,  195.51/s)  LR: 6.970e-04  Data: 0.058 (0.039)
Train: 23 [ 350/989 ( 35%)]  Loss: 3.37 (3.34)  Time: 0.636s,  201.32/s  (0.654s,  195.62/s)  LR: 6.970e-04  Data: 0.032 (0.038)
Train: 23 [ 400/989 ( 40%)]  Loss: 2.77 (3.28)  Time: 0.700s,  182.79/s  (0.655s,  195.44/s)  LR: 6.970e-04  Data: 0.046 (0.039)
Train: 23 [ 450/989 ( 46%)]  Loss: 2.91 (3.24)  Time: 0.607s,  211.02/s  (0.655s,  195.55/s)  LR: 6.970e-04  Data: 0.037 (0.038)
Train: 23 [ 500/989 ( 51%)]  Loss: 3.58 (3.27)  Time: 0.629s,  203.41/s  (0.655s,  195.52/s)  LR: 6.970e-04  Data: 0.039 (0.038)
Train: 23 [ 550/989 ( 56%)]  Loss: 2.94 (3.24)  Time: 0.695s,  184.26/s  (0.655s,  195.44/s)  LR: 6.970e-04  Data: 0.037 (0.038)
Train: 23 [ 600/989 ( 61%)]  Loss: 3.39 (3.26)  Time: 0.639s,  200.40/s  (0.655s,  195.45/s)  LR: 6.970e-04  Data: 0.026 (0.037)
Train: 23 [ 650/989 ( 66%)]  Loss: 3.26 (3.26)  Time: 0.627s,  204.22/s  (0.655s,  195.54/s)  LR: 6.970e-04  Data: 0.022 (0.037)
Train: 23 [ 700/989 ( 71%)]  Loss: 3.83 (3.29)  Time: 0.659s,  194.15/s  (0.655s,  195.44/s)  LR: 6.970e-04  Data: 0.044 (0.037)
Train: 23 [ 750/989 ( 76%)]  Loss: 3.80 (3.33)  Time: 0.673s,  190.20/s  (0.656s,  195.18/s)  LR: 6.970e-04  Data: 0.029 (0.037)
Train: 23 [ 800/989 ( 81%)]  Loss: 3.46 (3.33)  Time: 0.624s,  205.05/s  (0.656s,  195.01/s)  LR: 6.970e-04  Data: 0.022 (0.037)
Train: 23 [ 850/989 ( 86%)]  Loss: 2.91 (3.31)  Time: 0.658s,  194.54/s  (0.658s,  194.49/s)  LR: 6.970e-04  Data: 0.029 (0.037)
Train: 23 [ 900/989 ( 91%)]  Loss: 3.19 (3.30)  Time: 0.685s,  186.91/s  (0.659s,  194.11/s)  LR: 6.970e-04  Data: 0.026 (0.036)
Train: 23 [ 950/989 ( 96%)]  Loss: 3.40 (3.31)  Time: 0.698s,  183.31/s  (0.660s,  193.82/s)  LR: 6.970e-04  Data: 0.029 (0.036)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.240 (1.240)  Loss:   1.545 ( 1.545)  Acc@1:  62.500 ( 62.500)  Acc@5:  87.500 ( 87.500)
Test: [  39/39]  Time: 0.048 (0.320)  Loss:   0.722 ( 1.575)  Acc@1:  87.500 ( 61.420)  Acc@5: 100.000 ( 86.320)
Current checkpoints:
 ('./output/train/Upd_Exp1_Image100/checkpoint-23.pth.tar', 61.42)
 ('./output/train/Upd_Exp1_Image100/checkpoint-22.pth.tar', 61.12)
 ('./output/train/Upd_Exp1_Image100/checkpoint-20.pth.tar', 60.16)
 ('./output/train/Upd_Exp1_Image100/checkpoint-21.pth.tar', 59.9)
 ('./output/train/Upd_Exp1_Image100/checkpoint-18.pth.tar', 58.64)
 ('./output/train/Upd_Exp1_Image100/checkpoint-19.pth.tar', 58.64)
 ('./output/train/Upd_Exp1_Image100/checkpoint-17.pth.tar', 56.2)
 ('./output/train/Upd_Exp1_Image100/checkpoint-16.pth.tar', 55.9)
 ('./output/train/Upd_Exp1_Image100/checkpoint-15.pth.tar', 52.66)
 ('./output/train/Upd_Exp1_Image100/checkpoint-13.pth.tar', 51.12)

Train: 24 [   0/989 (  0%)]  Loss: 3.28 (3.28)  Time: 1.818s,   70.40/s  (1.818s,   70.40/s)  LR: 6.962e-04  Data: 1.007 (1.007)
Train: 24 [  50/989 (  5%)]  Loss: 3.75 (3.52)  Time: 0.641s,  199.60/s  (0.678s,  188.66/s)  LR: 6.962e-04  Data: 0.029 (0.055)
Train: 24 [ 100/989 ( 10%)]  Loss: 3.03 (3.36)  Time: 0.666s,  192.32/s  (0.664s,  192.85/s)  LR: 6.962e-04  Data: 0.054 (0.046)
Train: 24 [ 150/989 ( 15%)]  Loss: 3.83 (3.47)  Time: 0.632s,  202.61/s  (0.658s,  194.45/s)  LR: 6.962e-04  Data: 0.035 (0.043)
Train: 24 [ 200/989 ( 20%)]  Loss: 2.80 (3.34)  Time: 0.669s,  191.33/s  (0.656s,  195.14/s)  LR: 6.962e-04  Data: 0.048 (0.041)
Train: 24 [ 250/989 ( 25%)]  Loss: 3.24 (3.32)  Time: 0.654s,  195.82/s  (0.654s,  195.69/s)  LR: 6.962e-04  Data: 0.033 (0.040)
Train: 24 [ 300/989 ( 30%)]  Loss: 3.89 (3.40)  Time: 0.652s,  196.29/s  (0.654s,  195.69/s)  LR: 6.962e-04  Data: 0.045 (0.039)
Train: 24 [ 350/989 ( 35%)]  Loss: 3.22 (3.38)  Time: 0.674s,  189.92/s  (0.654s,  195.66/s)  LR: 6.962e-04  Data: 0.046 (0.039)
Train: 24 [ 400/989 ( 40%)]  Loss: 2.99 (3.34)  Time: 0.708s,  180.90/s  (0.655s,  195.57/s)  LR: 6.962e-04  Data: 0.046 (0.039)
Train: 24 [ 450/989 ( 46%)]  Loss: 3.25 (3.33)  Time: 0.677s,  188.98/s  (0.654s,  195.66/s)  LR: 6.962e-04  Data: 0.040 (0.039)
Train: 24 [ 500/989 ( 51%)]  Loss: 2.79 (3.28)  Time: 0.627s,  203.98/s  (0.654s,  195.59/s)  LR: 6.962e-04  Data: 0.022 (0.038)
Train: 24 [ 550/989 ( 56%)]  Loss: 3.13 (3.27)  Time: 0.635s,  201.68/s  (0.654s,  195.69/s)  LR: 6.962e-04  Data: 0.034 (0.038)
Train: 24 [ 600/989 ( 61%)]  Loss: 3.07 (3.25)  Time: 0.675s,  189.76/s  (0.655s,  195.55/s)  LR: 6.962e-04  Data: 0.054 (0.038)
Train: 24 [ 650/989 ( 66%)]  Loss: 3.42 (3.26)  Time: 0.663s,  192.96/s  (0.655s,  195.46/s)  LR: 6.962e-04  Data: 0.044 (0.038)
Train: 24 [ 700/989 ( 71%)]  Loss: 3.19 (3.26)  Time: 0.665s,  192.58/s  (0.655s,  195.35/s)  LR: 6.962e-04  Data: 0.047 (0.038)
Train: 24 [ 750/989 ( 76%)]  Loss: 3.26 (3.26)  Time: 0.635s,  201.50/s  (0.656s,  195.21/s)  LR: 6.962e-04  Data: 0.014 (0.037)
Train: 24 [ 800/989 ( 81%)]  Loss: 3.23 (3.26)  Time: 0.632s,  202.58/s  (0.657s,  194.87/s)  LR: 6.962e-04  Data: 0.025 (0.037)
Train: 24 [ 850/989 ( 86%)]  Loss: 3.26 (3.26)  Time: 0.660s,  194.06/s  (0.658s,  194.57/s)  LR: 6.962e-04  Data: 0.026 (0.037)
Train: 24 [ 900/989 ( 91%)]  Loss: 3.45 (3.27)  Time: 0.603s,  212.23/s  (0.659s,  194.31/s)  LR: 6.962e-04  Data: 0.012 (0.037)
Train: 24 [ 950/989 ( 96%)]  Loss: 3.35 (3.27)  Time: 0.606s,  211.28/s  (0.660s,  193.96/s)  LR: 6.962e-04  Data: 0.022 (0.037)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.250 (1.250)  Loss:   1.601 ( 1.601)  Acc@1:  62.500 ( 62.500)  Acc@5:  86.719 ( 86.719)
Test: [  39/39]  Time: 0.051 (0.321)  Loss:   1.261 ( 1.485)  Acc@1:  62.500 ( 62.900)  Acc@5:  87.500 ( 86.520)
Current checkpoints:
 ('./output/train/Upd_Exp1_Image100/checkpoint-24.pth.tar', 62.9)
 ('./output/train/Upd_Exp1_Image100/checkpoint-23.pth.tar', 61.42)
 ('./output/train/Upd_Exp1_Image100/checkpoint-22.pth.tar', 61.12)
 ('./output/train/Upd_Exp1_Image100/checkpoint-20.pth.tar', 60.16)
 ('./output/train/Upd_Exp1_Image100/checkpoint-21.pth.tar', 59.9)
 ('./output/train/Upd_Exp1_Image100/checkpoint-18.pth.tar', 58.64)
 ('./output/train/Upd_Exp1_Image100/checkpoint-19.pth.tar', 58.64)
 ('./output/train/Upd_Exp1_Image100/checkpoint-17.pth.tar', 56.2)
 ('./output/train/Upd_Exp1_Image100/checkpoint-16.pth.tar', 55.9)
 ('./output/train/Upd_Exp1_Image100/checkpoint-15.pth.tar', 52.66)

Train: 25 [   0/989 (  0%)]  Loss: 3.37 (3.37)  Time: 1.900s,   67.35/s  (1.900s,   67.35/s)  LR: 6.952e-04  Data: 1.165 (1.165)
Train: 25 [  50/989 (  5%)]  Loss: 3.28 (3.33)  Time: 0.639s,  200.43/s  (0.682s,  187.59/s)  LR: 6.952e-04  Data: 0.026 (0.059)
Train: 25 [ 100/989 ( 10%)]  Loss: 3.51 (3.39)  Time: 0.635s,  201.70/s  (0.668s,  191.54/s)  LR: 6.952e-04  Data: 0.034 (0.048)
Train: 25 [ 150/989 ( 15%)]  Loss: 2.84 (3.25)  Time: 0.656s,  194.98/s  (0.663s,  193.14/s)  LR: 6.952e-04  Data: 0.026 (0.043)
Train: 25 [ 200/989 ( 20%)]  Loss: 3.28 (3.26)  Time: 0.665s,  192.47/s  (0.659s,  194.18/s)  LR: 6.952e-04  Data: 0.042 (0.041)
Train: 25 [ 250/989 ( 25%)]  Loss: 3.76 (3.34)  Time: 0.714s,  179.28/s  (0.658s,  194.43/s)  LR: 6.952e-04  Data: 0.060 (0.041)
Train: 25 [ 300/989 ( 30%)]  Loss: 3.55 (3.37)  Time: 0.679s,  188.45/s  (0.657s,  194.94/s)  LR: 6.952e-04  Data: 0.058 (0.040)
Train: 25 [ 350/989 ( 35%)]  Loss: 3.64 (3.41)  Time: 0.728s,  175.92/s  (0.656s,  195.18/s)  LR: 6.952e-04  Data: 0.031 (0.039)
Train: 25 [ 400/989 ( 40%)]  Loss: 3.23 (3.39)  Time: 0.653s,  195.97/s  (0.655s,  195.40/s)  LR: 6.952e-04  Data: 0.033 (0.039)
Train: 25 [ 450/989 ( 46%)]  Loss: 3.76 (3.42)  Time: 0.669s,  191.33/s  (0.655s,  195.48/s)  LR: 6.952e-04  Data: 0.052 (0.038)
Train: 25 [ 500/989 ( 51%)]  Loss: 3.57 (3.44)  Time: 0.655s,  195.39/s  (0.655s,  195.42/s)  LR: 6.952e-04  Data: 0.036 (0.038)
Train: 25 [ 550/989 ( 56%)]  Loss: 3.52 (3.44)  Time: 0.638s,  200.74/s  (0.655s,  195.39/s)  LR: 6.952e-04  Data: 0.044 (0.038)
Train: 25 [ 600/989 ( 61%)]  Loss: 2.98 (3.41)  Time: 0.651s,  196.73/s  (0.656s,  195.24/s)  LR: 6.952e-04  Data: 0.035 (0.038)
Train: 25 [ 650/989 ( 66%)]  Loss: 2.89 (3.37)  Time: 0.651s,  196.62/s  (0.656s,  195.21/s)  LR: 6.952e-04  Data: 0.045 (0.038)
Train: 25 [ 700/989 ( 71%)]  Loss: 3.65 (3.39)  Time: 0.636s,  201.15/s  (0.657s,  194.97/s)  LR: 6.952e-04  Data: 0.027 (0.038)
Train: 25 [ 750/989 ( 76%)]  Loss: 3.71 (3.41)  Time: 0.695s,  184.07/s  (0.657s,  194.92/s)  LR: 6.952e-04  Data: 0.039 (0.038)
Train: 25 [ 800/989 ( 81%)]  Loss: 3.30 (3.40)  Time: 0.637s,  201.03/s  (0.657s,  194.82/s)  LR: 6.952e-04  Data: 0.015 (0.038)
Train: 25 [ 850/989 ( 86%)]  Loss: 3.92 (3.43)  Time: 0.678s,  188.82/s  (0.658s,  194.48/s)  LR: 6.952e-04  Data: 0.034 (0.037)
Train: 25 [ 900/989 ( 91%)]  Loss: 3.39 (3.43)  Time: 0.666s,  192.12/s  (0.660s,  194.08/s)  LR: 6.952e-04  Data: 0.034 (0.037)
Train: 25 [ 950/989 ( 96%)]  Loss: 3.32 (3.42)  Time: 0.647s,  197.95/s  (0.661s,  193.72/s)  LR: 6.952e-04  Data: 0.028 (0.037)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.272 (1.272)  Loss:   1.546 ( 1.546)  Acc@1:  64.062 ( 64.062)  Acc@5:  87.500 ( 87.500)
Test: [  39/39]  Time: 0.049 (0.321)  Loss:   0.676 ( 1.496)  Acc@1:  87.500 ( 63.300)  Acc@5: 100.000 ( 86.760)
Current checkpoints:
 ('./output/train/Upd_Exp1_Image100/checkpoint-25.pth.tar', 63.3)
 ('./output/train/Upd_Exp1_Image100/checkpoint-24.pth.tar', 62.9)
 ('./output/train/Upd_Exp1_Image100/checkpoint-23.pth.tar', 61.42)
 ('./output/train/Upd_Exp1_Image100/checkpoint-22.pth.tar', 61.12)
 ('./output/train/Upd_Exp1_Image100/checkpoint-20.pth.tar', 60.16)
 ('./output/train/Upd_Exp1_Image100/checkpoint-21.pth.tar', 59.9)
 ('./output/train/Upd_Exp1_Image100/checkpoint-18.pth.tar', 58.64)
 ('./output/train/Upd_Exp1_Image100/checkpoint-19.pth.tar', 58.64)
 ('./output/train/Upd_Exp1_Image100/checkpoint-17.pth.tar', 56.2)
 ('./output/train/Upd_Exp1_Image100/checkpoint-16.pth.tar', 55.9)

Train: 26 [   0/989 (  0%)]  Loss: 3.89 (3.89)  Time: 2.019s,   63.40/s  (2.019s,   63.40/s)  LR: 6.943e-04  Data: 1.199 (1.199)
Train: 26 [  50/989 (  5%)]  Loss: 2.90 (3.39)  Time: 0.659s,  194.21/s  (0.681s,  188.04/s)  LR: 6.943e-04  Data: 0.028 (0.056)
Train: 26 [ 100/989 ( 10%)]  Loss: 3.70 (3.50)  Time: 0.656s,  195.07/s  (0.663s,  192.96/s)  LR: 6.943e-04  Data: 0.043 (0.047)
Train: 26 [ 150/989 ( 15%)]  Loss: 2.87 (3.34)  Time: 0.651s,  196.74/s  (0.659s,  194.14/s)  LR: 6.943e-04  Data: 0.030 (0.043)
Train: 26 [ 200/989 ( 20%)]  Loss: 3.68 (3.41)  Time: 0.642s,  199.34/s  (0.657s,  194.94/s)  LR: 6.943e-04  Data: 0.049 (0.041)
Train: 26 [ 250/989 ( 25%)]  Loss: 3.33 (3.40)  Time: 0.637s,  201.08/s  (0.655s,  195.47/s)  LR: 6.943e-04  Data: 0.049 (0.039)
Train: 26 [ 300/989 ( 30%)]  Loss: 3.45 (3.40)  Time: 0.639s,  200.18/s  (0.654s,  195.66/s)  LR: 6.943e-04  Data: 0.032 (0.039)
Train: 26 [ 350/989 ( 35%)]  Loss: 3.29 (3.39)  Time: 0.641s,  199.64/s  (0.654s,  195.57/s)  LR: 6.943e-04  Data: 0.031 (0.039)
Train: 26 [ 400/989 ( 40%)]  Loss: 3.24 (3.37)  Time: 0.667s,  191.99/s  (0.655s,  195.51/s)  LR: 6.943e-04  Data: 0.050 (0.040)
Train: 26 [ 450/989 ( 46%)]  Loss: 3.15 (3.35)  Time: 0.623s,  205.44/s  (0.655s,  195.51/s)  LR: 6.943e-04  Data: 0.033 (0.039)
Train: 26 [ 500/989 ( 51%)]  Loss: 2.67 (3.29)  Time: 0.632s,  202.68/s  (0.655s,  195.46/s)  LR: 6.943e-04  Data: 0.027 (0.039)
Train: 26 [ 550/989 ( 56%)]  Loss: 2.83 (3.25)  Time: 0.651s,  196.52/s  (0.655s,  195.29/s)  LR: 6.943e-04  Data: 0.046 (0.039)
Train: 26 [ 600/989 ( 61%)]  Loss: 3.70 (3.28)  Time: 0.665s,  192.36/s  (0.656s,  195.09/s)  LR: 6.943e-04  Data: 0.054 (0.039)
Train: 26 [ 650/989 ( 66%)]  Loss: 3.11 (3.27)  Time: 0.667s,  192.00/s  (0.656s,  195.00/s)  LR: 6.943e-04  Data: 0.047 (0.038)
Train: 26 [ 700/989 ( 71%)]  Loss: 3.58 (3.29)  Time: 0.635s,  201.50/s  (0.657s,  194.86/s)  LR: 6.943e-04  Data: 0.032 (0.038)
Train: 26 [ 750/989 ( 76%)]  Loss: 3.55 (3.31)  Time: 0.653s,  196.14/s  (0.658s,  194.66/s)  LR: 6.943e-04  Data: 0.032 (0.038)
Train: 26 [ 800/989 ( 81%)]  Loss: 3.66 (3.33)  Time: 0.673s,  190.10/s  (0.659s,  194.31/s)  LR: 6.943e-04  Data: 0.041 (0.038)
Train: 26 [ 850/989 ( 86%)]  Loss: 3.30 (3.33)  Time: 0.644s,  198.63/s  (0.660s,  193.97/s)  LR: 6.943e-04  Data: 0.035 (0.038)
Train: 26 [ 900/989 ( 91%)]  Loss: 3.09 (3.32)  Time: 0.835s,  153.31/s  (0.661s,  193.65/s)  LR: 6.943e-04  Data: 0.057 (0.037)
Train: 26 [ 950/989 ( 96%)]  Loss: 2.81 (3.29)  Time: 0.676s,  189.33/s  (0.662s,  193.37/s)  LR: 6.943e-04  Data: 0.029 (0.037)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.247 (1.247)  Loss:   1.683 ( 1.683)  Acc@1:  60.938 ( 60.938)  Acc@5:  87.500 ( 87.500)
Test: [  39/39]  Time: 0.070 (0.322)  Loss:   1.195 ( 1.548)  Acc@1:  62.500 ( 63.940)  Acc@5: 100.000 ( 87.300)
Current checkpoints:
 ('./output/train/Upd_Exp1_Image100/checkpoint-26.pth.tar', 63.94)
 ('./output/train/Upd_Exp1_Image100/checkpoint-25.pth.tar', 63.3)
 ('./output/train/Upd_Exp1_Image100/checkpoint-24.pth.tar', 62.9)
 ('./output/train/Upd_Exp1_Image100/checkpoint-23.pth.tar', 61.42)
 ('./output/train/Upd_Exp1_Image100/checkpoint-22.pth.tar', 61.12)
 ('./output/train/Upd_Exp1_Image100/checkpoint-20.pth.tar', 60.16)
 ('./output/train/Upd_Exp1_Image100/checkpoint-21.pth.tar', 59.9)
 ('./output/train/Upd_Exp1_Image100/checkpoint-18.pth.tar', 58.64)
 ('./output/train/Upd_Exp1_Image100/checkpoint-19.pth.tar', 58.64)
 ('./output/train/Upd_Exp1_Image100/checkpoint-17.pth.tar', 56.2)

Train: 27 [   0/989 (  0%)]  Loss: 3.54 (3.54)  Time: 1.837s,   69.68/s  (1.837s,   69.68/s)  LR: 6.933e-04  Data: 1.087 (1.087)
Train: 27 [  50/989 (  5%)]  Loss: 3.43 (3.49)  Time: 0.624s,  205.24/s  (0.680s,  188.34/s)  LR: 6.933e-04  Data: 0.019 (0.054)
Train: 27 [ 100/989 ( 10%)]  Loss: 3.09 (3.35)  Time: 0.646s,  198.08/s  (0.665s,  192.49/s)  LR: 6.933e-04  Data: 0.035 (0.046)
Train: 27 [ 150/989 ( 15%)]  Loss: 3.33 (3.35)  Time: 0.690s,  185.46/s  (0.662s,  193.39/s)  LR: 6.933e-04  Data: 0.041 (0.043)
Train: 27 [ 200/989 ( 20%)]  Loss: 3.19 (3.31)  Time: 0.648s,  197.64/s  (0.661s,  193.63/s)  LR: 6.933e-04  Data: 0.031 (0.041)
Train: 27 [ 250/989 ( 25%)]  Loss: 3.48 (3.34)  Time: 0.598s,  214.21/s  (0.659s,  194.19/s)  LR: 6.933e-04  Data: 0.023 (0.040)
Train: 27 [ 300/989 ( 30%)]  Loss: 2.69 (3.25)  Time: 0.608s,  210.37/s  (0.658s,  194.57/s)  LR: 6.933e-04  Data: 0.020 (0.040)
Train: 27 [ 350/989 ( 35%)]  Loss: 2.89 (3.20)  Time: 0.628s,  203.67/s  (0.657s,  194.81/s)  LR: 6.933e-04  Data: 0.038 (0.039)
Train: 27 [ 400/989 ( 40%)]  Loss: 3.54 (3.24)  Time: 0.607s,  210.86/s  (0.657s,  194.89/s)  LR: 6.933e-04  Data: 0.028 (0.039)
Train: 27 [ 450/989 ( 46%)]  Loss: 2.65 (3.18)  Time: 0.660s,  193.94/s  (0.657s,  194.92/s)  LR: 6.933e-04  Data: 0.032 (0.038)
Train: 27 [ 500/989 ( 51%)]  Loss: 3.73 (3.23)  Time: 0.636s,  201.11/s  (0.657s,  194.77/s)  LR: 6.933e-04  Data: 0.031 (0.038)
Train: 27 [ 550/989 ( 56%)]  Loss: 3.39 (3.24)  Time: 0.619s,  206.85/s  (0.657s,  194.81/s)  LR: 6.933e-04  Data: 0.057 (0.038)
Train: 27 [ 600/989 ( 61%)]  Loss: 2.77 (3.21)  Time: 0.623s,  205.57/s  (0.657s,  194.70/s)  LR: 6.933e-04  Data: 0.029 (0.038)
Train: 27 [ 650/989 ( 66%)]  Loss: 2.60 (3.17)  Time: 0.706s,  181.22/s  (0.658s,  194.48/s)  LR: 6.933e-04  Data: 0.050 (0.037)
Train: 27 [ 700/989 ( 71%)]  Loss: 3.25 (3.17)  Time: 0.658s,  194.47/s  (0.659s,  194.38/s)  LR: 6.933e-04  Data: 0.050 (0.037)
Train: 27 [ 750/989 ( 76%)]  Loss: 3.72 (3.20)  Time: 0.697s,  183.55/s  (0.659s,  194.21/s)  LR: 6.933e-04  Data: 0.037 (0.037)
Train: 27 [ 800/989 ( 81%)]  Loss: 3.12 (3.20)  Time: 0.709s,  180.57/s  (0.660s,  194.00/s)  LR: 6.933e-04  Data: 0.039 (0.037)
Train: 27 [ 850/989 ( 86%)]  Loss: 2.94 (3.19)  Time: 0.638s,  200.72/s  (0.661s,  193.64/s)  LR: 6.933e-04  Data: 0.032 (0.037)
Train: 27 [ 900/989 ( 91%)]  Loss: 3.57 (3.21)  Time: 0.646s,  198.01/s  (0.662s,  193.27/s)  LR: 6.933e-04  Data: 0.020 (0.037)
Train: 27 [ 950/989 ( 96%)]  Loss: 2.67 (3.18)  Time: 0.647s,  197.85/s  (0.663s,  192.98/s)  LR: 6.933e-04  Data: 0.025 (0.037)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.249 (1.249)  Loss:   1.534 ( 1.534)  Acc@1:  62.500 ( 62.500)  Acc@5:  87.500 ( 87.500)
Test: [  39/39]  Time: 0.049 (0.321)  Loss:   0.732 ( 1.398)  Acc@1:  75.000 ( 65.720)  Acc@5: 100.000 ( 88.060)
Current checkpoints:
 ('./output/train/Upd_Exp1_Image100/checkpoint-27.pth.tar', 65.72)
 ('./output/train/Upd_Exp1_Image100/checkpoint-26.pth.tar', 63.94)
 ('./output/train/Upd_Exp1_Image100/checkpoint-25.pth.tar', 63.3)
 ('./output/train/Upd_Exp1_Image100/checkpoint-24.pth.tar', 62.9)
 ('./output/train/Upd_Exp1_Image100/checkpoint-23.pth.tar', 61.42)
 ('./output/train/Upd_Exp1_Image100/checkpoint-22.pth.tar', 61.12)
 ('./output/train/Upd_Exp1_Image100/checkpoint-20.pth.tar', 60.16)
 ('./output/train/Upd_Exp1_Image100/checkpoint-21.pth.tar', 59.9)
 ('./output/train/Upd_Exp1_Image100/checkpoint-18.pth.tar', 58.64)
 ('./output/train/Upd_Exp1_Image100/checkpoint-19.pth.tar', 58.64)

Train: 28 [   0/989 (  0%)]  Loss: 3.53 (3.53)  Time: 1.709s,   74.90/s  (1.709s,   74.90/s)  LR: 6.922e-04  Data: 0.976 (0.976)
Train: 28 [  50/989 (  5%)]  Loss: 2.97 (3.25)  Time: 0.635s,  201.70/s  (0.676s,  189.41/s)  LR: 6.922e-04  Data: 0.039 (0.051)
Train: 28 [ 100/989 ( 10%)]  Loss: 3.68 (3.39)  Time: 0.690s,  185.58/s  (0.661s,  193.61/s)  LR: 6.922e-04  Data: 0.027 (0.043)
Train: 28 [ 150/989 ( 15%)]  Loss: 3.48 (3.41)  Time: 0.681s,  187.90/s  (0.657s,  194.96/s)  LR: 6.922e-04  Data: 0.054 (0.040)
Train: 28 [ 200/989 ( 20%)]  Loss: 3.16 (3.36)  Time: 0.640s,  200.02/s  (0.655s,  195.49/s)  LR: 6.922e-04  Data: 0.034 (0.039)
Train: 28 [ 250/989 ( 25%)]  Loss: 3.06 (3.31)  Time: 0.623s,  205.37/s  (0.654s,  195.69/s)  LR: 6.922e-04  Data: 0.034 (0.038)
Train: 28 [ 300/989 ( 30%)]  Loss: 3.45 (3.33)  Time: 0.645s,  198.52/s  (0.654s,  195.86/s)  LR: 6.922e-04  Data: 0.017 (0.037)
Train: 28 [ 350/989 ( 35%)]  Loss: 3.93 (3.41)  Time: 0.614s,  208.45/s  (0.654s,  195.86/s)  LR: 6.922e-04  Data: 0.020 (0.036)
Train: 28 [ 400/989 ( 40%)]  Loss: 2.92 (3.35)  Time: 0.629s,  203.57/s  (0.654s,  195.86/s)  LR: 6.922e-04  Data: 0.024 (0.036)
Train: 28 [ 450/989 ( 46%)]  Loss: 3.21 (3.34)  Time: 0.630s,  203.19/s  (0.653s,  195.89/s)  LR: 6.922e-04  Data: 0.026 (0.036)
Train: 28 [ 500/989 ( 51%)]  Loss: 3.59 (3.36)  Time: 0.633s,  202.28/s  (0.653s,  195.92/s)  LR: 6.922e-04  Data: 0.031 (0.036)
Train: 28 [ 550/989 ( 56%)]  Loss: 2.85 (3.32)  Time: 0.630s,  203.28/s  (0.653s,  195.90/s)  LR: 6.922e-04  Data: 0.030 (0.036)
Train: 28 [ 600/989 ( 61%)]  Loss: 2.83 (3.28)  Time: 0.646s,  198.01/s  (0.654s,  195.72/s)  LR: 6.922e-04  Data: 0.038 (0.035)
Train: 28 [ 650/989 ( 66%)]  Loss: 3.30 (3.28)  Time: 0.727s,  176.06/s  (0.654s,  195.59/s)  LR: 6.922e-04  Data: 0.065 (0.036)
Train: 28 [ 700/989 ( 71%)]  Loss: 3.54 (3.30)  Time: 0.851s,  150.43/s  (0.655s,  195.32/s)  LR: 6.922e-04  Data: 0.037 (0.036)
Train: 28 [ 750/989 ( 76%)]  Loss: 3.17 (3.29)  Time: 0.659s,  194.32/s  (0.656s,  195.12/s)  LR: 6.922e-04  Data: 0.018 (0.035)
Train: 28 [ 800/989 ( 81%)]  Loss: 2.96 (3.27)  Time: 0.637s,  200.85/s  (0.657s,  194.87/s)  LR: 6.922e-04  Data: 0.033 (0.036)
Train: 28 [ 850/989 ( 86%)]  Loss: 3.14 (3.26)  Time: 0.654s,  195.63/s  (0.658s,  194.57/s)  LR: 6.922e-04  Data: 0.024 (0.036)
Train: 28 [ 900/989 ( 91%)]  Loss: 3.04 (3.25)  Time: 0.782s,  163.78/s  (0.659s,  194.29/s)  LR: 6.922e-04  Data: 0.055 (0.035)
Train: 28 [ 950/989 ( 96%)]  Loss: 3.69 (3.27)  Time: 0.606s,  211.19/s  (0.660s,  193.84/s)  LR: 6.922e-04  Data: 0.034 (0.036)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.242 (1.242)  Loss:   1.413 ( 1.413)  Acc@1:  66.406 ( 66.406)  Acc@5:  90.625 ( 90.625)
Test: [  39/39]  Time: 0.048 (0.321)  Loss:   0.803 ( 1.482)  Acc@1:  75.000 ( 65.260)  Acc@5: 100.000 ( 88.500)
Current checkpoints:
 ('./output/train/Upd_Exp1_Image100/checkpoint-27.pth.tar', 65.72)
 ('./output/train/Upd_Exp1_Image100/checkpoint-28.pth.tar', 65.26)
 ('./output/train/Upd_Exp1_Image100/checkpoint-26.pth.tar', 63.94)
 ('./output/train/Upd_Exp1_Image100/checkpoint-25.pth.tar', 63.3)
 ('./output/train/Upd_Exp1_Image100/checkpoint-24.pth.tar', 62.9)
 ('./output/train/Upd_Exp1_Image100/checkpoint-23.pth.tar', 61.42)
 ('./output/train/Upd_Exp1_Image100/checkpoint-22.pth.tar', 61.12)
 ('./output/train/Upd_Exp1_Image100/checkpoint-20.pth.tar', 60.16)
 ('./output/train/Upd_Exp1_Image100/checkpoint-21.pth.tar', 59.9)
 ('./output/train/Upd_Exp1_Image100/checkpoint-18.pth.tar', 58.64)

Train: 29 [   0/989 (  0%)]  Loss: 3.28 (3.28)  Time: 1.928s,   66.38/s  (1.928s,   66.38/s)  LR: 6.912e-04  Data: 1.129 (1.129)
Train: 29 [  50/989 (  5%)]  Loss: 3.12 (3.20)  Time: 0.687s,  186.28/s  (0.686s,  186.52/s)  LR: 6.912e-04  Data: 0.040 (0.060)
Train: 29 [ 100/989 ( 10%)]  Loss: 2.92 (3.11)  Time: 0.657s,  194.79/s  (0.670s,  191.01/s)  LR: 6.912e-04  Data: 0.040 (0.049)
Train: 29 [ 150/989 ( 15%)]  Loss: 3.74 (3.26)  Time: 0.647s,  197.98/s  (0.665s,  192.55/s)  LR: 6.912e-04  Data: 0.037 (0.045)
Train: 29 [ 200/989 ( 20%)]  Loss: 2.72 (3.16)  Time: 0.648s,  197.68/s  (0.663s,  192.94/s)  LR: 6.912e-04  Data: 0.038 (0.043)
Train: 29 [ 250/989 ( 25%)]  Loss: 3.58 (3.23)  Time: 0.628s,  203.86/s  (0.661s,  193.70/s)  LR: 6.912e-04  Data: 0.023 (0.041)
Train: 29 [ 300/989 ( 30%)]  Loss: 3.11 (3.21)  Time: 0.632s,  202.52/s  (0.660s,  193.95/s)  LR: 6.912e-04  Data: 0.038 (0.040)
Train: 29 [ 350/989 ( 35%)]  Loss: 3.53 (3.25)  Time: 0.621s,  206.00/s  (0.660s,  194.01/s)  LR: 6.912e-04  Data: 0.021 (0.040)
Train: 29 [ 400/989 ( 40%)]  Loss: 2.65 (3.18)  Time: 0.716s,  178.81/s  (0.659s,  194.10/s)  LR: 6.912e-04  Data: 0.054 (0.039)
Train: 29 [ 450/989 ( 46%)]  Loss: 2.85 (3.15)  Time: 0.654s,  195.62/s  (0.659s,  194.12/s)  LR: 6.912e-04  Data: 0.050 (0.039)
Train: 29 [ 500/989 ( 51%)]  Loss: 3.21 (3.16)  Time: 0.644s,  198.78/s  (0.659s,  194.37/s)  LR: 6.912e-04  Data: 0.035 (0.038)
Train: 29 [ 550/989 ( 56%)]  Loss: 3.63 (3.20)  Time: 0.637s,  200.89/s  (0.659s,  194.34/s)  LR: 6.912e-04  Data: 0.030 (0.038)
Train: 29 [ 600/989 ( 61%)]  Loss: 3.70 (3.23)  Time: 0.604s,  211.93/s  (0.659s,  194.37/s)  LR: 6.912e-04  Data: 0.023 (0.038)
Train: 29 [ 650/989 ( 66%)]  Loss: 2.78 (3.20)  Time: 0.643s,  199.01/s  (0.659s,  194.28/s)  LR: 6.912e-04  Data: 0.033 (0.038)
Train: 29 [ 700/989 ( 71%)]  Loss: 2.56 (3.16)  Time: 0.593s,  215.75/s  (0.659s,  194.27/s)  LR: 6.912e-04  Data: 0.038 (0.038)
Train: 29 [ 750/989 ( 76%)]  Loss: 2.74 (3.13)  Time: 0.636s,  201.19/s  (0.659s,  194.15/s)  LR: 6.912e-04  Data: 0.040 (0.038)
Train: 29 [ 800/989 ( 81%)]  Loss: 3.58 (3.16)  Time: 0.692s,  184.97/s  (0.660s,  193.96/s)  LR: 6.912e-04  Data: 0.043 (0.038)
Train: 29 [ 850/989 ( 86%)]  Loss: 3.45 (3.18)  Time: 0.693s,  184.78/s  (0.661s,  193.74/s)  LR: 6.912e-04  Data: 0.052 (0.037)
Train: 29 [ 900/989 ( 91%)]  Loss: 3.05 (3.17)  Time: 0.660s,  193.91/s  (0.662s,  193.45/s)  LR: 6.912e-04  Data: 0.029 (0.037)
Train: 29 [ 950/989 ( 96%)]  Loss: 3.18 (3.17)  Time: 0.686s,  186.58/s  (0.663s,  193.20/s)  LR: 6.912e-04  Data: 0.058 (0.037)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.254 (1.254)  Loss:   1.458 ( 1.458)  Acc@1:  64.062 ( 64.062)  Acc@5:  88.281 ( 88.281)
Test: [  39/39]  Time: 0.048 (0.320)  Loss:   1.114 ( 1.522)  Acc@1:  62.500 ( 64.500)  Acc@5: 100.000 ( 87.680)
Current checkpoints:
 ('./output/train/Upd_Exp1_Image100/checkpoint-27.pth.tar', 65.72)
 ('./output/train/Upd_Exp1_Image100/checkpoint-28.pth.tar', 65.26)
 ('./output/train/Upd_Exp1_Image100/checkpoint-29.pth.tar', 64.5)
 ('./output/train/Upd_Exp1_Image100/checkpoint-26.pth.tar', 63.94)
 ('./output/train/Upd_Exp1_Image100/checkpoint-25.pth.tar', 63.3)
 ('./output/train/Upd_Exp1_Image100/checkpoint-24.pth.tar', 62.9)
 ('./output/train/Upd_Exp1_Image100/checkpoint-23.pth.tar', 61.42)
 ('./output/train/Upd_Exp1_Image100/checkpoint-22.pth.tar', 61.12)
 ('./output/train/Upd_Exp1_Image100/checkpoint-20.pth.tar', 60.16)
 ('./output/train/Upd_Exp1_Image100/checkpoint-21.pth.tar', 59.9)

Train: 30 [   0/989 (  0%)]  Loss: 3.10 (3.10)  Time: 1.760s,   72.72/s  (1.760s,   72.72/s)  LR: 6.900e-04  Data: 1.017 (1.017)
Train: 30 [  50/989 (  5%)]  Loss: 2.53 (2.82)  Time: 0.642s,  199.42/s  (0.681s,  187.83/s)  LR: 6.900e-04  Data: 0.032 (0.057)
Train: 30 [ 100/989 ( 10%)]  Loss: 3.43 (3.02)  Time: 0.658s,  194.52/s  (0.665s,  192.44/s)  LR: 6.900e-04  Data: 0.044 (0.047)
Train: 30 [ 150/989 ( 15%)]  Loss: 2.62 (2.92)  Time: 0.634s,  201.95/s  (0.660s,  193.80/s)  LR: 6.900e-04  Data: 0.042 (0.044)
Train: 30 [ 200/989 ( 20%)]  Loss: 2.82 (2.90)  Time: 0.642s,  199.52/s  (0.658s,  194.43/s)  LR: 6.900e-04  Data: 0.035 (0.042)
Train: 30 [ 250/989 ( 25%)]  Loss: 3.74 (3.04)  Time: 0.616s,  207.94/s  (0.657s,  194.94/s)  LR: 6.900e-04  Data: 0.028 (0.041)
Train: 30 [ 300/989 ( 30%)]  Loss: 3.85 (3.16)  Time: 0.633s,  202.30/s  (0.657s,  194.88/s)  LR: 6.900e-04  Data: 0.032 (0.040)
Train: 30 [ 350/989 ( 35%)]  Loss: 2.70 (3.10)  Time: 0.630s,  203.26/s  (0.656s,  194.99/s)  LR: 6.900e-04  Data: 0.027 (0.039)
Train: 30 [ 400/989 ( 40%)]  Loss: 3.60 (3.16)  Time: 0.621s,  206.23/s  (0.656s,  195.04/s)  LR: 6.900e-04  Data: 0.020 (0.039)
Train: 30 [ 450/989 ( 46%)]  Loss: 3.76 (3.22)  Time: 0.627s,  204.04/s  (0.655s,  195.29/s)  LR: 6.900e-04  Data: 0.025 (0.038)
Train: 30 [ 500/989 ( 51%)]  Loss: 3.05 (3.20)  Time: 0.648s,  197.48/s  (0.655s,  195.29/s)  LR: 6.900e-04  Data: 0.036 (0.038)
Train: 30 [ 550/989 ( 56%)]  Loss: 3.12 (3.19)  Time: 0.631s,  202.75/s  (0.656s,  195.15/s)  LR: 6.900e-04  Data: 0.031 (0.038)
Train: 30 [ 600/989 ( 61%)]  Loss: 3.17 (3.19)  Time: 0.621s,  206.08/s  (0.656s,  195.08/s)  LR: 6.900e-04  Data: 0.029 (0.038)
Train: 30 [ 650/989 ( 66%)]  Loss: 3.94 (3.25)  Time: 0.623s,  205.33/s  (0.656s,  195.11/s)  LR: 6.900e-04  Data: 0.038 (0.038)
Train: 30 [ 700/989 ( 71%)]  Loss: 3.53 (3.26)  Time: 0.768s,  166.58/s  (0.656s,  194.99/s)  LR: 6.900e-04  Data: 0.029 (0.037)
Train: 30 [ 750/989 ( 76%)]  Loss: 3.30 (3.27)  Time: 0.630s,  203.26/s  (0.657s,  194.83/s)  LR: 6.900e-04  Data: 0.021 (0.037)
Train: 30 [ 800/989 ( 81%)]  Loss: 3.23 (3.26)  Time: 0.706s,  181.40/s  (0.658s,  194.57/s)  LR: 6.900e-04  Data: 0.031 (0.037)
Train: 30 [ 850/989 ( 86%)]  Loss: 3.24 (3.26)  Time: 0.691s,  185.36/s  (0.659s,  194.29/s)  LR: 6.900e-04  Data: 0.037 (0.037)
Train: 30 [ 900/989 ( 91%)]  Loss: 3.36 (3.27)  Time: 0.657s,  194.74/s  (0.660s,  193.83/s)  LR: 6.900e-04  Data: 0.031 (0.037)
Train: 30 [ 950/989 ( 96%)]  Loss: 3.62 (3.29)  Time: 0.662s,  193.29/s  (0.662s,  193.40/s)  LR: 6.900e-04  Data: 0.025 (0.037)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.223 (1.223)  Loss:   1.661 ( 1.661)  Acc@1:  59.375 ( 59.375)  Acc@5:  85.156 ( 85.156)
Test: [  39/39]  Time: 0.036 (0.321)  Loss:   1.316 ( 1.361)  Acc@1:  50.000 ( 66.660)  Acc@5: 100.000 ( 88.960)
Current checkpoints:
 ('./output/train/Upd_Exp1_Image100/checkpoint-30.pth.tar', 66.66)
 ('./output/train/Upd_Exp1_Image100/checkpoint-27.pth.tar', 65.72)
 ('./output/train/Upd_Exp1_Image100/checkpoint-28.pth.tar', 65.26)
 ('./output/train/Upd_Exp1_Image100/checkpoint-29.pth.tar', 64.5)
 ('./output/train/Upd_Exp1_Image100/checkpoint-26.pth.tar', 63.94)
 ('./output/train/Upd_Exp1_Image100/checkpoint-25.pth.tar', 63.3)
 ('./output/train/Upd_Exp1_Image100/checkpoint-24.pth.tar', 62.9)
 ('./output/train/Upd_Exp1_Image100/checkpoint-23.pth.tar', 61.42)
 ('./output/train/Upd_Exp1_Image100/checkpoint-22.pth.tar', 61.12)
 ('./output/train/Upd_Exp1_Image100/checkpoint-20.pth.tar', 60.16)

Train: 31 [   0/989 (  0%)]  Loss: 3.12 (3.12)  Time: 1.896s,   67.50/s  (1.896s,   67.50/s)  LR: 6.889e-04  Data: 1.148 (1.148)
Train: 31 [  50/989 (  5%)]  Loss: 3.37 (3.24)  Time: 0.642s,  199.30/s  (0.686s,  186.55/s)  LR: 6.889e-04  Data: 0.039 (0.057)
Train: 31 [ 100/989 ( 10%)]  Loss: 2.81 (3.10)  Time: 0.633s,  202.10/s  (0.672s,  190.60/s)  LR: 6.889e-04  Data: 0.050 (0.047)
Train: 31 [ 150/989 ( 15%)]  Loss: 3.52 (3.20)  Time: 0.653s,  195.93/s  (0.667s,  192.04/s)  LR: 6.889e-04  Data: 0.040 (0.043)
Train: 31 [ 200/989 ( 20%)]  Loss: 3.09 (3.18)  Time: 0.651s,  196.64/s  (0.663s,  193.02/s)  LR: 6.889e-04  Data: 0.034 (0.041)
Train: 31 [ 250/989 ( 25%)]  Loss: 2.75 (3.11)  Time: 0.665s,  192.56/s  (0.662s,  193.41/s)  LR: 6.889e-04  Data: 0.044 (0.040)
Train: 31 [ 300/989 ( 30%)]  Loss: 2.27 (2.99)  Time: 0.681s,  187.92/s  (0.661s,  193.56/s)  LR: 6.889e-04  Data: 0.051 (0.040)
Train: 31 [ 350/989 ( 35%)]  Loss: 2.35 (2.91)  Time: 0.630s,  203.27/s  (0.661s,  193.77/s)  LR: 6.889e-04  Data: 0.036 (0.039)
Train: 31 [ 400/989 ( 40%)]  Loss: 3.30 (2.95)  Time: 0.655s,  195.37/s  (0.660s,  193.96/s)  LR: 6.889e-04  Data: 0.049 (0.039)
Train: 31 [ 450/989 ( 46%)]  Loss: 3.16 (2.97)  Time: 0.643s,  199.12/s  (0.660s,  194.03/s)  LR: 6.889e-04  Data: 0.032 (0.039)
Train: 31 [ 500/989 ( 51%)]  Loss: 3.44 (3.02)  Time: 0.635s,  201.44/s  (0.659s,  194.14/s)  LR: 6.889e-04  Data: 0.033 (0.038)
Train: 31 [ 550/989 ( 56%)]  Loss: 3.75 (3.08)  Time: 0.633s,  202.28/s  (0.659s,  194.30/s)  LR: 6.889e-04  Data: 0.026 (0.038)
Train: 31 [ 600/989 ( 61%)]  Loss: 2.51 (3.03)  Time: 0.646s,  198.00/s  (0.659s,  194.37/s)  LR: 6.889e-04  Data: 0.048 (0.037)
Train: 31 [ 650/989 ( 66%)]  Loss: 2.48 (2.99)  Time: 0.645s,  198.60/s  (0.658s,  194.39/s)  LR: 6.889e-04  Data: 0.038 (0.037)
Train: 31 [ 700/989 ( 71%)]  Loss: 3.86 (3.05)  Time: 0.572s,  223.67/s  (0.658s,  194.42/s)  LR: 6.889e-04  Data: 0.014 (0.037)
Train: 31 [ 750/989 ( 76%)]  Loss: 2.56 (3.02)  Time: 0.635s,  201.59/s  (0.659s,  194.19/s)  LR: 6.889e-04  Data: 0.034 (0.037)
Train: 31 [ 800/989 ( 81%)]  Loss: 3.59 (3.05)  Time: 0.653s,  196.02/s  (0.660s,  193.98/s)  LR: 6.889e-04  Data: 0.038 (0.037)
Train: 31 [ 850/989 ( 86%)]  Loss: 3.06 (3.05)  Time: 0.720s,  177.73/s  (0.661s,  193.57/s)  LR: 6.889e-04  Data: 0.010 (0.037)
Train: 31 [ 900/989 ( 91%)]  Loss: 2.87 (3.04)  Time: 0.663s,  192.98/s  (0.662s,  193.38/s)  LR: 6.889e-04  Data: 0.040 (0.036)
Train: 31 [ 950/989 ( 96%)]  Loss: 3.29 (3.06)  Time: 0.654s,  195.75/s  (0.663s,  193.05/s)  LR: 6.889e-04  Data: 0.038 (0.036)
Distributing BatchNorm running means and vars
Test: [   0/39]  Time: 1.208 (1.208)  Loss:   1.499 ( 1.499)  Acc@1:  65.625 ( 65.625)  Acc@5:  89.062 ( 89.062)
Test: [  39/39]  Time: 0.042 (0.321)  Loss:   0.800 ( 1.357)  Acc@1:  75.000 ( 66.780)  Acc@5: 100.000 ( 89.540)
Current checkpoints:
 ('./output/train/Upd_Exp1_Image100/checkpoint-31.pth.tar', 66.78)
 ('./output/train/Upd_Exp1_Image100/checkpoint-30.pth.tar', 66.66)
 ('./output/train/Upd_Exp1_Image100/checkpoint-27.pth.tar', 65.72)
 ('./output/train/Upd_Exp1_Image100/checkpoint-28.pth.tar', 65.26)
 ('./output/train/Upd_Exp1_Image100/checkpoint-29.pth.tar', 64.5)
 ('./output/train/Upd_Exp1_Image100/checkpoint-26.pth.tar', 63.94)
 ('./output/train/Upd_Exp1_Image100/checkpoint-25.pth.tar', 63.3)
 ('./output/train/Upd_Exp1_Image100/checkpoint-24.pth.tar', 62.9)
 ('./output/train/Upd_Exp1_Image100/checkpoint-23.pth.tar', 61.42)
 ('./output/train/Upd_Exp1_Image100/checkpoint-22.pth.tar', 61.12)

Train: 32 [   0/989 (  0%)]  Loss: 2.54 (2.54)  Time: 1.825s,   70.14/s  (1.825s,   70.14/s)  LR: 6.877e-04  Data: 1.091 (1.091)
Train: 32 [  50/989 (  5%)]  Loss: 3.35 (2.94)  Time: 0.620s,  206.48/s  (0.682s,  187.78/s)  LR: 6.877e-04  Data: 0.036 (0.055)
Train: 32 [ 100/989 ( 10%)]  Loss: 3.36 (3.08)  Time: 0.650s,  196.95/s  (0.667s,  191.85/s)  LR: 6.877e-04  Data: 0.034 (0.045)
slurmstepd: error: *** JOB 2006602 ON i45 CANCELLED AT 2024-03-03T02:41:25 DUE TO TIME LIMIT ***
