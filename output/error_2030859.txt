Training with a single process on 1 device (cuda:0).
WARNING: No pretrained configuration specified for MONet_T_16 model. Using a default. Please add a config to the model pretrained_cfg registry or pass explicitly.
Model MONet_T_16 created, param count:11741226
Data processing configuration for current model + dataset:
	input_size: (3, 32, 32)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.875
	crop_mode: center
AMP not enabled. Training in float32.
/home/sharipov/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Scheduled epochs: 300. LR stepped per epoch.
Train: 0 [   0/781 (  0%)]  Loss: 2.30 (2.30)  Time: 2.097s,   30.52/s  (2.097s,   30.52/s)  LR: 1.000e-05  Data: 1.145 (1.145)
Train: 0 [  50/781 (  6%)]  Loss: 2.28 (2.28)  Time: 0.079s,  809.78/s  (0.119s,  535.78/s)  LR: 1.000e-05  Data: 0.012 (0.033)
Train: 0 [ 100/781 ( 13%)]  Loss: 2.14 (2.25)  Time: 0.080s,  804.85/s  (0.100s,  642.90/s)  LR: 1.000e-05  Data: 0.010 (0.021)
Train: 0 [ 150/781 ( 19%)]  Loss: 2.05 (2.22)  Time: 0.079s,  805.95/s  (0.093s,  689.52/s)  LR: 1.000e-05  Data: 0.009 (0.018)
Train: 0 [ 200/781 ( 26%)]  Loss: 2.15 (2.21)  Time: 0.080s,  804.50/s  (0.089s,  715.59/s)  LR: 1.000e-05  Data: 0.009 (0.016)
Train: 0 [ 250/781 ( 32%)]  Loss: 2.15 (2.19)  Time: 0.080s,  804.39/s  (0.087s,  731.51/s)  LR: 1.000e-05  Data: 0.010 (0.015)
Train: 0 [ 300/781 ( 38%)]  Loss: 2.12 (2.19)  Time: 0.079s,  813.15/s  (0.086s,  743.31/s)  LR: 1.000e-05  Data: 0.010 (0.014)
Train: 0 [ 350/781 ( 45%)]  Loss: 2.29 (2.18)  Time: 0.080s,  799.62/s  (0.085s,  751.61/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 0 [ 400/781 ( 51%)]  Loss: 2.16 (2.17)  Time: 0.079s,  806.17/s  (0.084s,  758.19/s)  LR: 1.000e-05  Data: 0.010 (0.013)
Train: 0 [ 450/781 ( 58%)]  Loss: 2.13 (2.17)  Time: 0.081s,  788.50/s  (0.084s,  763.52/s)  LR: 1.000e-05  Data: 0.014 (0.013)
Train: 0 [ 500/781 ( 64%)]  Loss: 2.18 (2.17)  Time: 0.079s,  809.97/s  (0.083s,  767.81/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 0 [ 550/781 ( 71%)]  Loss: 2.25 (2.16)  Time: 0.080s,  795.96/s  (0.083s,  771.30/s)  LR: 1.000e-05  Data: 0.009 (0.012)
Train: 0 [ 600/781 ( 77%)]  Loss: 2.12 (2.16)  Time: 0.079s,  807.58/s  (0.083s,  774.30/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 0 [ 650/781 ( 83%)]  Loss: 2.07 (2.16)  Time: 0.080s,  795.36/s  (0.082s,  776.31/s)  LR: 1.000e-05  Data: 0.009 (0.012)
Train: 0 [ 700/781 ( 90%)]  Loss: 2.14 (2.15)  Time: 0.079s,  811.12/s  (0.082s,  778.34/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Train: 0 [ 750/781 ( 96%)]  Loss: 2.15 (2.15)  Time: 0.080s,  801.88/s  (0.082s,  780.09/s)  LR: 1.000e-05  Data: 0.010 (0.012)
Test: [   0/156]  Time: 1.651 (1.651)  Loss:   1.819 ( 1.819)  Acc@1:  39.062 ( 39.062)  Acc@5:  92.188 ( 92.188)
Test: [  50/156]  Time: 0.024 (0.055)  Loss:   1.914 ( 1.881)  Acc@1:  34.375 ( 33.701)  Acc@5:  84.375 ( 85.417)
Test: [ 100/156]  Time: 0.023 (0.040)  Loss:   1.913 ( 1.881)  Acc@1:  26.562 ( 33.400)  Acc@5:  81.250 ( 85.613)
Test: [ 150/156]  Time: 0.016 (0.034)  Loss:   1.873 ( 1.879)  Acc@1:  34.375 ( 33.361)  Acc@5:  84.375 ( 85.399)
Test: [ 156/156]  Time: 0.335 (0.035)  Loss:   1.733 ( 1.877)  Acc@1:  18.750 ( 33.290)  Acc@5:  87.500 ( 85.460)
Current checkpoints:
 ('./output/train/Upd_Exp5_CIFAR10_16_layer/checkpoint-0.pth.tar', 33.29)

Train: 1 [   0/781 (  0%)]  Loss: 2.04 (2.04)  Time: 0.311s,  206.01/s  (0.311s,  206.01/s)  LR: 1.900e-05  Data: 0.108 (0.108)
Train: 1 [  50/781 (  6%)]  Loss: 2.13 (2.10)  Time: 0.079s,  814.80/s  (0.084s,  762.96/s)  LR: 1.900e-05  Data: 0.009 (0.012)
Train: 1 [ 100/781 ( 13%)]  Loss: 2.21 (2.10)  Time: 0.080s,  803.69/s  (0.082s,  782.65/s)  LR: 1.900e-05  Data: 0.009 (0.011)
Train: 1 [ 150/781 ( 19%)]  Loss: 2.11 (2.10)  Time: 0.078s,  815.37/s  (0.081s,  788.56/s)  LR: 1.900e-05  Data: 0.010 (0.011)
Train: 1 [ 200/781 ( 26%)]  Loss: 1.96 (2.10)  Time: 0.079s,  812.71/s  (0.081s,  792.71/s)  LR: 1.900e-05  Data: 0.010 (0.011)
Train: 1 [ 250/781 ( 32%)]  Loss: 1.98 (2.10)  Time: 0.079s,  813.43/s  (0.080s,  795.24/s)  LR: 1.900e-05  Data: 0.009 (0.011)
Train: 1 [ 300/781 ( 38%)]  Loss: 2.14 (2.10)  Time: 0.079s,  805.47/s  (0.080s,  797.14/s)  LR: 1.900e-05  Data: 0.010 (0.010)
Train: 1 [ 350/781 ( 45%)]  Loss: 2.21 (2.09)  Time: 0.079s,  810.45/s  (0.080s,  798.45/s)  LR: 1.900e-05  Data: 0.009 (0.010)
Train: 1 [ 400/781 ( 51%)]  Loss: 2.14 (2.09)  Time: 0.078s,  816.55/s  (0.080s,  799.56/s)  LR: 1.900e-05  Data: 0.010 (0.010)
Train: 1 [ 450/781 ( 58%)]  Loss: 2.07 (2.09)  Time: 0.079s,  806.11/s  (0.080s,  800.31/s)  LR: 1.900e-05  Data: 0.009 (0.010)
Train: 1 [ 500/781 ( 64%)]  Loss: 1.96 (2.09)  Time: 0.079s,  810.64/s  (0.080s,  800.86/s)  LR: 1.900e-05  Data: 0.010 (0.010)
Train: 1 [ 550/781 ( 71%)]  Loss: 2.05 (2.09)  Time: 0.079s,  813.36/s  (0.080s,  800.56/s)  LR: 1.900e-05  Data: 0.010 (0.010)
Train: 1 [ 600/781 ( 77%)]  Loss: 2.10 (2.09)  Time: 0.081s,  792.50/s  (0.080s,  801.18/s)  LR: 1.900e-05  Data: 0.014 (0.010)
Train: 1 [ 650/781 ( 83%)]  Loss: 2.26 (2.08)  Time: 0.080s,  803.61/s  (0.080s,  801.59/s)  LR: 1.900e-05  Data: 0.009 (0.010)
Train: 1 [ 700/781 ( 90%)]  Loss: 2.18 (2.08)  Time: 0.079s,  814.52/s  (0.080s,  801.99/s)  LR: 1.900e-05  Data: 0.010 (0.010)
Train: 1 [ 750/781 ( 96%)]  Loss: 2.18 (2.08)  Time: 0.080s,  802.21/s  (0.080s,  802.42/s)  LR: 1.900e-05  Data: 0.009 (0.010)
Test: [   0/156]  Time: 0.089 (0.089)  Loss:   1.706 ( 1.706)  Acc@1:  39.062 ( 39.062)  Acc@5:  87.500 ( 87.500)
Test: [  50/156]  Time: 0.023 (0.025)  Loss:   1.812 ( 1.762)  Acc@1:  35.938 ( 38.358)  Acc@5:  89.062 ( 87.040)
Test: [ 100/156]  Time: 0.023 (0.024)  Loss:   1.816 ( 1.763)  Acc@1:  28.125 ( 38.041)  Acc@5:  85.938 ( 87.098)
Test: [ 150/156]  Time: 0.016 (0.024)  Loss:   1.759 ( 1.760)  Acc@1:  39.062 ( 37.759)  Acc@5:  84.375 ( 87.272)
Test: [ 156/156]  Time: 0.011 (0.023)  Loss:   1.722 ( 1.759)  Acc@1:  25.000 ( 37.600)  Acc@5:  87.500 ( 87.340)
Current checkpoints:
 ('./output/train/Upd_Exp5_CIFAR10_16_layer/checkpoint-1.pth.tar', 37.6)
 ('./output/train/Upd_Exp5_CIFAR10_16_layer/checkpoint-0.pth.tar', 33.29)

Train: 2 [   0/781 (  0%)]  Loss: 1.96 (1.96)  Time: 0.270s,  236.84/s  (0.270s,  236.84/s)  LR: 2.800e-05  Data: 0.117 (0.117)
Train: 2 [  50/781 (  6%)]  Loss: 2.16 (2.07)  Time: 0.079s,  815.24/s  (0.084s,  765.39/s)  LR: 2.800e-05  Data: 0.010 (0.012)
Train: 2 [ 100/781 ( 13%)]  Loss: 2.18 (2.07)  Time: 0.079s,  811.51/s  (0.082s,  782.17/s)  LR: 2.800e-05  Data: 0.010 (0.011)
Train: 2 [ 150/781 ( 19%)]  Loss: 2.16 (2.07)  Time: 0.079s,  812.51/s  (0.081s,  790.43/s)  LR: 2.800e-05  Data: 0.014 (0.011)
Train: 2 [ 200/781 ( 26%)]  Loss: 2.11 (2.07)  Time: 0.081s,  785.54/s  (0.081s,  793.95/s)  LR: 2.800e-05  Data: 0.010 (0.010)
Train: 2 [ 250/781 ( 32%)]  Loss: 2.10 (2.07)  Time: 0.080s,  801.22/s  (0.080s,  796.32/s)  LR: 2.800e-05  Data: 0.010 (0.010)
Train: 2 [ 300/781 ( 38%)]  Loss: 1.99 (2.06)  Time: 0.080s,  804.82/s  (0.080s,  798.49/s)  LR: 2.800e-05  Data: 0.008 (0.010)
Train: 2 [ 350/781 ( 45%)]  Loss: 2.16 (2.06)  Time: 0.080s,  801.23/s  (0.080s,  799.91/s)  LR: 2.800e-05  Data: 0.010 (0.010)
Train: 2 [ 400/781 ( 51%)]  Loss: 2.10 (2.06)  Time: 0.079s,  809.92/s  (0.080s,  801.50/s)  LR: 2.800e-05  Data: 0.010 (0.010)
Train: 2 [ 450/781 ( 58%)]  Loss: 2.07 (2.06)  Time: 0.080s,  803.37/s  (0.080s,  801.74/s)  LR: 2.800e-05  Data: 0.010 (0.010)
Train: 2 [ 500/781 ( 64%)]  Loss: 2.15 (2.06)  Time: 0.079s,  809.27/s  (0.080s,  802.45/s)  LR: 2.800e-05  Data: 0.009 (0.010)
Train: 2 [ 550/781 ( 71%)]  Loss: 2.13 (2.06)  Time: 0.079s,  810.73/s  (0.080s,  802.78/s)  LR: 2.800e-05  Data: 0.009 (0.010)
Train: 2 [ 600/781 ( 77%)]  Loss: 2.09 (2.05)  Time: 0.078s,  816.34/s  (0.080s,  803.12/s)  LR: 2.800e-05  Data: 0.009 (0.010)
Train: 2 [ 650/781 ( 83%)]  Loss: 2.19 (2.05)  Time: 0.079s,  815.07/s  (0.080s,  803.35/s)  LR: 2.800e-05  Data: 0.010 (0.010)
Train: 2 [ 700/781 ( 90%)]  Loss: 2.16 (2.05)  Time: 0.079s,  814.71/s  (0.080s,  803.67/s)  LR: 2.800e-05  Data: 0.015 (0.010)
Train: 2 [ 750/781 ( 96%)]  Loss: 2.02 (2.05)  Time: 0.078s,  819.10/s  (0.080s,  804.01/s)  LR: 2.800e-05  Data: 0.010 (0.010)
Test: [   0/156]  Time: 0.092 (0.092)  Loss:   1.619 ( 1.619)  Acc@1:  46.875 ( 46.875)  Acc@5:  92.188 ( 92.188)
Test: [  50/156]  Time: 0.023 (0.025)  Loss:   1.696 ( 1.676)  Acc@1:  43.750 ( 42.494)  Acc@5:  89.062 ( 90.227)
Test: [ 100/156]  Time: 0.024 (0.024)  Loss:   1.772 ( 1.679)  Acc@1:  35.938 ( 42.126)  Acc@5:  89.062 ( 89.991)
Test: [ 150/156]  Time: 0.016 (0.024)  Loss:   1.731 ( 1.678)  Acc@1:  43.750 ( 41.846)  Acc@5:  85.938 ( 89.890)
Test: [ 156/156]  Time: 0.011 (0.023)  Loss:   1.762 ( 1.677)  Acc@1:  31.250 ( 41.800)  Acc@5:  93.750 ( 89.980)
Current checkpoints:
 ('./output/train/Upd_Exp5_CIFAR10_16_layer/checkpoint-2.pth.tar', 41.8)
 ('./output/train/Upd_Exp5_CIFAR10_16_layer/checkpoint-1.pth.tar', 37.6)
 ('./output/train/Upd_Exp5_CIFAR10_16_layer/checkpoint-0.pth.tar', 33.29)

Train: 3 [   0/781 (  0%)]  Loss: 2.16 (2.16)  Time: 0.281s,  227.63/s  (0.281s,  227.63/s)  LR: 3.700e-05  Data: 0.112 (0.112)
Train: 3 [  50/781 (  6%)]  Loss: 2.19 (2.02)  Time: 0.080s,  803.85/s  (0.083s,  767.35/s)  LR: 3.700e-05  Data: 0.014 (0.012)
Train: 3 [ 100/781 ( 13%)]  Loss: 2.25 (2.04)  Time: 0.081s,  789.60/s  (0.081s,  786.87/s)  LR: 3.700e-05  Data: 0.010 (0.011)
Train: 3 [ 150/781 ( 19%)]  Loss: 2.09 (2.04)  Time: 0.082s,  784.37/s  (0.081s,  792.88/s)  LR: 3.700e-05  Data: 0.010 (0.011)
Train: 3 [ 200/781 ( 26%)]  Loss: 2.16 (2.03)  Time: 0.078s,  817.95/s  (0.080s,  796.45/s)  LR: 3.700e-05  Data: 0.010 (0.010)
Train: 3 [ 250/781 ( 32%)]  Loss: 2.17 (2.03)  Time: 0.079s,  805.71/s  (0.080s,  798.87/s)  LR: 3.700e-05  Data: 0.010 (0.010)
Train: 3 [ 300/781 ( 38%)]  Loss: 1.89 (2.04)  Time: 0.079s,  807.96/s  (0.080s,  800.43/s)  LR: 3.700e-05  Data: 0.010 (0.010)
Train: 3 [ 350/781 ( 45%)]  Loss: 2.06 (2.04)  Time: 0.079s,  809.62/s  (0.080s,  801.15/s)  LR: 3.700e-05  Data: 0.010 (0.010)
Train: 3 [ 400/781 ( 51%)]  Loss: 2.13 (2.04)  Time: 0.079s,  807.69/s  (0.080s,  801.29/s)  LR: 3.700e-05  Data: 0.010 (0.010)
Train: 3 [ 450/781 ( 58%)]  Loss: 2.02 (2.04)  Time: 0.078s,  817.87/s  (0.080s,  801.90/s)  LR: 3.700e-05  Data: 0.009 (0.010)
Train: 3 [ 500/781 ( 64%)]  Loss: 1.76 (2.04)  Time: 0.079s,  805.66/s  (0.080s,  802.22/s)  LR: 3.700e-05  Data: 0.010 (0.010)
Train: 3 [ 550/781 ( 71%)]  Loss: 1.96 (2.03)  Time: 0.078s,  817.37/s  (0.080s,  802.73/s)  LR: 3.700e-05  Data: 0.010 (0.010)
Train: 3 [ 600/781 ( 77%)]  Loss: 2.10 (2.03)  Time: 0.078s,  817.97/s  (0.080s,  803.27/s)  LR: 3.700e-05  Data: 0.010 (0.010)
Train: 3 [ 650/781 ( 83%)]  Loss: 1.88 (2.03)  Time: 0.079s,  810.43/s  (0.080s,  803.55/s)  LR: 3.700e-05  Data: 0.010 (0.010)
Train: 3 [ 700/781 ( 90%)]  Loss: 1.88 (2.03)  Time: 0.079s,  808.18/s  (0.080s,  803.89/s)  LR: 3.700e-05  Data: 0.009 (0.010)
Train: 3 [ 750/781 ( 96%)]  Loss: 1.93 (2.03)  Time: 0.078s,  822.59/s  (0.080s,  803.87/s)  LR: 3.700e-05  Data: 0.012 (0.010)
Test: [   0/156]  Time: 0.090 (0.090)  Loss:   1.634 ( 1.634)  Acc@1:  40.625 ( 40.625)  Acc@5:  90.625 ( 90.625)
Test: [  50/156]  Time: 0.023 (0.024)  Loss:   1.664 ( 1.632)  Acc@1:  43.750 ( 43.964)  Acc@5:  92.188 ( 90.839)
Test: [ 100/156]  Time: 0.023 (0.024)  Loss:   1.709 ( 1.633)  Acc@1:  40.625 ( 43.750)  Acc@5:  90.625 ( 90.532)
Test: [ 150/156]  Time: 0.016 (0.023)  Loss:   1.629 ( 1.633)  Acc@1:  45.312 ( 43.460)  Acc@5:  90.625 ( 90.666)
Test: [ 156/156]  Time: 0.011 (0.023)  Loss:   1.628 ( 1.633)  Acc@1:  37.500 ( 43.310)  Acc@5:  93.750 ( 90.680)
Current checkpoints:
 ('./output/train/Upd_Exp5_CIFAR10_16_layer/checkpoint-3.pth.tar', 43.31)
 ('./output/train/Upd_Exp5_CIFAR10_16_layer/checkpoint-2.pth.tar', 41.8)
 ('./output/train/Upd_Exp5_CIFAR10_16_layer/checkpoint-1.pth.tar', 37.6)
 ('./output/train/Upd_Exp5_CIFAR10_16_layer/checkpoint-0.pth.tar', 33.29)

Train: 4 [   0/781 (  0%)]  Loss: 2.00 (2.00)  Time: 0.266s,  240.23/s  (0.266s,  240.23/s)  LR: 4.600e-05  Data: 0.108 (0.108)
Train: 4 [  50/781 (  6%)]  Loss: 2.08 (2.03)  Time: 0.079s,  809.14/s  (0.085s,  751.47/s)  LR: 4.600e-05  Data: 0.010 (0.012)
Train: 4 [ 100/781 ( 13%)]  Loss: 1.95 (2.01)  Time: 0.079s,  809.84/s  (0.082s,  779.34/s)  LR: 4.600e-05  Data: 0.010 (0.011)
Train: 4 [ 150/781 ( 19%)]  Loss: 1.98 (2.01)  Time: 0.079s,  810.26/s  (0.081s,  788.83/s)  LR: 4.600e-05  Data: 0.010 (0.011)
Train: 4 [ 200/781 ( 26%)]  Loss: 2.06 (2.01)  Time: 0.079s,  809.66/s  (0.081s,  794.27/s)  LR: 4.600e-05  Data: 0.008 (0.010)
Train: 4 [ 250/781 ( 32%)]  Loss: 1.90 (2.01)  Time: 0.078s,  821.42/s  (0.080s,  797.53/s)  LR: 4.600e-05  Data: 0.010 (0.010)
Train: 4 [ 300/781 ( 38%)]  Loss: 1.94 (2.01)  Time: 0.078s,  818.57/s  (0.080s,  799.20/s)  LR: 4.600e-05  Data: 0.009 (0.010)
Train: 4 [ 350/781 ( 45%)]  Loss: 1.80 (2.00)  Time: 0.078s,  817.74/s  (0.080s,  800.56/s)  LR: 4.600e-05  Data: 0.009 (0.010)
Train: 4 [ 400/781 ( 51%)]  Loss: 2.09 (2.00)  Time: 0.078s,  819.28/s  (0.080s,  801.69/s)  LR: 4.600e-05  Data: 0.010 (0.010)
Train: 4 [ 450/781 ( 58%)]  Loss: 2.09 (2.01)  Time: 0.079s,  805.79/s  (0.080s,  802.13/s)  LR: 4.600e-05  Data: 0.009 (0.010)
Train: 4 [ 500/781 ( 64%)]  Loss: 2.16 (2.01)  Time: 0.078s,  818.63/s  (0.080s,  803.04/s)  LR: 4.600e-05  Data: 0.009 (0.010)
Train: 4 [ 550/781 ( 71%)]  Loss: 2.10 (2.01)  Time: 0.078s,  820.60/s  (0.080s,  803.77/s)  LR: 4.600e-05  Data: 0.010 (0.010)
Train: 4 [ 600/781 ( 77%)]  Loss: 1.99 (2.01)  Time: 0.079s,  808.39/s  (0.080s,  804.35/s)  LR: 4.600e-05  Data: 0.014 (0.010)
Train: 4 [ 650/781 ( 83%)]  Loss: 1.78 (2.01)  Time: 0.079s,  810.28/s  (0.080s,  804.90/s)  LR: 4.600e-05  Data: 0.010 (0.010)
Train: 4 [ 700/781 ( 90%)]  Loss: 2.16 (2.00)  Time: 0.079s,  808.88/s  (0.080s,  804.49/s)  LR: 4.600e-05  Data: 0.009 (0.010)
Train: 4 [ 750/781 ( 96%)]  Loss: 1.94 (2.00)  Time: 0.079s,  809.13/s  (0.080s,  804.81/s)  LR: 4.600e-05  Data: 0.010 (0.010)
Test: [   0/156]  Time: 0.092 (0.092)  Loss:   1.501 ( 1.501)  Acc@1:  50.000 ( 50.000)  Acc@5:  93.750 ( 93.750)
Test: [  50/156]  Time: 0.023 (0.024)  Loss:   1.600 ( 1.555)  Acc@1:  43.750 ( 47.273)  Acc@5:  93.750 ( 92.433)
Test: [ 100/156]  Time: 0.023 (0.024)  Loss:   1.629 ( 1.560)  Acc@1:  32.812 ( 46.906)  Acc@5:  92.188 ( 92.404)
Test: [ 150/156]  Time: 0.016 (0.023)  Loss:   1.592 ( 1.561)  Acc@1:  48.438 ( 46.461)  Acc@5:  90.625 ( 92.343)
Test: [ 156/156]  Time: 0.011 (0.023)  Loss:   1.630 ( 1.560)  Acc@1:  43.750 ( 46.360)  Acc@5:  93.750 ( 92.420)
Current checkpoints:
 ('./output/train/Upd_Exp5_CIFAR10_16_layer/checkpoint-4.pth.tar', 46.36)
 ('./output/train/Upd_Exp5_CIFAR10_16_layer/checkpoint-3.pth.tar', 43.31)
 ('./output/train/Upd_Exp5_CIFAR10_16_layer/checkpoint-2.pth.tar', 41.8)
 ('./output/train/Upd_Exp5_CIFAR10_16_layer/checkpoint-1.pth.tar', 37.6)
 ('./output/train/Upd_Exp5_CIFAR10_16_layer/checkpoint-0.pth.tar', 33.29)

Train: 5 [   0/781 (  0%)]  Loss: 2.15 (2.15)  Time: 0.270s,  237.11/s  (0.270s,  237.11/s)  LR: 5.500e-05  Data: 0.110 (0.110)
Train: 5 [  50/781 (  6%)]  Loss: 2.04 (2.00)  Time: 0.079s,  807.84/s  (0.083s,  767.29/s)  LR: 5.500e-05  Data: 0.010 (0.012)
Train: 5 [ 100/781 ( 13%)]  Loss: 2.15 (1.99)  Time: 0.078s,  816.74/s  (0.081s,  786.76/s)  LR: 5.500e-05  Data: 0.009 (0.011)
Train: 5 [ 150/781 ( 19%)]  Loss: 2.15 (1.99)  Time: 0.079s,  808.64/s  (0.081s,  793.12/s)  LR: 5.500e-05  Data: 0.010 (0.010)
Train: 5 [ 200/781 ( 26%)]  Loss: 1.85 (1.98)  Time: 0.079s,  805.31/s  (0.080s,  796.88/s)  LR: 5.500e-05  Data: 0.009 (0.010)
Train: 5 [ 250/781 ( 32%)]  Loss: 1.81 (1.98)  Time: 0.080s,  801.92/s  (0.080s,  798.29/s)  LR: 5.500e-05  Data: 0.015 (0.010)
Train: 5 [ 300/781 ( 38%)]  Loss: 1.71 (1.98)  Time: 0.080s,  802.67/s  (0.080s,  800.36/s)  LR: 5.500e-05  Data: 0.009 (0.010)
slurmstepd: error: *** JOB 2030859 ON i14 CANCELLED AT 2024-04-14T18:36:08 ***
