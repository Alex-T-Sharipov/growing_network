Training with a single process on 1 device (cuda:0).
WARNING: No pretrained configuration specified for MONet_T_dynamic model. Using a default. Please add a config to the model pretrained_cfg registry or pass explicitly.
Traceback (most recent call last):
  File "/home/sharipov/monet/train.py", line 1207, in <module>
    main()
  File "/home/sharipov/monet/train.py", line 432, in main
    model = create_model(
  File "/home/sharipov/monet/timm/models/_factory.py", line 117, in create_model
    model = create_fn(
  File "/home/sharipov/monet/timm/models/monet.py", line 529, in MONet_T_dynamic
    model = _create_improved_MONet('MONet_T_dynamic', pretrained=pretrained, **model_args)
  File "/home/sharipov/monet/timm/models/monet.py", line 478, in _create_improved_MONet
    model = build_model_with_cfg(
  File "/home/sharipov/monet/timm/models/_builder.py", line 385, in build_model_with_cfg
    model = model_cls(**kwargs)
  File "/home/sharipov/monet/timm/models/monet.py", line 193, in __init__
    self.init_weights(nlhb=nlhb)
  File "/home/sharipov/monet/timm/models/monet.py", line 267, in init_weights
    named_apply(partial(_init_weights, head_bias=head_bias), module=self)  # num_blocks-first
  File "/home/sharipov/monet/timm/models/_manipulate.py", line 34, in named_apply
    named_apply(fn=fn, module=child_module, name=child_name, depth_first=depth_first, include_root=True)
  File "/home/sharipov/monet/timm/models/_manipulate.py", line 34, in named_apply
    named_apply(fn=fn, module=child_module, name=child_name, depth_first=depth_first, include_root=True)
  File "/home/sharipov/monet/timm/models/_manipulate.py", line 34, in named_apply
    named_apply(fn=fn, module=child_module, name=child_name, depth_first=depth_first, include_root=True)
  [Previous line repeated 4 more times]
  File "/home/sharipov/monet/timm/models/_manipulate.py", line 36, in named_apply
    fn(module=module, name=name)
  File "/home/sharipov/monet/timm/models/monet.py", line 298, in _init_weights
    print('init kaiming normal')
OSError: [Errno 122] Disk quota exceeded
[2024-04-14 15:44:26,068] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 1493879) of binary: /usr/bin/python3
Traceback (most recent call last):
  File "/home/sharipov/.local/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/sharipov/.local/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/home/sharipov/.local/lib/python3.9/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/home/sharipov/.local/lib/python3.9/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/home/sharipov/.local/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/sharipov/.local/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-04-14_15:44:26
  host      : i32.izar.cluster
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 1493879)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
