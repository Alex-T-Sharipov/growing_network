Training with a single process on 1 device (cuda:0).
WARNING: No pretrained configuration specified for MONet_T_8 model. Using a default. Please add a config to the model pretrained_cfg registry or pass explicitly.
Model MONet_T_8 created, param count:5946650
Data processing configuration for current model + dataset:
	input_size: (3, 32, 32)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.875
	crop_mode: center
AMP not enabled. Training in float32.
/home/sharipov/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Scheduled epochs: 300. LR stepped per epoch.
Train: 0 [   0/781 (  0%)]  Loss: 2.30 (2.30)  Time: 2.112s,   30.31/s  (2.112s,   30.31/s)  LR: 1.000e-05  Data: 1.106 (1.106)
Train: 0 [  50/781 (  6%)]  Loss: 2.27 (2.29)  Time: 0.052s, 1236.26/s  (0.094s,  682.90/s)  LR: 1.000e-05  Data: 0.001 (0.024)
Train: 0 [ 100/781 ( 13%)]  Loss: 2.18 (2.26)  Time: 0.051s, 1245.54/s  (0.073s,  872.98/s)  LR: 1.000e-05  Data: 0.001 (0.013)
Train: 0 [ 150/781 ( 19%)]  Loss: 2.18 (2.24)  Time: 0.052s, 1237.06/s  (0.066s,  962.56/s)  LR: 1.000e-05  Data: 0.001 (0.009)
Train: 0 [ 200/781 ( 26%)]  Loss: 2.12 (2.23)  Time: 0.054s, 1194.15/s  (0.063s, 1014.29/s)  LR: 1.000e-05  Data: 0.006 (0.007)
Train: 0 [ 250/781 ( 32%)]  Loss: 2.21 (2.22)  Time: 0.052s, 1231.15/s  (0.061s, 1047.91/s)  LR: 1.000e-05  Data: 0.001 (0.006)
Train: 0 [ 300/781 ( 38%)]  Loss: 2.18 (2.21)  Time: 0.053s, 1201.37/s  (0.060s, 1072.44/s)  LR: 1.000e-05  Data: 0.001 (0.006)
Train: 0 [ 350/781 ( 45%)]  Loss: 2.23 (2.20)  Time: 0.052s, 1220.06/s  (0.059s, 1088.91/s)  LR: 1.000e-05  Data: 0.001 (0.005)
Train: 0 [ 400/781 ( 51%)]  Loss: 2.20 (2.20)  Time: 0.052s, 1242.40/s  (0.058s, 1102.88/s)  LR: 1.000e-05  Data: 0.001 (0.005)
Train: 0 [ 450/781 ( 58%)]  Loss: 2.09 (2.19)  Time: 0.052s, 1231.73/s  (0.057s, 1114.24/s)  LR: 1.000e-05  Data: 0.001 (0.004)
Train: 0 [ 500/781 ( 64%)]  Loss: 2.10 (2.19)  Time: 0.052s, 1227.15/s  (0.057s, 1123.59/s)  LR: 1.000e-05  Data: 0.001 (0.004)
Train: 0 [ 550/781 ( 71%)]  Loss: 2.17 (2.18)  Time: 0.056s, 1139.89/s  (0.057s, 1131.38/s)  LR: 1.000e-05  Data: 0.008 (0.004)
Train: 0 [ 600/781 ( 77%)]  Loss: 2.08 (2.18)  Time: 0.053s, 1204.14/s  (0.056s, 1137.82/s)  LR: 1.000e-05  Data: 0.001 (0.004)
Train: 0 [ 650/781 ( 83%)]  Loss: 2.01 (2.17)  Time: 0.053s, 1203.69/s  (0.056s, 1143.63/s)  LR: 1.000e-05  Data: 0.007 (0.004)
Train: 0 [ 700/781 ( 90%)]  Loss: 2.18 (2.17)  Time: 0.053s, 1207.78/s  (0.056s, 1148.54/s)  LR: 1.000e-05  Data: 0.001 (0.003)
Train: 0 [ 750/781 ( 96%)]  Loss: 1.96 (2.17)  Time: 0.053s, 1209.45/s  (0.056s, 1152.47/s)  LR: 1.000e-05  Data: 0.001 (0.003)
Test: [   0/156]  Time: 1.518 (1.518)  Loss:   1.880 ( 1.880)  Acc@1:  42.188 ( 42.188)  Acc@5:  87.500 ( 87.500)
Test: [  50/156]  Time: 0.016 (0.045)  Loss:   1.944 ( 1.902)  Acc@1:  29.688 ( 32.782)  Acc@5:  84.375 ( 84.620)
Test: [ 100/156]  Time: 0.016 (0.031)  Loss:   1.949 ( 1.904)  Acc@1:  21.875 ( 32.998)  Acc@5:  81.250 ( 84.932)
Test: [ 150/156]  Time: 0.009 (0.026)  Loss:   1.888 ( 1.901)  Acc@1:  32.812 ( 32.978)  Acc@5:  84.375 ( 84.882)
Test: [ 156/156]  Time: 0.078 (0.025)  Loss:   1.750 ( 1.900)  Acc@1:  25.000 ( 32.910)  Acc@5:  93.750 ( 84.900)
Current checkpoints:
 ('./output/train/Upd_Exp5_CIFAR10_8_layer/checkpoint-0.pth.tar', 32.91)

Train: 1 [   0/781 (  0%)]  Loss: 2.11 (2.11)  Time: 0.188s,  340.80/s  (0.188s,  340.80/s)  LR: 1.900e-05  Data: 0.116 (0.116)
Train: 1 [  50/781 (  6%)]  Loss: 1.98 (2.10)  Time: 0.052s, 1236.30/s  (0.057s, 1113.29/s)  LR: 1.900e-05  Data: 0.001 (0.004)
Train: 1 [ 100/781 ( 13%)]  Loss: 2.21 (2.10)  Time: 0.052s, 1225.45/s  (0.055s, 1159.60/s)  LR: 1.900e-05  Data: 0.001 (0.003)
Train: 1 [ 150/781 ( 19%)]  Loss: 1.97 (2.10)  Time: 0.053s, 1206.42/s  (0.054s, 1178.36/s)  LR: 1.900e-05  Data: 0.001 (0.003)
Train: 1 [ 200/781 ( 26%)]  Loss: 2.15 (2.10)  Time: 0.052s, 1227.30/s  (0.054s, 1186.84/s)  LR: 1.900e-05  Data: 0.001 (0.002)
Train: 1 [ 250/781 ( 32%)]  Loss: 2.13 (2.10)  Time: 0.056s, 1150.86/s  (0.054s, 1191.72/s)  LR: 1.900e-05  Data: 0.001 (0.002)
Train: 1 [ 300/781 ( 38%)]  Loss: 2.30 (2.10)  Time: 0.053s, 1196.54/s  (0.054s, 1195.14/s)  LR: 1.900e-05  Data: 0.001 (0.002)
Train: 1 [ 350/781 ( 45%)]  Loss: 1.97 (2.10)  Time: 0.053s, 1204.28/s  (0.053s, 1197.42/s)  LR: 1.900e-05  Data: 0.001 (0.002)
Train: 1 [ 400/781 ( 51%)]  Loss: 2.14 (2.10)  Time: 0.053s, 1214.86/s  (0.053s, 1199.04/s)  LR: 1.900e-05  Data: 0.001 (0.002)
Train: 1 [ 450/781 ( 58%)]  Loss: 2.08 (2.10)  Time: 0.049s, 1302.86/s  (0.053s, 1201.11/s)  LR: 1.900e-05  Data: 0.001 (0.002)
Train: 1 [ 500/781 ( 64%)]  Loss: 2.15 (2.10)  Time: 0.054s, 1178.63/s  (0.053s, 1202.20/s)  LR: 1.900e-05  Data: 0.009 (0.002)
Train: 1 [ 550/781 ( 71%)]  Loss: 2.16 (2.10)  Time: 0.052s, 1223.55/s  (0.053s, 1203.69/s)  LR: 1.900e-05  Data: 0.001 (0.002)
Train: 1 [ 600/781 ( 77%)]  Loss: 1.95 (2.09)  Time: 0.052s, 1236.28/s  (0.053s, 1204.52/s)  LR: 1.900e-05  Data: 0.006 (0.002)
Train: 1 [ 650/781 ( 83%)]  Loss: 2.21 (2.09)  Time: 0.052s, 1220.10/s  (0.053s, 1204.52/s)  LR: 1.900e-05  Data: 0.001 (0.002)
Train: 1 [ 700/781 ( 90%)]  Loss: 2.06 (2.09)  Time: 0.052s, 1225.39/s  (0.053s, 1205.49/s)  LR: 1.900e-05  Data: 0.001 (0.002)
Train: 1 [ 750/781 ( 96%)]  Loss: 2.14 (2.09)  Time: 0.056s, 1145.95/s  (0.053s, 1205.98/s)  LR: 1.900e-05  Data: 0.007 (0.002)
Test: [   0/156]  Time: 0.059 (0.059)  Loss:   1.784 ( 1.784)  Acc@1:  39.062 ( 39.062)  Acc@5:  89.062 ( 89.062)
Test: [  50/156]  Time: 0.016 (0.017)  Loss:   1.831 ( 1.792)  Acc@1:  42.188 ( 37.408)  Acc@5:  90.625 ( 87.286)
Test: [ 100/156]  Time: 0.016 (0.016)  Loss:   1.884 ( 1.795)  Acc@1:  28.125 ( 37.098)  Acc@5:  84.375 ( 87.485)
Test: [ 150/156]  Time: 0.009 (0.016)  Loss:   1.794 ( 1.791)  Acc@1:  43.750 ( 37.324)  Acc@5:  84.375 ( 87.500)
Test: [ 156/156]  Time: 0.006 (0.016)  Loss:   1.642 ( 1.791)  Acc@1:  37.500 ( 37.220)  Acc@5:  93.750 ( 87.600)
Current checkpoints:
 ('./output/train/Upd_Exp5_CIFAR10_8_layer/checkpoint-1.pth.tar', 37.22)
 ('./output/train/Upd_Exp5_CIFAR10_8_layer/checkpoint-0.pth.tar', 32.91)

Train: 2 [   0/781 (  0%)]  Loss: 2.00 (2.00)  Time: 0.170s,  376.41/s  (0.170s,  376.41/s)  LR: 2.800e-05  Data: 0.117 (0.117)
Train: 2 [  50/781 (  6%)]  Loss: 2.09 (2.07)  Time: 0.057s, 1121.86/s  (0.057s, 1118.39/s)  LR: 2.800e-05  Data: 0.007 (0.004)
Train: 2 [ 100/781 ( 13%)]  Loss: 2.20 (2.07)  Time: 0.049s, 1305.50/s  (0.055s, 1166.90/s)  LR: 2.800e-05  Data: 0.001 (0.003)
Train: 2 [ 150/781 ( 19%)]  Loss: 2.20 (2.08)  Time: 0.052s, 1239.96/s  (0.054s, 1181.83/s)  LR: 2.800e-05  Data: 0.001 (0.002)
Train: 2 [ 200/781 ( 26%)]  Loss: 2.21 (2.08)  Time: 0.052s, 1226.93/s  (0.054s, 1187.93/s)  LR: 2.800e-05  Data: 0.001 (0.002)
Train: 2 [ 250/781 ( 32%)]  Loss: 2.15 (2.08)  Time: 0.054s, 1182.11/s  (0.054s, 1193.53/s)  LR: 2.800e-05  Data: 0.001 (0.002)
Train: 2 [ 300/781 ( 38%)]  Loss: 2.01 (2.08)  Time: 0.049s, 1295.50/s  (0.053s, 1198.56/s)  LR: 2.800e-05  Data: 0.001 (0.002)
Train: 2 [ 350/781 ( 45%)]  Loss: 1.97 (2.08)  Time: 0.053s, 1196.60/s  (0.053s, 1199.86/s)  LR: 2.800e-05  Data: 0.001 (0.002)
Train: 2 [ 400/781 ( 51%)]  Loss: 2.18 (2.07)  Time: 0.053s, 1197.89/s  (0.053s, 1201.49/s)  LR: 2.800e-05  Data: 0.001 (0.002)
Train: 2 [ 450/781 ( 58%)]  Loss: 2.03 (2.07)  Time: 0.053s, 1203.44/s  (0.053s, 1203.22/s)  LR: 2.800e-05  Data: 0.001 (0.002)
Train: 2 [ 500/781 ( 64%)]  Loss: 1.82 (2.07)  Time: 0.058s, 1106.70/s  (0.053s, 1203.99/s)  LR: 2.800e-05  Data: 0.008 (0.002)
Train: 2 [ 550/781 ( 71%)]  Loss: 2.01 (2.07)  Time: 0.055s, 1161.78/s  (0.053s, 1204.83/s)  LR: 2.800e-05  Data: 0.001 (0.002)
Train: 2 [ 600/781 ( 77%)]  Loss: 2.00 (2.06)  Time: 0.052s, 1239.63/s  (0.053s, 1205.90/s)  LR: 2.800e-05  Data: 0.001 (0.002)
Train: 2 [ 650/781 ( 83%)]  Loss: 2.16 (2.06)  Time: 0.051s, 1254.77/s  (0.053s, 1206.46/s)  LR: 2.800e-05  Data: 0.001 (0.002)
Train: 2 [ 700/781 ( 90%)]  Loss: 2.00 (2.06)  Time: 0.052s, 1227.25/s  (0.053s, 1207.24/s)  LR: 2.800e-05  Data: 0.001 (0.002)
Train: 2 [ 750/781 ( 96%)]  Loss: 2.13 (2.06)  Time: 0.052s, 1232.83/s  (0.053s, 1208.03/s)  LR: 2.800e-05  Data: 0.001 (0.002)
Test: [   0/156]  Time: 0.061 (0.061)  Loss:   1.659 ( 1.659)  Acc@1:  45.312 ( 45.312)  Acc@5:  92.188 ( 92.188)
Test: [  50/156]  Time: 0.016 (0.017)  Loss:   1.727 ( 1.706)  Acc@1:  42.188 ( 40.349)  Acc@5:  89.062 ( 89.093)
Test: [ 100/156]  Time: 0.016 (0.016)  Loss:   1.746 ( 1.708)  Acc@1:  31.250 ( 41.213)  Acc@5:  89.062 ( 89.001)
Test: [ 150/156]  Time: 0.009 (0.016)  Loss:   1.741 ( 1.707)  Acc@1:  43.750 ( 40.966)  Acc@5:  85.938 ( 89.062)
Test: [ 156/156]  Time: 0.006 (0.016)  Loss:   1.658 ( 1.706)  Acc@1:  31.250 ( 40.820)  Acc@5:  93.750 ( 89.150)
Current checkpoints:
 ('./output/train/Upd_Exp5_CIFAR10_8_layer/checkpoint-2.pth.tar', 40.82)
 ('./output/train/Upd_Exp5_CIFAR10_8_layer/checkpoint-1.pth.tar', 37.22)
 ('./output/train/Upd_Exp5_CIFAR10_8_layer/checkpoint-0.pth.tar', 32.91)

Train: 3 [   0/781 (  0%)]  Loss: 2.10 (2.10)  Time: 0.168s,  381.91/s  (0.168s,  381.91/s)  LR: 3.700e-05  Data: 0.114 (0.114)
Train: 3 [  50/781 (  6%)]  Loss: 2.11 (2.05)  Time: 0.053s, 1207.82/s  (0.057s, 1120.30/s)  LR: 3.700e-05  Data: 0.001 (0.005)
Train: 3 [ 100/781 ( 13%)]  Loss: 1.92 (2.05)  Time: 0.052s, 1227.17/s  (0.055s, 1160.94/s)  LR: 3.700e-05  Data: 0.001 (0.003)
Train: 3 [ 150/781 ( 19%)]  Loss: 2.06 (2.06)  Time: 0.048s, 1321.33/s  (0.054s, 1177.66/s)  LR: 3.700e-05  Data: 0.001 (0.003)
Train: 3 [ 200/781 ( 26%)]  Loss: 2.19 (2.06)  Time: 0.053s, 1213.29/s  (0.054s, 1187.05/s)  LR: 3.700e-05  Data: 0.001 (0.003)
Train: 3 [ 250/781 ( 32%)]  Loss: 1.98 (2.06)  Time: 0.052s, 1232.97/s  (0.054s, 1193.35/s)  LR: 3.700e-05  Data: 0.001 (0.002)
Train: 3 [ 300/781 ( 38%)]  Loss: 1.83 (2.06)  Time: 0.052s, 1237.01/s  (0.053s, 1198.01/s)  LR: 3.700e-05  Data: 0.001 (0.002)
Train: 3 [ 350/781 ( 45%)]  Loss: 1.95 (2.06)  Time: 0.052s, 1234.27/s  (0.053s, 1200.87/s)  LR: 3.700e-05  Data: 0.001 (0.002)
Train: 3 [ 400/781 ( 51%)]  Loss: 1.90 (2.06)  Time: 0.052s, 1221.03/s  (0.053s, 1202.85/s)  LR: 3.700e-05  Data: 0.009 (0.002)
Train: 3 [ 450/781 ( 58%)]  Loss: 2.10 (2.05)  Time: 0.053s, 1210.47/s  (0.053s, 1204.32/s)  LR: 3.700e-05  Data: 0.001 (0.002)
Train: 3 [ 500/781 ( 64%)]  Loss: 2.15 (2.05)  Time: 0.053s, 1212.43/s  (0.053s, 1205.34/s)  LR: 3.700e-05  Data: 0.001 (0.002)
Train: 3 [ 550/781 ( 71%)]  Loss: 1.93 (2.05)  Time: 0.052s, 1230.57/s  (0.053s, 1206.25/s)  LR: 3.700e-05  Data: 0.001 (0.002)
Train: 3 [ 600/781 ( 77%)]  Loss: 2.12 (2.05)  Time: 0.053s, 1205.49/s  (0.053s, 1206.80/s)  LR: 3.700e-05  Data: 0.001 (0.002)
Train: 3 [ 650/781 ( 83%)]  Loss: 2.11 (2.05)  Time: 0.052s, 1226.74/s  (0.053s, 1207.35/s)  LR: 3.700e-05  Data: 0.001 (0.002)
Train: 3 [ 700/781 ( 90%)]  Loss: 2.02 (2.05)  Time: 0.052s, 1231.15/s  (0.053s, 1206.47/s)  LR: 3.700e-05  Data: 0.001 (0.002)
Train: 3 [ 750/781 ( 96%)]  Loss: 2.18 (2.05)  Time: 0.052s, 1227.50/s  (0.053s, 1206.90/s)  LR: 3.700e-05  Data: 0.001 (0.002)
Test: [   0/156]  Time: 0.066 (0.066)  Loss:   1.636 ( 1.636)  Acc@1:  43.750 ( 43.750)  Acc@5:  92.188 ( 92.188)
Test: [  50/156]  Time: 0.016 (0.017)  Loss:   1.708 ( 1.668)  Acc@1:  40.625 ( 43.413)  Acc@5:  92.188 ( 90.349)
Test: [ 100/156]  Time: 0.016 (0.016)  Loss:   1.728 ( 1.669)  Acc@1:  37.500 ( 43.131)  Acc@5:  87.500 ( 90.316)
Test: [ 150/156]  Time: 0.009 (0.016)  Loss:   1.691 ( 1.666)  Acc@1:  43.750 ( 43.181)  Acc@5:  87.500 ( 90.252)
Test: [ 156/156]  Time: 0.006 (0.016)  Loss:   1.650 ( 1.666)  Acc@1:  37.500 ( 43.060)  Acc@5:  93.750 ( 90.320)
Current checkpoints:
 ('./output/train/Upd_Exp5_CIFAR10_8_layer/checkpoint-3.pth.tar', 43.06)
 ('./output/train/Upd_Exp5_CIFAR10_8_layer/checkpoint-2.pth.tar', 40.82)
 ('./output/train/Upd_Exp5_CIFAR10_8_layer/checkpoint-1.pth.tar', 37.22)
 ('./output/train/Upd_Exp5_CIFAR10_8_layer/checkpoint-0.pth.tar', 32.91)

Train: 4 [   0/781 (  0%)]  Loss: 2.22 (2.22)  Time: 0.164s,  389.15/s  (0.164s,  389.15/s)  LR: 4.600e-05  Data: 0.105 (0.105)
Train: 4 [  50/781 (  6%)]  Loss: 1.88 (2.03)  Time: 0.052s, 1220.98/s  (0.057s, 1119.83/s)  LR: 4.600e-05  Data: 0.001 (0.004)
Train: 4 [ 100/781 ( 13%)]  Loss: 2.10 (2.02)  Time: 0.052s, 1228.30/s  (0.055s, 1164.70/s)  LR: 4.600e-05  Data: 0.001 (0.003)
Train: 4 [ 150/781 ( 19%)]  Loss: 1.77 (2.03)  Time: 0.052s, 1224.27/s  (0.054s, 1180.47/s)  LR: 4.600e-05  Data: 0.001 (0.003)
Train: 4 [ 200/781 ( 26%)]  Loss: 1.83 (2.03)  Time: 0.053s, 1203.61/s  (0.054s, 1188.71/s)  LR: 4.600e-05  Data: 0.001 (0.003)
Train: 4 [ 250/781 ( 32%)]  Loss: 1.90 (2.03)  Time: 0.057s, 1123.75/s  (0.054s, 1193.44/s)  LR: 4.600e-05  Data: 0.007 (0.002)
Train: 4 [ 300/781 ( 38%)]  Loss: 2.20 (2.02)  Time: 0.053s, 1217.58/s  (0.054s, 1196.07/s)  LR: 4.600e-05  Data: 0.001 (0.002)
Train: 4 [ 350/781 ( 45%)]  Loss: 1.99 (2.03)  Time: 0.052s, 1229.76/s  (0.053s, 1199.08/s)  LR: 4.600e-05  Data: 0.008 (0.002)
Train: 4 [ 400/781 ( 51%)]  Loss: 1.95 (2.03)  Time: 0.057s, 1124.65/s  (0.053s, 1200.45/s)  LR: 4.600e-05  Data: 0.008 (0.002)
Train: 4 [ 450/781 ( 58%)]  Loss: 2.09 (2.02)  Time: 0.056s, 1143.66/s  (0.053s, 1201.20/s)  LR: 4.600e-05  Data: 0.007 (0.002)
Train: 4 [ 500/781 ( 64%)]  Loss: 2.06 (2.02)  Time: 0.051s, 1248.62/s  (0.053s, 1202.89/s)  LR: 4.600e-05  Data: 0.001 (0.002)
Train: 4 [ 550/781 ( 71%)]  Loss: 2.03 (2.02)  Time: 0.051s, 1249.64/s  (0.053s, 1204.39/s)  LR: 4.600e-05  Data: 0.001 (0.002)
Train: 4 [ 600/781 ( 77%)]  Loss: 1.96 (2.02)  Time: 0.053s, 1203.07/s  (0.053s, 1205.13/s)  LR: 4.600e-05  Data: 0.001 (0.002)
Train: 4 [ 650/781 ( 83%)]  Loss: 2.05 (2.02)  Time: 0.052s, 1219.75/s  (0.053s, 1205.85/s)  LR: 4.600e-05  Data: 0.001 (0.002)
Train: 4 [ 700/781 ( 90%)]  Loss: 2.14 (2.02)  Time: 0.052s, 1229.65/s  (0.053s, 1206.80/s)  LR: 4.600e-05  Data: 0.001 (0.002)
Train: 4 [ 750/781 ( 96%)]  Loss: 2.18 (2.02)  Time: 0.052s, 1227.46/s  (0.053s, 1207.47/s)  LR: 4.600e-05  Data: 0.001 (0.002)
Test: [   0/156]  Time: 0.064 (0.064)  Loss:   1.566 ( 1.566)  Acc@1:  46.875 ( 46.875)  Acc@5:  92.188 ( 92.188)
Test: [  50/156]  Time: 0.016 (0.017)  Loss:   1.683 ( 1.608)  Acc@1:  34.375 ( 42.157)  Acc@5:  90.625 ( 91.544)
Test: [ 100/156]  Time: 0.016 (0.016)  Loss:   1.688 ( 1.611)  Acc@1:  35.938 ( 43.348)  Acc@5:  87.500 ( 91.414)
Test: [ 150/156]  Time: 0.009 (0.016)  Loss:   1.653 ( 1.609)  Acc@1:  39.062 ( 43.553)  Acc@5:  89.062 ( 91.422)
Test: [ 156/156]  Time: 0.006 (0.016)  Loss:   1.517 ( 1.609)  Acc@1:  43.750 ( 43.380)  Acc@5:  93.750 ( 91.480)
Current checkpoints:
 ('./output/train/Upd_Exp5_CIFAR10_8_layer/checkpoint-4.pth.tar', 43.38)
 ('./output/train/Upd_Exp5_CIFAR10_8_layer/checkpoint-3.pth.tar', 43.06)
 ('./output/train/Upd_Exp5_CIFAR10_8_layer/checkpoint-2.pth.tar', 40.82)
 ('./output/train/Upd_Exp5_CIFAR10_8_layer/checkpoint-1.pth.tar', 37.22)
 ('./output/train/Upd_Exp5_CIFAR10_8_layer/checkpoint-0.pth.tar', 32.91)

Train: 5 [   0/781 (  0%)]  Loss: 2.10 (2.10)  Time: 0.170s,  375.95/s  (0.170s,  375.95/s)  LR: 5.500e-05  Data: 0.115 (0.115)
Train: 5 [  50/781 (  6%)]  Loss: 1.91 (2.00)  Time: 0.052s, 1232.19/s  (0.057s, 1125.11/s)  LR: 5.500e-05  Data: 0.001 (0.004)
Train: 5 [ 100/781 ( 13%)]  Loss: 1.83 (2.00)  Time: 0.052s, 1229.66/s  (0.055s, 1167.60/s)  LR: 5.500e-05  Data: 0.001 (0.003)
Train: 5 [ 150/781 ( 19%)]  Loss: 2.05 (2.00)  Time: 0.053s, 1206.86/s  (0.054s, 1182.02/s)  LR: 5.500e-05  Data: 0.001 (0.002)
Train: 5 [ 200/781 ( 26%)]  Loss: 1.70 (2.00)  Time: 0.053s, 1209.98/s  (0.054s, 1190.77/s)  LR: 5.500e-05  Data: 0.001 (0.002)
Train: 5 [ 250/781 ( 32%)]  Loss: 2.08 (2.01)  Time: 0.052s, 1238.83/s  (0.054s, 1195.10/s)  LR: 5.500e-05  Data: 0.001 (0.002)
Train: 5 [ 300/781 ( 38%)]  Loss: 2.13 (2.01)  Time: 0.055s, 1157.79/s  (0.053s, 1198.58/s)  LR: 5.500e-05  Data: 0.001 (0.002)
Train: 5 [ 350/781 ( 45%)]  Loss: 1.72 (2.01)  Time: 0.055s, 1164.12/s  (0.053s, 1199.33/s)  LR: 5.500e-05  Data: 0.007 (0.002)
Train: 5 [ 400/781 ( 51%)]  Loss: 2.14 (2.01)  Time: 0.052s, 1225.69/s  (0.053s, 1201.40/s)  LR: 5.500e-05  Data: 0.001 (0.002)
Train: 5 [ 450/781 ( 58%)]  Loss: 1.88 (2.00)  Time: 0.052s, 1225.36/s  (0.053s, 1203.05/s)  LR: 5.500e-05  Data: 0.001 (0.002)
Train: 5 [ 500/781 ( 64%)]  Loss: 2.18 (2.00)  Time: 0.052s, 1223.79/s  (0.053s, 1204.23/s)  LR: 5.500e-05  Data: 0.001 (0.002)
Train: 5 [ 550/781 ( 71%)]  Loss: 2.09 (2.00)  Time: 0.052s, 1219.88/s  (0.053s, 1205.43/s)  LR: 5.500e-05  Data: 0.001 (0.002)
Train: 5 [ 600/781 ( 77%)]  Loss: 1.91 (2.00)  Time: 0.053s, 1218.36/s  (0.053s, 1206.22/s)  LR: 5.500e-05  Data: 0.001 (0.002)
Train: 5 [ 650/781 ( 83%)]  Loss: 1.88 (1.99)  Time: 0.048s, 1321.18/s  (0.053s, 1206.92/s)  LR: 5.500e-05  Data: 0.001 (0.002)
Train: 5 [ 700/781 ( 90%)]  Loss: 1.98 (1.99)  Time: 0.055s, 1163.76/s  (0.053s, 1207.22/s)  LR: 5.500e-05  Data: 0.001 (0.002)
Train: 5 [ 750/781 ( 96%)]  Loss: 2.14 (1.99)  Time: 0.053s, 1196.94/s  (0.053s, 1207.38/s)  LR: 5.500e-05  Data: 0.001 (0.002)
Test: [   0/156]  Time: 0.067 (0.067)  Loss:   1.545 ( 1.545)  Acc@1:  50.000 ( 50.000)  Acc@5:  90.625 ( 90.625)
Test: [  50/156]  Time: 0.016 (0.017)  Loss:   1.608 ( 1.570)  Acc@1:  43.750 ( 46.875)  Acc@5:  92.188 ( 91.667)
Test: [ 100/156]  Time: 0.016 (0.016)  Loss:   1.621 ( 1.567)  Acc@1:  40.625 ( 47.293)  Acc@5:  92.188 ( 91.801)
Test: [ 150/156]  Time: 0.009 (0.016)  Loss:   1.582 ( 1.566)  Acc@1:  45.312 ( 47.010)  Acc@5:  90.625 ( 91.846)
Test: [ 156/156]  Time: 0.006 (0.016)  Loss:   1.528 ( 1.567)  Acc@1:  43.750 ( 46.860)  Acc@5:  93.750 ( 91.890)
Current checkpoints:
 ('./output/train/Upd_Exp5_CIFAR10_8_layer/checkpoint-5.pth.tar', 46.86)
 ('./output/train/Upd_Exp5_CIFAR10_8_layer/checkpoint-4.pth.tar', 43.38)
 ('./output/train/Upd_Exp5_CIFAR10_8_layer/checkpoint-3.pth.tar', 43.06)
 ('./output/train/Upd_Exp5_CIFAR10_8_layer/checkpoint-2.pth.tar', 40.82)
 ('./output/train/Upd_Exp5_CIFAR10_8_layer/checkpoint-1.pth.tar', 37.22)
 ('./output/train/Upd_Exp5_CIFAR10_8_layer/checkpoint-0.pth.tar', 32.91)

Train: 6 [   0/781 (  0%)]  Loss: 1.91 (1.91)  Time: 0.171s,  374.04/s  (0.171s,  374.04/s)  LR: 6.400e-05  Data: 0.114 (0.114)
Train: 6 [  50/781 (  6%)]  Loss: 2.14 (2.03)  Time: 0.054s, 1194.23/s  (0.057s, 1119.39/s)  LR: 6.400e-05  Data: 0.001 (0.004)
Train: 6 [ 100/781 ( 13%)]  Loss: 1.92 (2.02)  Time: 0.053s, 1217.71/s  (0.055s, 1164.99/s)  LR: 6.400e-05  Data: 0.001 (0.003)
Train: 6 [ 150/781 ( 19%)]  Loss: 1.77 (2.01)  Time: 0.052s, 1226.48/s  (0.054s, 1182.39/s)  LR: 6.400e-05  Data: 0.001 (0.003)
Train: 6 [ 200/781 ( 26%)]  Loss: 2.25 (2.00)  Time: 0.053s, 1215.66/s  (0.054s, 1190.86/s)  LR: 6.400e-05  Data: 0.001 (0.002)
Train: 6 [ 250/781 ( 32%)]  Loss: 2.23 (2.00)  Time: 0.054s, 1187.36/s  (0.053s, 1196.39/s)  LR: 6.400e-05  Data: 0.009 (0.002)
Train: 6 [ 300/781 ( 38%)]  Loss: 2.06 (2.00)  Time: 0.049s, 1313.02/s  (0.053s, 1200.15/s)  LR: 6.400e-05  Data: 0.001 (0.002)
Train: 6 [ 350/781 ( 45%)]  Loss: 2.18 (1.99)  Time: 0.052s, 1223.79/s  (0.053s, 1202.20/s)  LR: 6.400e-05  Data: 0.001 (0.002)
Train: 6 [ 400/781 ( 51%)]  Loss: 2.16 (1.99)  Time: 0.052s, 1223.15/s  (0.053s, 1203.68/s)  LR: 6.400e-05  Data: 0.001 (0.002)
Train: 6 [ 450/781 ( 58%)]  Loss: 2.06 (1.99)  Time: 0.054s, 1191.80/s  (0.053s, 1204.75/s)  LR: 6.400e-05  Data: 0.001 (0.002)
Train: 6 [ 500/781 ( 64%)]  Loss: 2.06 (1.99)  Time: 0.051s, 1245.88/s  (0.053s, 1205.77/s)  LR: 6.400e-05  Data: 0.001 (0.002)
slurmstepd: error: *** JOB 2030860 ON i65 CANCELLED AT 2024-04-14T18:36:05 ***
